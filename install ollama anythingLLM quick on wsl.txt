# Last amended: 23rd May, 2024
# Quick install of WSL ubuntu, ollama and AnythingLLM
# For detailed install instructions, refer this file in GitHub:
#	https://github.com/harnalashok/LLMs/blob/main/install%20ollama%20and%20anythingLLM%20on%20ubuntu.txt

# ollama and AnythingLLM
# Ref: YouTube Video: https://www.youtube.com/watch?v=IJYC6zf86lU



######################################################
############### AA.1 INSTALL latest wsl Ubuntu #######
######################################################

1.0 Open powershell as Administrator. Write the command:

	wsl --install
	
# 1.0.1 Reboot Windows when asked for. 
#	Create your userid and password in Ubuntu
#	Recommended for office machines: ashok/ashok 


1.1  Update ubuntu and install some needed software:

sudo apt update
sudo apt install -y net-tools  curl git-all build-essential


######################################################
############### AA.2 INSTALL ollama ##################
######################################################
# 2.0 Install/Uninstall Ollama by following instructions here:
#     See https://github.com/ollama/ollama/blob/main/docs/linux.md:
#	   ollama port: 11434
#


	$ curl -fsSL https://ollama.com/install.sh | sh


# 2.0.1 Issue following commands to download llama3:
#        Downloaded models are saved to folder /usr/share/ollama/.ollama/models

	$ ollama pull llama3:8b



# 2.0.2 On terminal, write a question to get an answer.
#       Exit by typing:  /bye


# 2.0.3 ollama supports GPUs with compute capability of 5.


# 2.0.4 List all models downloaded:

	$ ollama list

#	OR 

	$ ollama



# 2.0.5 ollama service can be started/restarted/shut, as:

	sudo systemctl start/stop ollama




#######################################################
############### AA.3 INSTALL Python environment #######
#######################################################


# 3.0 In Ubuntu, download and Install Anaconda, if not installed

#    a. Download  
	$ wget -c https://repo.anaconda.com/archive/Anaconda3-2024.02-1-Linux-x86_64.sh

#    b. Install
        $ bash Anaconda3-2024.02-1-Linux-x86_64.sh


# 3.1 Close Ubuntu terminal and then open Ubuntu terminal


# 3.2 Create conda environment and install packages
#       Copy and paste all the following conda and pip
#	commands in ubuntu terminal in one go:


conda create -y --name langchain python=3.11
conda activate langchain
conda config --add channels conda-forge
conda install -y spyder jupyter jupyterlab pandas numpy pip-tools ipython langchain beautifulsoup4 pypdf transformers fastai::accelerate huggingface_hub git openai streamlit
conda install -y -c huggingface -c conda-forge datasets
pip install ollama chromadb langchain-experimental tensorflow



######################################################
############### AA.4 INSTALL CUDA ####################
############# AND Appropriate driver #################
######################################################

# 4.0 Install CUDA as in this video. Installation depends upon
#	which GPU is available. For GPU 1060 or 730 install
#	 CUDA 11.4




######################################################
############### BB. anythingLLM ######################
######################################################


# 5.0 Download anythingLLM.exe from here:
#     https://useanything.com/download
#	And put it inside a folder.

# 5.1 Right click on it and 'Run as Administrator'
#	Allow it to run/install
#	Get Started screen will come.
#	Minimize it

# 5.2 Start chromadb. Issue the following two commands
#     in (wsl) Ubuntu. After the command issue, keep the terminal
#	open:

		$ conda activate langchain
		$ chroma run --path /home/ashok/Documents/chroma

#		You should also have an sqlite3 database in chroma folder.



# 5.3 Begin configuring anythingLLM  as:	
	
#	a. Click Get Started
#	b. Select Ollama as LLM Preference
#	c. Write Ollama base URL, as:  http://127.0.0.1:11434
#		Just writing '127.0.0.1:11434' would not work. http://
#			has to be written also.
#	d. Press Tab and under Chat model selection,
#	     you should see a list of installed models installed. Select one of them
#	f. Under Token Context Window, set token size of 4096.
	   (By default, Ollama uses a context window size of 2048 tokens.
	    It can be change: See this link of FAQ under ollama/docs/faq.md:
	    https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size )
#	g. In the next screen, select 'AnythingLLMEmbdder'
#	h. In the next screen select LanceDB
#		(see below for using chromadb)
#	i  Click Next again
#	j. Select 'For education', click Next
#	h. Give a Workspace name. AnythingLLM can have
#	i.  multiple workspaces for difft document types.
#	Done


# 5.4  Using chromadb vector database:
#      Ref: https://docs.trychroma.com/usage-guide#running-chroma-in-clientserver-mode
#
#	In anythingLLM, down below on the right, Click Settings icon ('spanner'). Then in
#	 the left panel click 'Vector Database' just write the chroma URL, as:
#
#			http://127.0.0.1:8000 and leave the other two fields blank
#			and save changes. No need to specify API key or token.
#

# Next click AnythingLLM hyperlink at the top. Click on your Workspace name and then enter a message.
#	You should get a response.


######################################
########## DD. NVIDIA driver ##########
############### Problems ##############
#######################################

# 7.0
	It is possible that after privateGPT installs CUDA
	Or you have installed CUDA, then mutiple NVIDIA
	drivers (not one) could be installed. It is also
	possible that when you reboot Ubuntu, screen resolution
	changes rather increases ao that everything is very small.
	Then, in Ubuntu search and open Software & Updates--> Resources (tab)
	Select that minimum driver for your purposes. For example, for CUDA 11.4
	driver version 470 is OK. Higher versions, even though available,
	create problems. It is trial-and-error. Select a driver and 
	reboot to see if it is OK.
	
	Min driver needed?
		See this link for CUDA version and min driver needed:
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html
		OR,
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html#minor-version-compatibility

#####################################
########## EE. Ingestion ############
################ AND ################
############# GPU Usage #############
#####################################


#8.1 Upload a big pdf file:
#			Run in a separate terminal command:
#
#				 nvidia-smi
#
# 			Under GPU-Util one finds GPU utilization
#			increasing from 0% to 33% or some other percentage.

# 8.2 Multiple pdf files upload:
#			Multiple pdf files can be uploaded at the same time.
			But GPU usage increases.
#

# 8.3 Screenshots of nvidia-smi commands:

				
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   47C    P5    13W /  N/A |    817MiB /  6044MiB |     33%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama              61MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                483MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+

## 30%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:17:50 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   62C    P2    49W /  N/A |   1277MiB /  6044MiB |     30%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                521MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


## 52%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:18:49 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   64C    P2    58W /  N/A |   1279MiB /  6044MiB |     52%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                523MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


------------------------
System Prompt Examples:
-------------------------

	The system prompt can effectively provide your chat bot specialized roles,
	and results tailored to the prompt you have given the model. Examples of 
	system prompts can be be found here: ChatGPT-3.5 Roles:
	https://www.w3schools.com/gen_ai/chatgpt-3-5/chatgpt-3-5_roles.php

	Some interesting examples to try include:

    		You are -X-. You have all the knowledge and personality of -X-. Answer
    		as if you were -X- using their manner of speaking and vocabulary.
        
        	Example: You are Shakespeare. You have all the knowledge and personality
        		 of Shakespeare. Answer as if you were Shakespeare using their manner
        		 of speaking and vocabulary.

			You are an expert (at) -role-. Answer all questions using your expertise
			on -specific domain topic-.

        	Example: You are an expert software engineer. Answer all questions using your 
        		 expertise on Python.

			You are a -role- bot, respond with -response criteria needed-. If 
			no -response criteria- is needed, respond with -alternate response-.
			
        	Example: You are a grammar checking bot, respond with any grammatical corrections
        		 needed. If no corrections are needed, respond with “verified”.
        		 
 	 
        		 


############ DONE #################






# 3.2 Syntax to remove conda env -- myenv

        conda remove -y --name langchain --all
