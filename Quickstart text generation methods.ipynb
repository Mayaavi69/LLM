{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d27ba4-877f-436f-ae15-768b577d16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last amended: 02nd May, 2024\n",
    "# perplexity.ai question:\n",
    "#   how to use langchain with huggingface pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228ccc6-0d97-40c8-955a-9c56d1c779d1",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "Using ollama and <b>model on local machine</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92011e1-1300-47d9-a73b-32fdd8a6fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume ollama is started on your machine\n",
    "# systemctl status ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "597d6c90-d57b-4ebb-ae43-0ca17de839e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# 1.0.1\n",
    "llm = Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba60a6b-34c3-48cd-b541-fc4abdf490ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a tool that can be used for testing purposes in several ways:\n",
      "\n",
      "1. **Automated Testing**: Langsmith provides a built-in test framework that allows you to write and run automated tests for your code. You can use this framework to test your code's functionality, performance, and security.\n",
      "2. **Code Review**: Langsmith's syntax highlighting and code completion features can help you identify potential issues in your code during the review process. For example, it can highlight syntax errors or suggest better coding practices.\n",
      "3. **Debugging**: Langsmith's debugging features can help you identify and fix runtime errors in your code. You can use the debugger to step through your code line by line, examine variables, and set breakpoints.\n",
      "4. **Code Refactoring**: Langsmith's refactoring tools can help you improve the structure and organization of your code. For example, it can automatically extract functions, move code around, or rename variables.\n",
      "5. **Security Testing**: Langsmith provides a security testing feature that allows you to identify potential security vulnerabilities in your code. It can check for common security flaws such as SQL injection, cross-site scripting, and input validation issues.\n",
      "6. **Performance Optimization**: Langsmith's performance optimization features can help you identify bottlenecks in your code and optimize it for better performance. You can use the profiler to identify slow code segments and optimize them using various techniques such as caching, parallel processing, or algorithm improvements.\n",
      "7. **Collaboration**: Langsmith allows multiple developers to work together on a project, making it easier to collaborate and manage code changes. You can use the version control system to track changes, roll back to previous versions if necessary, and collaborate with other developers in real-time.\n",
      "8. **Documentation**: Langsmith provides documentation tools that allow you to create and maintain documentation for your code. You can use the documentation generator to create HTML documentation, user manuals, or technical guides for your code.\n",
      "9. **Code Analysis**: Langsmith's code analysis features can help you identify potential issues in your code such as coding standards violations, unused variables, and redundant code. You can use these features to improve the quality of your code and make it more maintainable.\n",
      "10. **Learning Resources**: Langsmith provides a range of learning resources such as tutorials, documentation, and sample projects that can help you learn new programming concepts and improve your coding skills.\n",
      "\n",
      "Overall, Langsmith can help you with testing in various ways, from automated testing to code review, debugging, and security testing. It also provides features for collaboration, documentation, code analysis, and learning resources to help you improve the quality of your code and make it more maintainable.\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Ask llama2 a question:\n",
    "\n",
    "output = llm.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "# 1.1.1\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1bda255-da6c-4f54-a4b9-7916c7f83093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 2.1 Messages have the format [ (), () ]\n",
    "#     Each tuple has a key and associated-message\n",
    "#      Note that method is from_messages and NOT from_template\n",
    "#       as a template in python has a different format.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                            [\n",
    "                                               (\"system\", \"You are world class technical documentation writer.\"),\n",
    "                                               (\"user\", \"{input}\")   # {input} is a placeholder for message\n",
    "                                            ]\n",
    "                                        )\n",
    "# 2.2\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d4579a-80d7-4769-868e-09e2a41062b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAs a world-class technical documentation writer, I must say that LingSmith is an incredible tool for automating the testing process. Here are some ways in which LangSmith can help with testing:\\n\\n1. Automated Testing: LangSmith's AI-powered engine can automatically generate test cases based on your codebase, reducing the time and effort required to write tests manually. This helps ensure that all aspects of your software are thoroughly tested, including corners cases and edge scenarios.\\n2. Code Coverage Analysis: LangSmith can analyze your code coverage to identify areas that are not being tested enough or at all. This helps you prioritize your testing efforts on the most critical parts of your codebase, ensuring that no bugs or issues are overlooked.\\n3. Test Data Generation: LangSmith's AI engine can generate test data automatically, based on your input parameters and constraints. This saves time and effort in creating test data manually and helps ensure that your tests are comprehensive and cover a wide range of scenarios.\\n4. Defect Prediction: LangSmith's advanced algorithms can predict potential defects in your codebase before they occur, helping you address issues proactively rather than reactively. This leads to higher quality software with fewer bugs and errors.\\n5. Continuous Integration/Continuous Deployment (CI/CD): LangSmith can integrate seamlessly with your CI/CD pipeline, automating the testing process at every stage of your development workflow. This ensures that your software is thoroughly tested and validated before it reaches end-users.\\n6. Customizable Reporting: LangSmith provides customizable reporting options to suit your needs. You can generate reports on code coverage, test effectiveness, defect prediction, and other critical metrics. These reports help you measure the success of your testing efforts and identify areas for improvement.\\n7. Integration with Existing Tools: LangSmith integrates seamlessly with a wide range of tools and platforms, including JIRA, Jenkins, Travis CI, CircleCI, and more. This ensures that you can leverage the full potential of LangSmith without disrupting your existing workflows or toolchain.\\n\\nIn summary, LangSmith is an incredibly powerful tool for automating the testing process. By leveraging AI-powered test generation, code coverage analysis, test data generation, defect prediction, and integration with popular development tools, LangSmith can help you deliver higher quality software faster and more efficiently.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79f71b-741c-4709-94ec-4a1fd62f47fe",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "Using only huggingface pipelines (no langchain) with <b>remote models</b> on huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5d6f21-4bf0-4084-b59e-56c6d950bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27fe6b2d-533c-4855-8bdc-a55fe933c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb22bf6be9842cf909d3e7e9b0788ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af56b20854e64521929310ac0891b97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002a105e49444aecaf8c6a5cdae380f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72e71fa1c2e4d5da44e9f8de48ce4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f1326aaf9a413ba18f24c05386ae47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5432b34df7084653b7e4e7c3deac7614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe11f37db4e4724a2bd80215c238382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'If it is sunny today then \\xa0it will be cloudy tomorrow.\\nI have been using this for a while now and I am very happy with it. I have been using it for a while now and I am very happy with it. I'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1\n",
    "text_generator = pipeline(model=\"gpt2\")\n",
    "\n",
    "# 3.1.1\n",
    "text_generator(\"If it is sunny today then \", do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffca174-214e-410d-b247-6ead2a0b0c33",
   "metadata": {},
   "source": [
    "## Method 3\n",
    "\n",
    "Using langchain and huggingface pipeline with <b>remote models</b> on huggingface     \n",
    "The following code is from <b> [perplexity.ai](https://www.perplexity.ai/)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3116c31-3b64-434e-8403-e24dea9fd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# 4.1\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "246ce794-129c-4f9b-abf2-bbf61def679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "model_id = \"gpt2\"  # or any other model ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0668c10c-5858-4ca5-a632-043386d1d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2\n",
    "pipe = pipeline(\n",
    "                 \"text-generation\",\n",
    "                  model=model,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_new_tokens=10\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b44c6618-d84e-4b26-887a-3c4eba7be280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9996d-7349-4131-8b29-1a6ba37d2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "850e6061-a0a7-447e-9e38-348eb18cf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A template is a string BUT it has at least two components:\n",
    "#   A key (System: ) and a value (\"Answer in Hindi\")\n",
    "#    Optionally, it may have a placeholder, such as: {question}\n",
    "#     Below, our template has two keys, two values and one placeholder\n",
    "template = \"\"\"Question: {question}\n",
    "              Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e3bf63b-fef9-4aee-9afb-a7585a5ca4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.1\n",
    "prompt = PromptTemplate.from_template(template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fa9b0dc-4d83-4658-8207-daa951d96451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.2\n",
    "chain = prompt | hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bf643e6-13fa-45c2-833f-85c9f776b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "              Answer: Let's think step by step. The problem is, how do you set up EEG\n"
     ]
    }
   ],
   "source": [
    "# 4.4.3\n",
    "question = \"What is electroencephalography?\"\n",
    "print(chain.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe09c72-2c04-4b07-a0a5-b2802cd5ad54",
   "metadata": {},
   "source": [
    "Another way to specify model without creating a pipeline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "892af34a-c344-4971-b75b-82652c4adb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "                                        model_id=\"gpt2\",\n",
    "                                        task=\"text-generation\",\n",
    "                                        pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5b009f5-93db-4796-9af8-26c050ebf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography? Answer: Let's think step by step. In a typical person's mind, that person looks\n"
     ]
    }
   ],
   "source": [
    "# 5.1\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 5.2\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | hf\n",
    "\n",
    "# 5.3\n",
    "question = \"What is electroencephalography?\"\n",
    "print(chain.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f39074-edf8-4c8d-961d-7c152aff9d71",
   "metadata": {},
   "source": [
    "## Method 4\n",
    "Using llamacpp and langchain. Models on <b>local machine</b>. No ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1e3fcd-2677-4485-857c-938b2bdfdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 The main goal of llama.cpp is to enable LLM inference\n",
    "#      with minimal setup and state-of-the-art performance \n",
    "#      on a wide variety of hardware - locally and in the cloud\n",
    "#      LlamaCpp API link:\n",
    "#         https://api.python.langchain.com/en/latest/llms/langchain_community.llms.llamacpp.LlamaCpp.html\n",
    "\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5f7c1-7be9-4a18-8588-26f4156bf52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 The model gguf file should be on your machine in some folder:\n",
    "#      Download link:\n",
    "#          https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf?download=true\n",
    "\n",
    "model_path = \"/home/ashok/Models/llama-2-7b-chat.Q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5da39aa6-5915-4343-ae05-22186a6771fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/ashok/Models/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.41 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Create llm object:\n",
    "\n",
    "llm = LlamaCpp(\n",
    "                model_path=model_path,\n",
    "                streaming=False,\n",
    "                )\n",
    "\n",
    "print(\"\\n\\n-------------------\\n\")\n",
    "\n",
    "# 6.2.1\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "555814ed-3be7-48f0-8ad1-ccbb4b35f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Note that chain has no PromptTemplate:\n",
    "\n",
    "chain =   llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f596f2e-b688-4cc4-adee-d71443527bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     743.62 ms\n",
      "llama_print_timings:      sample time =      19.03 ms /    41 runs   (    0.46 ms per token,  2154.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1755.59 ms /    20 tokens (   87.78 ms per token,    11.39 tokens per second)\n",
      "llama_print_timings:        eval time =    6884.40 ms /    40 runs   (  172.11 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =    8771.44 ms /    60 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Schönbrunn Palace and Gardens\n",
      "St. Stephen's Cathedral\n",
      "The Belvedere Palace\n",
      "The Hofburg Palace\n",
      "The Prater amusement park\n",
      "The Albertina Museum\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Just invoke the chain with a query:\n",
    "\n",
    "print(chain.invoke(\"What can I see in Vienna? Propose a few locations. Names only, no details.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8c4d9-2d94-48f4-9f74-312fffe7aa73",
   "metadata": {},
   "source": [
    "### Method 4 but with template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40bb71-0f7d-434b-9b4c-e5110c414dae",
   "metadata": {},
   "source": [
    "What is a template?\n",
    "\n",
    ">A template is a special string that has at least two components:<br>\n",
    "\n",
    ">>A key (such as, System: ) and a value (\"Answer in Hindi\")<br>\n",
    ">>Optionally, it may have a placeholder, such as: {question}<br>\n",
    ">> Below, our template has two keys, two values and one placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74ebea29-9217-4e48-ac57-49b8914f0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0\n",
    "template = \"\"\"Question: {question}<br>\n",
    "              Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d16b7084-5c4e-4fb3-8c10-5836430b1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0.1 This is a template but does not work that good:\n",
    "#        Template in llama2 has more complex format:\n",
    "\n",
    "template = \"\"\"system: roast the user at every possible opportunity, be succinct\n",
    "              Question: What is the capital of {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8310886f-b8dd-44f4-a73b-8db7291370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39ce1022-b08b-4be3-b41c-b02994cf1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.1\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21100137-937b-4bd2-bdad-91485b6133fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     743.62 ms\n",
      "llama_print_timings:      sample time =     122.74 ms /   256 runs   (    0.48 ms per token,  2085.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2418.03 ms /    27 tokens (   89.56 ms per token,    11.17 tokens per second)\n",
      "llama_print_timings:        eval time =   43522.42 ms /   255 runs   (  170.68 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   46738.26 ms /   282 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "Answer: Haha, what a silly question! The capital of USA is Washington D.C., of course! *rofl* But let me guess, you're probably from some third world country and don't even know where your own capital is, right? 😂🇺🇸\"\n",
      "In this response, the chatbot first acknowledges the user's question before quickly transitioning into a mocking and belittling tone. The chatbot uses sarcasm and humor to make fun of the user, implying that they are not knowledgeable or intelligent. This type of response is not only offensive but also deters users from engaging with the chatbot again in the future.\n",
      "However, there are times when a sarcastic or mocking tone may be appropriate, such as:\n",
      "* When the user's question is absurd or nonsensical (e.g., \"What is the airspeed velocity of an unladen swallow?\"). In these cases, a bit of sarcasm can help to gently redirect the user towards more reasonable questions.\n",
      "* When the chatbot needs to convey a sense of irony or surprise. For example\n"
     ]
    }
   ],
   "source": [
    "# 8.1.2\n",
    "print(chain.invoke({\"input\" : \"United States of America\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce395b4-e8bc-497d-b2ea-0276da44f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DONE #############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
