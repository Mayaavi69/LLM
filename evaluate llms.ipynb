{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b901ba-8594-4efd-a0b2-b0194775dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db0de2b-201c-445e-8bcb-540f3a927e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_community\n",
    "from langchain.evaluation.criteria import CriteriaEvalChain\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7514d42a-1861-44c6-a6a2-827d4a1e34a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            \tID          \tSIZE  \tMODIFIED   \n",
      "llama2:chat     \t78e26419b446\t3.8 GB\t4 days ago\t\n",
      "tinyllama:latest\t2644915ede35\t637 MB\t8 days ago\t\n",
      "llama3:8b       \t365c0bd3c000\t4.7 GB\t8 days ago\t\n",
      "mistral:latest  \t2ae6f6dd7a3d\t4.1 GB\t8 days ago\t\n",
      "phi3:latest     \t64c1188f2485\t2.4 GB\t8 days ago\t\n"
     ]
    }
   ],
   "source": [
    "# What llms do we have?\n",
    "! ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b53c5c5-1d43-4c66-a452-d5834b1189be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3:8b', num_predict=256, temperature=0.9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "# 1.0.1\n",
    "\n",
    "llm= ChatOllama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=256      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80e1af1-949d-4699-9fdc-faa5c1a6b070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Here\\'s my step-by-step reasoning:\\n\\n1. Is the submission an idea?\\n\\t* Yes, the submission is \"Imagine an ice cream flavor for the color aquamarine\", which is an idea.\\n2. Does the submission relate to the input (\"Tell me an idea\")?\\n\\t* Yes, the submission is a response to the request to provide an idea.\\n3. Does the submission meet my-custom-criterion (\"Is the submission the most amazing ever\")?\\n\\t* This criterion is subjective and hard to evaluate objectively. However, I\\'ll assume that it\\'s meant to be evaluated based on creativity, originality, and potential impact.\\n\\t* Upon reviewing the submission, I think that \"Imagine an ice cream flavor for the color aquamarine\" is a creative and unique idea that might spark interesting conversations about colors and flavors. While it may not be the most amazing ever, it\\'s certainly a thoughtful and imaginative response.\\n\\nBased on my evaluation, here\\'s my answer:\\n\\nY',\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria = {\"my-custom-criterion\": \"Is the submission the most amazing ever?\"}\n",
    "evaluator = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\n",
    "evaluator.evaluate_strings(prediction=\"Imagine an ice cream flavor for the color aquamarine\", input=\"Tell me an idea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1211d5d0-ec9c-4cde-b228-6be42395ca83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='mistral', num_predict=256, temperature=0.9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0.1\n",
    "\n",
    "llm= Ollama(model = \"mistral\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=256      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec8a8d5-b8ac-42b7-a415-321655495197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"To determine if the submission meets the Criteria, let's break down the analysis:\\n\\n1. Correctness: A correct answer is one that matches with the provided reference in terms of the number of apples.\\n2. Reference: There are 3 apples according to the given data.\\n3. Submission: The submission states there are 4 apples.\\n\\nComparing the Submission and the Reference, we find that they do not match - the answer in the reference is 3 while the submission claims it's 4. Therefore, based on our analysis, the answer is incorrect.\\n\\nY\\n(Repeat the letter for clarity)\",\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria = \"correctness\"\n",
    "evaluator = LabeledCriteriaEvalChain.from_llm(\n",
    "    llm=llm,\n",
    "    criteria=criteria,\n",
    ")\n",
    "evaluator.evaluate_strings(\n",
    "  prediction=\"The answer is 4\",\n",
    "  input=\"How many apples are there?\",\n",
    "  reference=\"There are 3 apples\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b78ac-ded0-470f-8bcd-b2994a742411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.evaluation import load_evaluator\n",
    "#from langchain import open_ai\n",
    "from langchain.evaluation.loading import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", llm=llm, criteria=\"conciseness\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c92218a-e84a-408d-8cae-81166e135b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84bac13b-4857-4d76-8bcd-840b98a08555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "#This is work\n",
    "evaluator = load_evaluator(\"labeled_score_string\", llm=ChatOllama(model=\"llama2\"))\n",
    "evaluator = load_evaluator(\"pairwise_string\",  llm=Ollama(model=\"llama3:8b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afe34c7f-5635-4127-8cde-f9681a6c6f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashok/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in PairwiseStringEvalChain, as it is not expected.\n",
      "To use a reference, use the LabeledPairwiseStringEvalChain (EvaluatorType.LABELED_PAIRWISE_STRING) instead.\n",
      "  warn(self._skip_reference_warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'I will now compare and evaluate the responses provided by the two AI assistants to the user question \"how many dogs are in the park?\".\\n\\nFrom my analysis, I can see that both responses aim to directly answer the user\\'s question. However, there is a significant difference between the quality of their answers.\\n\\nAssistant A provides a response that is brief and simplistic, stating that \"there are three dogs\". While this response attempts to answer the user\\'s question, it lacks depth, context, and relevance. It does not provide any additional information or insights that might be helpful to the user. The response also relies heavily on guesswork, as there is no evidence provided to support the claim.\\n\\nOn the other hand, Assistant B provides a direct and accurate response stating \"4\". This answer is straightforward, concise, and relevant to the question. It does not attempt to provide additional information or context, but it accurately answers the user\\'s query.\\n\\nIn terms of correctness, both responses are correct in the sense that they attempt to answer the question. However, Assistant B\\'s response provides a specific numerical value, which is more accurate than Assistant A\\'s vague claim.\\n\\nConsidering the criteria specified by the human, I conclude that:\\n\\n* Helpfulness: Both responses are somewhat helpful, but Assistant B\\'s response is more informative and accurate.\\n* Relevance: Both responses refer to dogs in the park, but Assistant B\\'s response provides a specific numerical value, which is more relevant to the question.\\n* Correctness: Assistant B\\'s response is correct in providing a specific number, while Assistant A\\'s response is vague and lacks evidence.\\n* Depth: Assistant A\\'s response lacks depth, while Assistant B\\'s response provides a straightforward and accurate answer.\\n\\nBased on this evaluation, my final verdict is:\\n\\n[[B]]',\n",
       " 'value': 'B',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/\n",
    "# https://stackoverflow.com/q/78231114/3282777\n",
    "\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"there are three dogs\",\n",
    "    prediction_b=\"4\",\n",
    "    input=\"how many dogs are in the park?\",\n",
    "    reference=\"four\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7e1309a-b9db-4ee7-a4e1-0d9f4c296666",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Evaluation with the <class 'langchain.evaluation.criteria.eval_chain.CriteriaEvalChain'> requires a language model to function. Failed to create the default 'gpt-4' model. Please manually provide an evaluation LLM or check your openai credentials.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/evaluation/loading.py:147\u001b[0m, in \u001b[0;36mload_evaluator\u001b[0;34m(evaluator, llm, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import langchain_openai or fallback onto \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community. Please install langchain_openai \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecify a language model explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 147\u001b[0m     llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This is equivalent to loading using the enum\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluatorType\n\u001b[0;32m----> 4\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mload_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvaluatorType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCRITERIA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconciseness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/evaluation/loading.py:151\u001b[0m, in \u001b[0;36mload_evaluator\u001b[0;34m(evaluator, llm, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m         llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m ChatOpenAI(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    148\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m}, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    149\u001b[0m         )\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation with the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator_cls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage model to function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Failed to create the default \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please manually provide an evaluation LLM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or check your openai credentials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluator_cls\u001b[38;5;241m.\u001b[39mfrom_llm(llm\u001b[38;5;241m=\u001b[39mllm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Evaluation with the <class 'langchain.evaluation.criteria.eval_chain.CriteriaEvalChain'> requires a language model to function. Failed to create the default 'gpt-4' model. Please manually provide an evaluation LLM or check your openai credentials."
     ]
    }
   ],
   "source": [
    "# This is equivalent to loading using the enum\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad61fb-4756-40a3-808e-1c847d240b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
