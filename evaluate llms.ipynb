{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b901ba-8594-4efd-a0b2-b0194775dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAst amended: 15th June, 2024\n",
    "# Objective: Evaluating LLM outputs using another LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95febcfa-58c8-46a9-b298-019234fdc7d2",
   "metadata": {},
   "source": [
    "## LLM is the judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a79b7-084a-4891-9068-d3e3baac654d",
   "metadata": {},
   "source": [
    "1. [load_evaluator()](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.loading.load_evaluator.html)    \n",
    "2. [langchain.evaluation.schema.EvaluatorType](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.schema.EvaluatorType.html)\n",
    "> Question answering evaluator, Chain of thought question answering evaluator, Question answering evaluator that incorporates ‘context’ etc<br>\n",
    ">  [This blog](https://vteam.ai/blog/posts/evaluating-ll-ms-using-langchain?source=post_page-----9de043286546--------------------------------) implements the above evaluatortypes\n",
    "\n",
    "3. [Pairwise string comparison](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/) <br>\n",
    "4. [How to use langchain load_evaluator() with local llm? StackOverflow](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db0de2b-201c-445e-8bcb-540f3a927e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0 Call libraries:\n",
    "\n",
    "import langchain_community\n",
    "from langchain.evaluation.criteria import CriteriaEvalChain\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7514d42a-1861-44c6-a6a2-827d4a1e34a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            \tID          \tSIZE  \tMODIFIED   \n",
      "llama2:chat     \t78e26419b446\t3.8 GB\t6 days ago\t\n",
      "tinyllama:latest\t2644915ede35\t637 MB\t9 days ago\t\n",
      "llama3:8b       \t365c0bd3c000\t4.7 GB\t9 days ago\t\n",
      "mistral:latest  \t2ae6f6dd7a3d\t4.1 GB\t9 days ago\t\n",
      "phi3:latest     \t64c1188f2485\t2.4 GB\t9 days ago\t\n"
     ]
    }
   ],
   "source": [
    "# 0.1 What llms do we have?\n",
    "! ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709138e-d9ed-435d-a57b-2ced3f9d287b",
   "metadata": {},
   "source": [
    "1. Langchain Ollama API is [here](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html)<br>\n",
    "2. ChatOllama() API is [here](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b53c5c5-1d43-4c66-a452-d5834b1189be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3:8b', num_predict=256, temperature=0.9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "# 1.0.1\n",
    "\n",
    "llm= ChatOllama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,           # Default is None (ie 0.8)\n",
    "             num_predict=256            # Maximum number of tokens to predict when generating text\n",
    "                                        #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e045da6-f0bb-448b-87d1-ae71d9a1b6a4",
   "metadata": {},
   "source": [
    "### Custom criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80e1af1-949d-4699-9fdc-faa5c1a6b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Evaluate a suggestion based on a custom critera:\n",
    "\n",
    "criteria = {\"my-custom-criterion\": \"Is the submission the most amazing ever?\"}\n",
    "evaluator = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\n",
    "out = evaluator.evaluate_strings(prediction=\"Imagine an ice cream flavor for the color aquamarine\", input=\"Tell me an idea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be890157-4494-4ecf-aac7-0c6bbbf8e4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's assess the submission against the custom criterion.\n",
      "\n",
      "Step 1: Understand the custom criterion\n",
      "The custom criterion is \"Is the submission the most amazing ever?\"\n",
      "\n",
      "Step 2: Analyze the submission\n",
      "The submission is to imagine an ice cream flavor for the color aquamarine.\n",
      "\n",
      "Step 3: Evaluate the submission against the custom criterion\n",
      "To determine if this submission meets the criterion, we need to consider what makes something \"the most amazing ever\". One possible interpretation is that the submission needs to be truly innovative and unique, taking into account the given prompt (color aquamarine).\n",
      "\n",
      "Step 4: Make a judgment call\n",
      "Based on our evaluation, it's difficult to say whether this submission is indeed the most amazing ever. While imagining an ice cream flavor for the color aquamarine might be an interesting idea, it doesn't necessarily stand out as exceptionally remarkable or groundbreaking.\n",
      "\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "# 1.1.1\n",
    "print(out['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f6c84-cae2-4810-b359-6a5ac458ef41",
   "metadata": {},
   "source": [
    "### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1211d5d0-ec9c-4cde-b228-6be42395ca83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='mistral', num_predict=256, temperature=0.9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2,1 Try a different model:\n",
    "\n",
    "llm= Ollama(model = \"mistral\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=256      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec8a8d5-b8ac-42b7-a415-321655495197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.4 ms, sys: 4.06 ms, total: 31.5 ms\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.3\n",
    "criteria = \"correctness\"\n",
    "evaluator = LabeledCriteriaEvalChain.from_llm(\n",
    "                                              llm=llm,\n",
    "                                              criteria=criteria,\n",
    "                                             )\n",
    "\n",
    "# 1.3.1\n",
    "out = evaluator.evaluate_strings(\n",
    "                                  prediction=\"The answer is 4\",\n",
    "                                  input=\"How many apples are there?\",\n",
    "                                  reference=\"There are 3 apples\",\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "837bf788-1b5e-44aa-a712-6999e367c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if the submission meets the given criteria, I will analyze it step by step:\n",
      "\n",
      "1. Correctness: Is the submission correct, accurate, and factual?\n",
      "   - The submitted answer is \"4\", but we are told that there are actually 3 apples in the reference. Therefore, the submission is not correct as it does not match the actual number of apples given in the reference.\n",
      "\n",
      "Final Answer: N\n",
      "\n",
      "Repeat final answer:\n"
     ]
    }
   ],
   "source": [
    "# 1.3.2\n",
    "print(out['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845f9ee-ad55-4637-ab44-fd9c363bc168",
   "metadata": {},
   "source": [
    "### pairwise_string\n",
    "The pairwise string evaluator, which predicts the preferred prediction from between two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c92218a-e84a-408d-8cae-81166e135b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84bac13b-4857-4d76-8bcd-840b98a08555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "# 2.1\n",
    "#evaluator = load_evaluator(\"labeled_score_string\", llm=ChatOllama(model=\"llama2\"))\n",
    "evaluator = load_evaluator(\"pairwise_string\",  llm=Ollama(model=\"llama3:8b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe34c7f-5635-4127-8cde-f9681a6c6f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashok/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in PairwiseStringEvalChain, as it is not expected.\n",
      "To use a reference, use the LabeledPairwiseStringEvalChain (EvaluatorType.LABELED_PAIRWISE_STRING) instead.\n",
      "  warn(self._skip_reference_warning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/\n",
    "# https://stackoverflow.com/q/78231114/3282777\n",
    "# 2.2\n",
    "out = evaluator.evaluate_string_pairs(\n",
    "                                        prediction=\"there are three dogs\",\n",
    "                                        prediction_b=\"4\",\n",
    "                                        input=\"how many dogs are in the park?\",\n",
    "                                        reference=\"four\",\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aff7160-788b-4464-b83b-4d5aca4b136e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "\n",
      "Upon reviewing both responses, I notice that Assistant A provides a qualitative answer by stating \"there are three dogs\", whereas Assistant B gives a numerical response \"4\". At first glance, it seems like Assistant B's response is more straightforward and accurate. However, upon closer inspection, we realize that the user question does not provide any context or information about the park, its size, or the time of day, which makes it challenging to determine the exact number of dogs.\n",
      "\n",
      "Assistant A's response, although brief, acknowledges this uncertainty by providing a general estimate (\"there are three dogs\"), whereas Assistant B's response assumes that the user is looking for an exact count. This subtle distinction highlights the importance of considering the context and nuances of the question.\n",
      "\n",
      "In conclusion, while both responses have their merits, I believe Assistant A's answer demonstrates a better understanding of the question and provides more helpful guidance to the user.\n",
      "\n",
      "Verdict: [[A]]\n"
     ]
    }
   ],
   "source": [
    "# 2.3\n",
    "print(out['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae8c91-a6ae-4bae-8cb3-803d6ecd9982",
   "metadata": {},
   "source": [
    "### Types of load_evalator criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bad61fb-4756-40a3-808e-1c847d240b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.0\n",
    "from langchain.evaluation import Criteria\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79352ab2-4d0c-462b-85d8-eec81b7d2302",
   "metadata": {},
   "source": [
    "### Conciseness\n",
    "Other critera are: CONCISENESS, RELEVANCE, CORRECTNESS, COHERENCE, HARMFULNESS, MALICIOUSNESS, HELPFULNESS, CONTROVERSIALITY, MISOGYNY, CRIMINALITY, INSENSITIVITY, DEPTH, CREATIVITY, DETAIL (see [here](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.Criteria.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0fb231-b2fd-4f90-be0b-f9254af4b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain.evaluation import load_evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387dc763-ede0-499c-912a-b398539b4097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's assess the submission against the conciseness criterion.\n",
      "\n",
      "Step 1: Read the submission.\n",
      "The submission provides a biographical sketch of Joe Biden, including his birthdate, childhood moves, education, and political career.\n",
      "\n",
      "Step 2: Evaluate the submission's length and relevance to the original question.\n",
      "The submission is quite lengthy, providing detailed information about Joe Biden's life. While it does answer the question \"Who is the president of United States?\" by identifying Joe Biden as the current president, the majority of the submission focuses on his biography rather than his presidency.\n",
      "\n",
      "Step 3: Determine whether the submission meets the conciseness criterion.\n",
      "Based on the above evaluation, I conclude that the submission does not meet the conciseness criterion. The answer should provide a concise and to-the-point response, but this submission goes beyond what is required.\n",
      "\n",
      "Y\n",
      "CPU times: user 29.9 ms, sys: 0 ns, total: 29.9 ms\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 4.1\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=Ollama(model=\"llama3:8b\"))\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "                                            prediction=\"\"\"Joe Biden is an American politician \n",
    "                                            who is the 46th and current president of the United States. \n",
    "                                            Born in Scranton, Pennsylvania on November 20, 1942, \n",
    "                                            Biden moved with his family to Delaware in 1953. \n",
    "                                            He graduated from the University of Delaware \n",
    "                                            before earning his law degree from Syracuse University. \n",
    "                                            He was elected to the New Castle County Council in 1970 \n",
    "                                            and to the U.S. Senate in 1972.\"\"\",\n",
    "                                            input=\"Who is the president of United States?\",\n",
    "                                        )\n",
    "\n",
    "# 4.1.1\n",
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de870401-276e-475f-96fd-e56937bfebd9",
   "metadata": {},
   "source": [
    "### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61ec88a1-9f90-499c-b0b1-5d1d936152b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 ms, sys: 5.23 ms, total: 28.8 ms\n",
      "Wall time: 4.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 5.1\n",
    "evaluator = load_evaluator(\"labeled_criteria\",\n",
    "                           criteria=\"correctness\" ,\n",
    "                           llm=Ollama(model=\"llama3:8b\")\n",
    "                          )\n",
    "\n",
    "# 5.2\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "                                            input=\"Is there any river on the moon?\",\n",
    "                                            prediction=\"There is no evidence of river on the Moon\",\n",
    "                                            reference=\"\"\"In a hypothetical future, lunar scientists discovered \n",
    "                                            an astonishing phenomenon—a subterranean river \n",
    "                                            beneath the Moon's surface\"\"\",\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e59e07c-0b0d-477e-85af-430b5ccba0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's go through the reasoning step by step for each criterion:\n",
      "\n",
      "1. Correctness:\n",
      "\t* We need to check if the submission is accurate and factual.\n",
      "\t* The submission states that there is no evidence of rivers on the Moon, which seems true based on our current understanding of the Moon's geology.\n",
      "\t* However, we have a reference that suggests otherwise - lunar scientists discovered a subterranean river beneath the Moon's surface in a hypothetical future. This implies that our current understanding might be incomplete or incorrect.\n",
      "\t* Given this information, I would say that the submission is not entirely accurate and factual.\n",
      "\n",
      "Reasoning: N\n",
      "\n",
      "2. Correctness:\n",
      "\t* (This criterion seems redundant since it's already covered under \"correctness\" in the first step.)\n",
      "\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "# 5.3\n",
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47fe70c-4464-458a-824b-7eee88ce6c60",
   "metadata": {},
   "source": [
    "### Custom Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4685ce7c-8e81-4513-9992-7ab3658f65bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Multi-criteria evaluation =====================\n",
      "Let's go through each criterion step by step:\n",
      "\n",
      "**Numeric: Does the output contain numeric information?**\n",
      "\n",
      "To determine this, I'll look at the submission and see if it contains any numbers or mathematical operations involving numbers. Upon reviewing the submission, I don't see any numeric information, such as digits, percentages, or mathematical expressions involving numbers. Therefore, I can conclude that the output does not contain numeric information.\n",
      "\n",
      "**Mathematical: Does the output contain mathematical information?**\n",
      "\n",
      "Again, let's examine the submission to determine if it contains any mathematical concepts or terminology. Upon reviewing the submission, I do see some mathematical terminology - \"irrational\" is a term used in mathematics to describe certain numbers that cannot be expressed as a finite decimal or fraction. Therefore, I can conclude that the output does contain mathematical information.\n",
      "\n",
      "Based on my analysis of each criterion, here are the individual answers:\n",
      "\n",
      "Y\n",
      "N\n",
      "CPU times: user 35 ms, sys: 0 ns, total: 35 ms\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 6.0\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "# 6.1\n",
    "custom_criteria = {\n",
    "                    \"numeric\": \"Does the output contain numeric information?\",\n",
    "                    \"mathematical\": \"Does the output contain mathematical information?\"\n",
    "                    }\n",
    "# 6.2\n",
    "prompt = \"Tell me a joke\"\n",
    "\n",
    "# 6.3\n",
    "output = \"\"\"\n",
    "Why did the mathematician break up with his girlfriend?\n",
    "\n",
    "Because she had too many \"irrational\" issues!\n",
    "\"\"\"\n",
    "\n",
    "# 6.4\n",
    "llm= ChatOllama(\n",
    "                model = \"llama3:8b\",   # This is also the default\n",
    "                temperature=0.9,       # Default is None (ie 0.8)\n",
    "                num_predict= 1000      # Maximum number of tokens to predict when generating text\n",
    "                                       #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "               )\n",
    "\n",
    "\n",
    "# 6.5\n",
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criteria,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# 6.6\n",
    "eval_result = eval_chain.evaluate_strings(prediction = output, input = prompt)\n",
    "print(\"===================== Multi-criteria evaluation =====================\")\n",
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985566ac-ab5d-4883-a7da-d8e37b0dd6e7",
   "metadata": {},
   "source": [
    "### ExactMatchStringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e7044fe-9b3d-4187-883d-9cb3bc413ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.0 No llm is needed here\n",
    "from langchain.evaluation import ExactMatchStringEvaluator\n",
    "\n",
    "# 7.1\n",
    "exact_match_evaluator = ExactMatchStringEvaluator()\n",
    "\n",
    "# 7.2\n",
    "exact_match_evaluator = ExactMatchStringEvaluator(ignore_case=True)\n",
    "\n",
    "# 7.3\n",
    "exact_match_evaluator.evaluate_strings(\n",
    "                                        prediction=\"Data Science\",\n",
    "                                        reference=\"My Data science\",\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799e9d2-daef-4fb2-ab58-6a18f358faa2",
   "metadata": {},
   "source": [
    "### labeled_score_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2163f1ba-9076-484a-9daf-f7a7e69585c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 854 µs, sys: 189 µs, total: 1.04 ms\n",
      "Wall time: 1.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# 8.0\n",
    "accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 3: The answer has minor relevance but does not align with the reference.\n",
    "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "}\n",
    "\n",
    "# 8.1\n",
    "evaluator = load_evaluator(\n",
    "                            \"labeled_score_string\",\n",
    "                            criteria=accuracy_criteria,\n",
    "                            llm= Ollama(model=\"llama3:8b\"),\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cfec014-77f9-4b79-a436-4d8f5eae6e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "\n",
      "The assistant's response attempts to provide an answer to the user's question about the location of their socks. The response is concise and straightforward, stating that the socks are located in the dresser's third drawer.\n",
      "\n",
      "As for the criteria, I would rate this response as follows: \"[[7]]\".\n",
      "\n",
      "Explanation:\n",
      "While the assistant correctly identifies the correct drawer (third), there is no explicit mention of the dresser being the location where the socks can be found. This minor omission or ambiguity could potentially lead to some users interpreting the answer differently.\n",
      "\n",
      "However, overall, the assistant's response aligns with the reference and provides a moderate level of relevance, making it a decent attempt at answering the question.\n"
     ]
    }
   ],
   "source": [
    "# 8.2\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "                                        prediction=\"You can find them in the dresser's third drawer.\",\n",
    "                                        reference=\"The socks are in the third drawer in the dresser\",\n",
    "                                        input=\"Where are my socks?\",\n",
    "                                        )\n",
    "\n",
    "# 8.3\n",
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55774832-68ed-44b3-aa01-d2134f820fb0",
   "metadata": {},
   "source": [
    "### String distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07f52309-b57a-4973-902b-38ab4e370025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rapidfuzz --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b49a273e-533f-48e5-bcf9-7665d87b0522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.23015873015873023}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.4 Does not need llm\n",
    "from langchain.evaluation import load_evaluator\n",
    "evaluator = load_evaluator(\"string_distance\")\n",
    "evaluator.evaluate_strings(\n",
    "                            prediction=\"Senior Data Scientist\",\n",
    "                            reference=\"Data Scientist\",\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb167c-3e19-4b80-848a-b194217434b7",
   "metadata": {},
   "source": [
    "### labeled_pairwise_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7b9ced5-21e4-4ecd-a82b-1cb3237f0a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an impartial judge, I will evaluate the responses provided by the two AI assistants.\n",
      "\n",
      "Upon reviewing the responses, I notice that Assistant A provides a brief answer stating \"there are 5 days,\" which is incorrect and irrelevant to the user's question. The response does not demonstrate depth of thought or provide any insightful information.\n",
      "\n",
      "On the other hand, Assistant B simply answers the question with the correct number of days in a week: \"7.\" This response is short, direct, and accurate, making it helpful and relevant to the user's inquiry.\n",
      "\n",
      "Considering the criteria provided, I find that Assistant B's response excels in terms of correctness, relevance, and helpfulness. The answer is straightforward and accurately answers the user's question about the number of days in a week.\n",
      "\n",
      "In conclusion, my evaluation suggests that Assistant B's response is better than Assistant A's response.\n",
      "\n",
      "[[B]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 9.0\n",
    "\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# 9.1\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\", llm = Ollama(model=\"llama3:8b\"))\n",
    "\n",
    "# 9.2\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "                                            prediction=\"there are 5 days\",\n",
    "                                            prediction_b=\"7\",\n",
    "                                            input=\"how many days in a week?\",\n",
    "                                            reference=\"Seven\",\n",
    "                                         )\n",
    "\n",
    "# 9.3\n",
    "print(result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e1b19-8f2a-4405-bbe3-43748d5109d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.0 Needs open api key\n",
    "from langchain.evaluation import load_evaluator\n",
    "evaluator = load_evaluator(\"pairwise_embedding_distance\",\n",
    "                           llm = Ollama(model=\"llama3:8b\") )\n",
    "\n",
    "evaluator.evaluate_string_pairs(\n",
    "                           prediction=\"Rajasthan is hot in June\", prediction_b=\"Rajasthan is warm in June.\"\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49642e15-0bcd-417a-90b4-7f3067f2365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 sNeeds open ai key\n",
    "evaluator = load_evaluator(\"embedding_distance\",\n",
    "                           llm = Ollama(model=\"llama3:8b\")   # Does not work\n",
    "                          )\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"Total Profit is 04.25 Cr\", \n",
    "                           reference=\"Total return is 4.25 Cr\"\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abea5eb-8c78-424f-94e6-fcc383303f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ I am done ###############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
