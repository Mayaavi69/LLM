{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b901ba-8594-4efd-a0b2-b0194775dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db0de2b-201c-445e-8bcb-540f3a927e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_community\n",
    "from langchain.evaluation.criteria import CriteriaEvalChain\n",
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7514d42a-1861-44c6-a6a2-827d4a1e34a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            \tID          \tSIZE  \tMODIFIED   \n",
      "llama2:chat     \t78e26419b446\t3.8 GB\t4 days ago\t\n",
      "tinyllama:latest\t2644915ede35\t637 MB\t8 days ago\t\n",
      "llama3:8b       \t365c0bd3c000\t4.7 GB\t8 days ago\t\n",
      "mistral:latest  \t2ae6f6dd7a3d\t4.1 GB\t8 days ago\t\n",
      "phi3:latest     \t64c1188f2485\t2.4 GB\t8 days ago\t\n"
     ]
    }
   ],
   "source": [
    "# What llms do we have?\n",
    "! ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b53c5c5-1d43-4c66-a452-d5834b1189be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='llama3:8b', num_predict=256, temperature=0.9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "# 1.0.1\n",
    "\n",
    "llm= ChatOllama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=256      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80e1af1-949d-4699-9fdc-faa5c1a6b070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Here\\'s my step-by-step reasoning:\\n\\n1. Is the submission an idea?\\n\\t* Yes, the submission is \"Imagine an ice cream flavor for the color aquamarine\", which is an idea.\\n2. Does the submission relate to the input (\"Tell me an idea\")?\\n\\t* Yes, the submission is a response to the request to provide an idea.\\n3. Does the submission meet my-custom-criterion (\"Is the submission the most amazing ever\")?\\n\\t* This criterion is subjective and hard to evaluate objectively. However, I\\'ll assume that it\\'s meant to be evaluated based on creativity, originality, and potential impact.\\n\\t* Upon reviewing the submission, I think that \"Imagine an ice cream flavor for the color aquamarine\" is a creative and unique idea that might spark interesting conversations about colors and flavors. While it may not be the most amazing ever, it\\'s certainly a thoughtful and imaginative response.\\n\\nBased on my evaluation, here\\'s my answer:\\n\\nY',\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria = {\"my-custom-criterion\": \"Is the submission the most amazing ever?\"}\n",
    "evaluator = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)\n",
    "evaluator.evaluate_strings(prediction=\"Imagine an ice cream flavor for the color aquamarine\", input=\"Tell me an idea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1211d5d0-ec9c-4cde-b228-6be42395ca83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='mistral', num_predict=256, temperature=0.9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0.1\n",
    "\n",
    "llm= Ollama(model = \"mistral\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=256      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec8a8d5-b8ac-42b7-a415-321655495197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"To determine if the submission meets the Criteria, let's break down the analysis:\\n\\n1. Correctness: A correct answer is one that matches with the provided reference in terms of the number of apples.\\n2. Reference: There are 3 apples according to the given data.\\n3. Submission: The submission states there are 4 apples.\\n\\nComparing the Submission and the Reference, we find that they do not match - the answer in the reference is 3 while the submission claims it's 4. Therefore, based on our analysis, the answer is incorrect.\\n\\nY\\n(Repeat the letter for clarity)\",\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteria = \"correctness\"\n",
    "evaluator = LabeledCriteriaEvalChain.from_llm(\n",
    "    llm=llm,\n",
    "    criteria=criteria,\n",
    ")\n",
    "evaluator.evaluate_strings(\n",
    "  prediction=\"The answer is 4\",\n",
    "  input=\"How many apples are there?\",\n",
    "  reference=\"There are 3 apples\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b78ac-ded0-470f-8bcd-b2994a742411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.evaluation import load_evaluator\n",
    "#from langchain import open_ai\n",
    "from langchain.evaluation.loading import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", llm=llm, criteria=\"conciseness\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c92218a-e84a-408d-8cae-81166e135b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84bac13b-4857-4d76-8bcd-840b98a08555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "#This is work\n",
    "evaluator = load_evaluator(\"labeled_score_string\", llm=ChatOllama(model=\"llama2\"))\n",
    "evaluator = load_evaluator(\"pairwise_string\",  llm=Ollama(model=\"llama3:8b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afe34c7f-5635-4127-8cde-f9681a6c6f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashok/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain/evaluation/schema.py:128: UserWarning: Ignoring reference in PairwiseStringEvalChain, as it is not expected.\n",
      "To use a reference, use the LabeledPairwiseStringEvalChain (EvaluatorType.LABELED_PAIRWISE_STRING) instead.\n",
      "  warn(self._skip_reference_warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'I will now compare and evaluate the responses provided by the two AI assistants to the user question \"how many dogs are in the park?\".\\n\\nFrom my analysis, I can see that both responses aim to directly answer the user\\'s question. However, there is a significant difference between the quality of their answers.\\n\\nAssistant A provides a response that is brief and simplistic, stating that \"there are three dogs\". While this response attempts to answer the user\\'s question, it lacks depth, context, and relevance. It does not provide any additional information or insights that might be helpful to the user. The response also relies heavily on guesswork, as there is no evidence provided to support the claim.\\n\\nOn the other hand, Assistant B provides a direct and accurate response stating \"4\". This answer is straightforward, concise, and relevant to the question. It does not attempt to provide additional information or context, but it accurately answers the user\\'s query.\\n\\nIn terms of correctness, both responses are correct in the sense that they attempt to answer the question. However, Assistant B\\'s response provides a specific numerical value, which is more accurate than Assistant A\\'s vague claim.\\n\\nConsidering the criteria specified by the human, I conclude that:\\n\\n* Helpfulness: Both responses are somewhat helpful, but Assistant B\\'s response is more informative and accurate.\\n* Relevance: Both responses refer to dogs in the park, but Assistant B\\'s response provides a specific numerical value, which is more relevant to the question.\\n* Correctness: Assistant B\\'s response is correct in providing a specific number, while Assistant A\\'s response is vague and lacks evidence.\\n* Depth: Assistant A\\'s response lacks depth, while Assistant B\\'s response provides a straightforward and accurate answer.\\n\\nBased on this evaluation, my final verdict is:\\n\\n[[B]]',\n",
       " 'value': 'B',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/\n",
    "# https://stackoverflow.com/q/78231114/3282777\n",
    "\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"there are three dogs\",\n",
    "    prediction_b=\"4\",\n",
    "    input=\"how many dogs are in the park?\",\n",
    "    reference=\"four\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6641e89-606e-4177-85e7-9a41b75c019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is equivalent to loading using the enum\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bad61fb-4756-40a3-808e-1c847d240b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0fb231-b2fd-4f90-be0b-f9254af4b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models.ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387dc763-ede0-499c-912a-b398539b4097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Let\\'s assess the submission based on the conciseness criterion.\\n\\nStep 1: Review the input prompt.\\nThe input prompt is \"Who is the president of United States?\"\\n\\nStep 2: Evaluate the submission\\'s relevance to the input prompt.\\nThe submission mentions Joe Biden, an American politician who is the 46th and current president of the United States. This directly answers the input prompt.\\n\\nStep 3: Assess the submission\\'s conciseness.\\nThe submission provides a detailed biography of Joe Biden, including his birthplace, date, education, and political career. While it does mention that he is the current president of the United States, which is relevant to the input prompt, the overall length of the submission exceeds what one would typically consider concise.\\n\\nConclusion: The submission does not meet the conciseness criterion.\\n\\nY', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "#evaluator = load_evaluator(\"pairwise_string\",  llm=Ollama(model=\"llama3:8b\"))\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=Ollama(model=\"llama3:8b\"))\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"\"\"Joe Biden is an American politician \n",
    "    who is the 46th and current president of the United States. \n",
    "    Born in Scranton, Pennsylvania on November 20, 1942, \n",
    "    Biden moved with his family to Delaware in 1953. \n",
    "    He graduated from the University of Delaware \n",
    "    before earning his law degree from Syracuse University. \n",
    "    He was elected to the New Castle County Council in 1970 \n",
    "    and to the U.S. Senate in 1972.\"\"\",\n",
    "    input=\"Who is the president of United States?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ec88a1-9f90-499c-b0b1-5d1d936152b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\" , llm=Ollama(model=\"llama3:8b\"))\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"Is there any river on the moon?\",\n",
    "    prediction=\"There is no evidence of river on the Moon\",\n",
    "    reference=\"\"\"In a hypothetical future, lunar scientists discovered \n",
    "    an astonishing phenomenon—a subterranean river \n",
    "    beneath the Moon's surface\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e59e07c-0b0d-477e-85af-430b5ccba0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Step-by-step reasoning for each criterion:\\n\\n1. Correctness:\\n\\t* The submission states \"There is no evidence of river on the Moon.\"\\n\\t* This statement is accurate and factual based on current scientific knowledge.\\n\\t* However, the reference provided suggests that in a hypothetical future scenario, scientists discovered a subterranean river beneath the Moon\\'s surface.\\n\\t* Since this discovery is not part of our current understanding, the submission is correct within the scope of current knowledge.\\n\\nReasoning: Y\\n\\n2. Correctness:\\n\\t* The same reasoning as above applies; the submission is accurate and factual based on current scientific knowledge.\\n\\t* However, it does not account for hypothetical future discoveries or scenarios that might contradict its claim.\\n\\t* In this sense, the submission is correct in stating that there is no evidence of river on the Moon.\\n\\nReasoning: Y\\n\\nBased on my reasoning, I conclude that the submission meets all criteria. Therefore, the answer is:\\n\\nY',\n",
       " 'value': 'Y',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4685ce7c-8e81-4513-9992-7ab3658f65bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Multi-criteria evaluation =====================\n",
      "{'reasoning': 'Here\\'s my step-by-step reasoning for each criterion:\\n\\n**Numeric: Does the output contain numeric information?**\\n\\nI\\'ll start by examining the submission: \"Why did the mathematician break up with his girlfriend?\\n\\nBecause she had too many \\'irrational\\' issues!\"\\n\\nAt first glance, I don\\'t see any numerical values or quantities mentioned in the submission. The word \"irrational\" is a mathematical term, but it\\'s not referring to a specific number or quantity. Therefore, I conclude that this criterion is NOT MET.\\n\\n**Mathematical: Does the output contain mathematical information?**\\n\\nNow, let\\'s look at the same submission again. This time, I\\'m focusing on whether the output contains any mathematical concepts or terms. The word \"irrational\" does appear in the submission, which is a mathematical term referring to numbers that cannot be expressed as a finite decimal or fraction. So, yes, this criterion IS MET.\\n\\n**Conclusion:**\\n\\nBased on my reasoning, I conclude that the submission meets one out of two criteria: it contains mathematical information (the word \"irrational\" is a mathematical term). However, it does not contain numeric information (there are no specific numbers or quantities mentioned in the submission).\\n\\nHere\\'s the single character answer:\\n\\nY', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "custom_criteria = {\n",
    "    \"numeric\": \"Does the output contain numeric information?\",\n",
    "    \"mathematical\": \"Does the output contain mathematical information?\"\n",
    "}\n",
    "prompt = \"Tell me a joke\"\n",
    "\n",
    "output = \"\"\"\n",
    "Why did the mathematician break up with his girlfriend?\n",
    "\n",
    "Because she had too many \"irrational\" issues!\n",
    "\"\"\"\n",
    "\n",
    "llm= ChatOllama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict= 1000      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criteria,\n",
    "    llm=llm\n",
    ")\n",
    "eval_result = eval_chain.evaluate_strings(prediction = output, input = prompt)\n",
    "print(\"===================== Multi-criteria evaluation =====================\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e7044fe-9b3d-4187-883d-9cb3bc413ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No llm is needed here\n",
    "from langchain.evaluation import ExactMatchStringEvaluator\n",
    "exact_match_evaluator = ExactMatchStringEvaluator()\n",
    "exact_match_evaluator = ExactMatchStringEvaluator(ignore_case=True)\n",
    "exact_match_evaluator.evaluate_strings(\n",
    "    prediction=\"Data Science\",\n",
    "    reference=\"My Data science\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2163f1ba-9076-484a-9daf-f7a7e69585c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "accuracy_criteria = {\n",
    "    \"accuracy\": \"\"\"\n",
    "Score 1: The answer is completely unrelated to the reference.\n",
    "Score 3: The answer has minor relevance but does not align with the reference.\n",
    "Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"\n",
    "}\n",
    "evaluator = load_evaluator(\n",
    "    \"labeled_score_string\",\n",
    "    criteria=accuracy_criteria,\n",
    "    llm= Ollama(model=\"llama3:8b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cfec014-77f9-4b79-a436-4d8f5eae6e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "\n",
      "The assistant's response is attempting to answer the user's question about the location of their socks. The assistant provides a specific location, stating that the socks are in the third drawer of the dresser, which aligns with the ground truth provided.\n",
      "\n",
      "Rating: [[7]]\n",
      "\n",
      "\n",
      "Explanation:\n",
      "The assistant's answer has moderate relevance and accuracy, as it correctly identifies the dresser as the location where the user can find their socks. However, the minor error is that the assistant does not mention that they have been told specifically that the drawer is the third one in the dresser, which could lead to some ambiguity if the dresser had multiple drawers. Nonetheless, the answer provides a helpful and accurate direction for the user to find their socks.\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"You can find them in the dresser's third drawer.\",\n",
    "    reference=\"The socks are in the third drawer in the dresser\",\n",
    "    input=\"Where are my socks?\",\n",
    ")\n",
    "\n",
    "print(eval_result['reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07f52309-b57a-4973-902b-38ab4e370025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /home/ashok/anaconda3/envs/langchain/lib/python3.11/site-packages/huggingface_hub-0.23.2-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading rapidfuzz-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.9.3\n"
     ]
    }
   ],
   "source": [
    "! pip install rapidfuzz -- quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b49a273e-533f-48e5-bcf9-7665d87b0522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.23015873015873023}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not need llm\n",
    "from langchain.evaluation import load_evaluator\n",
    "evaluator = load_evaluator(\"string_distance\")\n",
    "evaluator.evaluate_strings(\n",
    "    prediction=\"Senior Data Scientist\",\n",
    "    reference=\"Data Scientist\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7b9ced5-21e4-4ecd-a82b-1cb3237f0a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"I will now evaluate the responses provided by the two AI assistants, focusing on the criteria mentioned: helpfulness, relevance, accuracy, depth, and creativity.\\n\\nAssistant A's response is brief and straightforward. However, it is not entirely accurate, as there are seven days in a week (Monday to Sunday). The response lacks depth and does not provide any additional information or insights.\\n\\nOn the other hand, Assistant B's response is extremely concise and directly answers the user's question. It is accurate, stating that there are indeed 7 days in a week. While it may lack creativity and depth, its simplicity and correctness make it a more helpful and relevant answer.\\n\\nAfter comparing both responses, I conclude that Assistant B's answer better meets the criteria of helpfulness, relevance, accuracy, and depth. Therefore, my final verdict is:\\n\\n[[B]]\",\n",
       " 'value': 'B',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\", llm = Ollama(model=\"llama3:8b\"))\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"there are 5 days\",\n",
    "    prediction_b=\"7\",\n",
    "    input=\"how many days in a week?\",\n",
    "    reference=\"Seven\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b530f8-db45-4042-a804-a6f6e663a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs open api key\n",
    "from langchain.evaluation import load_evaluator\n",
    "evaluator = load_evaluator(\"pairwise_embedding_distance\",  llm = Ollama(model=\"llama3:8b\") )\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Rajasthan is hot in June\", prediction_b=\"Rajasthan is warm in June.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49642e15-0bcd-417a-90b4-7f3067f2365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs open ai key\n",
    "evaluator = load_evaluator(\"embedding_distance\", llm = Ollama(model=\"llama3:8b\"))\n",
    "evaluator.evaluate_strings(prediction=\"Total Profit is 04.25 Cr\", \n",
    "reference=\"Total return is 4.25 Cr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
