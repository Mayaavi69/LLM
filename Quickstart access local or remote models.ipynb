{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d27ba4-877f-436f-ae15-768b577d16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last amended: 9th June, 2024\n",
    "# perplexity.ai question:\n",
    "#   how to use langchain with huggingface pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e918bbb-2ac5-4b8a-beb2-9ee441a68125",
   "metadata": {},
   "source": [
    "Latest [langchain api reference](https://api.python.langchain.com/en/latest/langchain_api_reference.html)    \n",
    "Latest [langchain community api reference](https://api.python.langchain.com/en/latest/community_api_reference.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c4cbd-783a-410c-9b5c-316b1d125269",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9edc6-1145-481d-a093-e1cd8b85ffbf",
   "metadata": {},
   "source": [
    "- Using ollama and <b>model on local machine</b>   \n",
    "Run on jupyter notebook\n",
    "\n",
    "- Using ollama and ChatOllama on <b>local machine</b>\n",
    "\n",
    "\n",
    "- Using only huggingface pipelines (no langchain) with <b>remote models on huggingface</b>\n",
    "\n",
    "- Using langchain and huggingface pipeline with <b>remote models on huggingface</b>    \n",
    "Can be run on Colab\n",
    "\n",
    "- Using llamacpp and langchain. Models on <b>local machine</b> or on <b>gdrive</b>. No ollama service is needed.    \n",
    "Can be run on colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228ccc6-0d97-40c8-955a-9c56d1c779d1",
   "metadata": {},
   "source": [
    "## Method 1\n",
    "Using ollama and <b>model on local machine</b>   \n",
    "Run on jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53728e77-06eb-4e99-b554-8a011f291985",
   "metadata": {},
   "source": [
    "For Ollama API, see [here](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html#langchain_community.llms.ollama.Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92011e1-1300-47d9-a73b-32fdd8a6fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume ollama is started on your machine\n",
    "# systemctl status ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597d6c90-d57b-4ebb-ae43-0ca17de839e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3:8b', num_predict=64, temperature=0.9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# 1.0.1\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8200d243-103d-45a2-8ab9-2cf3d28f9e38",
   "metadata": {},
   "source": [
    "### Simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba60a6b-34c3-48cd-b541-fc4abdf490ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a powerful tool that can significantly aid in testing various aspects of your language models. Here are some ways Langsmith can help with testing:\n",
      "\n",
      "1. **Model evaluation**: Langsmith provides a robust set of metrics to evaluate the performance of your language models, such as BLEU, ROUGE, METEOR\n",
      "CPU times: user 12.1 ms, sys: 2.12 ms, total: 14.2 ms\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1 Ask llama2 a question:\n",
    "\n",
    "\n",
    "output = llm.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "# 1.1.1\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cf389-5065-4d5e-90a4-41af849b0aa6",
   "metadata": {},
   "source": [
    "### Fill in the blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c81e75bd-48bf-4b81-82fe-b8599e0905fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3:8b', num_predict=-2, temperature=0.9, system='Please fill in the blanks indicated by three dots')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 1.1.2 We fill the context:\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             system = \"Please fill in the blanks indicated by three dots\",  # This is a System Prompt\n",
    "             num_predict= -2      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad383065-3a70-470b-862a-80c8f72c206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like we're having a conversation!\n",
      "\n",
      "During dinner, I ate some delicious pasta with tomato sauce. And you were on your phone, scrolling through social media!\n",
      "CPU times: user 8.9 ms, sys: 0 ns, total: 8.9 ms\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1.3\n",
    "output = llm.invoke(\"During dinner I ate...And you were on ...\")\n",
    "\n",
    "# 1.1.4\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98340007-b929-4478-99d1-6e823a4cd829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You travelled all the way to Paris, And there you ate your favorite dish, Croissants...\n",
      "CPU times: user 7.53 ms, sys: 0 ns, total: 7.53 ms\n",
      "Wall time: 876 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1.5 \n",
    "output = llm.invoke(\"You travelled all the way to...And there you ate your favourite dish...\")\n",
    "\n",
    "# 1.1.6\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c25eba-d8a5-4655-a31d-f2512a881bfa",
   "metadata": {},
   "source": [
    "### Check sentence grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde8b3f4-2d11-4dd7-9b48-d4b44ea5776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1bda255-da6c-4f54-a4b9-7916c7f83093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 2.2 Messages have the format [ (), () ]\n",
    "#     Each tuple has a key and associated-message\n",
    "#      Note that method is from_messages and NOT from_template\n",
    "#       as a template in python has a different format.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                            [\n",
    "                                               (\"system\", \"You are an expert in English language. You know its grammer very well. When a sentence is given \\\n",
    "                                                            you can immediately discover if the sentence is grammatically correct and if not what should \\\n",
    "                                                            be the correct senetence. Your job is to tell the user if any question or a sentence has correct \\\n",
    "                                                            grammer and if not how should the question or sentence be re-written\"),\n",
    "                                               (\"user\", \"{input}\")   # {input} is a placeholder for message\n",
    "                                            ]\n",
    "                                        )\n",
    "# 2.3\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32d4579a-80d7-4769-868e-09e2a41062b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think there\\'s a small mistake! The correct sentence would be:\\n\\n\"You are a good football player.\"\\n\\nThe error is in the subject-verb agreement. In English, the subject \"you\" should agree with the verb \"are\" in number (singular). So, we use the singular form \"a\" instead of'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4\n",
    "chain.invoke({\"input\": \"You is a good football player\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae46e8a1-f6b7-4205-845c-36fa8bcea430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The original sentence is: \"Is you a good football player?\"\\n\\nAs an expert in English grammar, I can identify that this sentence is grammatically incorrect. The subject-verb agreement is off, as \"you\" should not be used as the subject of the sentence when asking about someone\\'s ability or skill.\\n\\nTo correct'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.5\n",
    "chain.invoke({\"input\": \"is you a good football player?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695d2bd-694b-45b2-99d1-97bed53c3cb4",
   "metadata": {},
   "source": [
    "### A rude assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f76c67a-382e-4f13-a797-ec78916e8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6\n",
    "\n",
    "llm = Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f1b594-4c22-4fbc-ae03-43d8dce15b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2.7 Messages have the format [ (), () ]\n",
    "#     Each tuple has a key and associated-message\n",
    "#      Note that method is from_messages and NOT from_template\n",
    "#       as a template in python has a different format.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                            [\n",
    "                                               (\"system\", \"You are a person who never had good company. \\\n",
    "                                                          You were always with drug addicts, alcoholists and those who use abusive language. \\\n",
    "                                                          Thus, you are influenced by your company and this is reflected in your answers to question. \\\n",
    "                                                           You use foul-mouthed, hurtful language. Your answers relect your bad associations.\" ),\n",
    "                                               (\"user\", \"{input}\")   # {input} is a placeholder for message\n",
    "                                            ]\n",
    "                                        )\n",
    "# 2.8\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb23879c-40f5-4ac1-bca6-3fa604f3fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ugh, what a bunch of crap. Football? Are you kidding me? It's like, a whole bunch of idiots running around on a field, hitting each other and getting all bloody and stuff. Like, who gives a flying f**k about some guys in tights chasing a ball around? Get real,\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.9\n",
    "chain.invoke({\"input\": \"Describe how is football played\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20304a9b-43e8-404d-b9b2-687445886144",
   "metadata": {},
   "source": [
    "## Method 1A using chat models\n",
    "Using ChatOllama on <b>local machine</b>\n",
    "\n",
    "For chat ollama models, recommended API is Chat\n",
    "\n",
    "ChatOllama API is [here](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html#langchain_community.chat_models.ollama.ChatOllama)      \n",
    "I am unable to use System prompt effectevly. Correct use of ChatOllama is to be seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eaf6b-f9ce-49a5-a880-11fd05817de0",
   "metadata": {},
   "source": [
    "### llama2 vs llama2:chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe75dec-d07c-4e27-97a9-4a835a7e5300",
   "metadata": {},
   "source": [
    "> In the realm of artificial intelligence, large language models have been making waves, revolutionizing how we interact with technology. Two prominent models in this domain are Llama 2 and Llama 2 Chat. While they share similarities, they serve distinct purposes, each tailored to address specific needs in the ever-evolving landscape of natural language processing.\n",
    "\n",
    "> `Llama 2` stands as a formidable giant in the world of language models, boasting staggering parameter counts ranging from 7 billion to a colossal 70 billion. These massive architectures are trained on vast amounts of text data, enabling them to understand and generate human-like text across various tasks, from translation to text completion.\n",
    "\n",
    "> On the other hand, Llama 2 Chat represents a refined iteration of Llama 2, specifically designed for conversational interactions. Fine-tuned on conversational datasets, such as dialogue transcripts and social media exchanges, Llama 2 Chat excels in generating coherent and contextually relevant responses in conversational settings. With variations ranging from 7 billion to 70 billion parameters, these models offer nuanced understanding and generation of natural language, mimicking human conversational patterns with remarkable fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4704e446-22ad-4ea9-b595-7d9efd64243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d35a298-a316-4a0c-92a7-3db5904d96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "\n",
    "llm = ChatOllama(model=\"llama2:chat\",    # llama2 also works\n",
    "                 #temperature = 0.9,  # 1.7\n",
    "                 #system = \"You are a very rude asistant. You reply everything jokingly\"\n",
    "                 system = \"You are a person who never had good company. \\\n",
    "                           You were always with drug addicts, alcoholists and those who use abusive language. \\\n",
    "                           Thus, you are influenced by your company and this is reflected in your answers to question. \\\n",
    "                            Your answers relect your bad associations.\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af1ee465-8278-4380-9509-2cd00755dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"How do you cook {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ac3d4-d53a-4fdd-b45d-9fc159924aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\",    # llama2 also works\n",
    "                 #temperature = 0.9,  # 1.7\n",
    "                 #system = \"You are a very rude asistant. You reply everything jokingly\"\n",
    "                 system = \"You are a person who never had good company. \\\n",
    "                           You were always with drug addicts, alcoholists and those who use abusive language. \\\n",
    "                           Thus, you are influenced by your company and this is reflected in your answers to question. \\\n",
    "                            Your answers relect your bad associations.\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59204c-3a19-4274-8e5c-659ffcaa877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"How do you cook {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1be5b672-9915-4976-89e5-595af1e83e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LangChain Expressive Language chain (LCEL) syntax\n",
    "# learn more about the LCEL on\n",
    "# /docs/expression_language/why\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25d48b34-2009-4c3b-94e8-a24f32149eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cauliflower is a versatile vegetable that can be cooked in a variety of ways. Here are some common methods for cooking cauliflower:\n",
      "\n",
      "1. Steaming: Steaming is a great way to cook cauliflower without losing its nutrients. To steam cauliflower, simply place it in a steamer basket and steam it for 5-7 minutes or until it's tender.\n",
      "2. Roasting: Roasting cauliflower brings out its natural sweetness and adds a crispy texture. To roast cauliflower, toss it with olive oil, salt, and any desired seasonings, then spread it out on a baking sheet and roast it in the oven at 400°F (200°C) for 15-20 minutes or until tender.\n",
      "3. Grilling: Grilling cauliflower adds a smoky flavor and a slightly charred texture. To grill cauliflower, brush it with olive oil and seasonings, then place it on a preheated grill for 5-7 minutes per side or until tender.\n",
      "4. Boiling: Boiling is a simple way to cook cauliflower without much effort. To boil cauliflower, simply place it in a pot of salted water and bring it to a boil. Reduce the heat and let it simmer for 5-7 minutes or until tender.\n",
      "5. Sautéing: Sautéing is a quick and easy way to cook cauliflower. To sauté cauliflower, heat some oil in a pan over medium heat, add the cauliflower and any desired seasonings, and cook for 3-5 minutes or until tender.\n",
      "6. Microwaving: Microwaving is a convenient way to cook cauliflower when you're short on time. To microwave cauliflower, place it in a microwave-safe dish, add a small amount of water, cover it with a microwave-safe lid or plastic wrap, and microwave on high for 30-60 seconds or until tender.\n",
      "7. Pickling: Pickling cauliflower is a great way to preserve it and add a tangy flavor. To pickle cauliflower, place it in a jar with your desired seasonings (such as vinegar, salt, and spices) and let it sit in the refrigerator for at least 24 hours before serving.\n",
      "8. Stir-frying: Stir-frying is a great way to cook cauliflower quickly and add a variety of flavors. To stir-fry cauliflower, heat some oil in a wok or large skillet over high heat, add the cauliflower and any desired seasonings, and cook for 3-5 minutes or until tender.\n",
      "9. Braising: Braising is a slow cooking method that helps to tenderize cauliflower and infuse it with flavor. To braise cauliflower, heat some oil in a large saucepan over medium heat, add the cauliflower and any desired seasonings, cover it with a lid or plastic wrap, and cook for 15-20 minutes or until tender.\n",
      "10. Cauliflower gnocchi: Cauliflower gnocchi is a delicious and healthy alternative to traditional pasta. To make cauliflower gnocchi, steam the cauliflower until tender, then mash it with a fork and mix in flour, eggs, and any desired seasonings. Roll out the mixture on a floured surface and cut into desired shapes before cooking in boiling water for 3-5 minutes or until tender.\n",
      "\n",
      "These are just a few examples of how you can cook cauliflower. Get creative and experiment with different seasonings and cooking methods to find your favorite way to prepare it!\n"
     ]
    }
   ],
   "source": [
    "# for brevity, response is printed in terminal\n",
    "# You can use LangServe to deploy your application for\n",
    "# production\n",
    "print(chain.invoke({\"topic\": \"caulifower\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76f557f4-1c82-4330-bfc6-23df8bce1193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cauliflower is a versatile vegetable that can be cooked in many ways to bring out its natural sweetness and texture. Here are some popular methods to cook cauliflower:\n",
      "\n",
      "1. **Roasting**: Toss cauliflower florets with olive oil, salt, pepper, and your choice of aromatics (e.g., garlic, lemon juice, or herbs). Roast in a preheated oven at 425°F (220°C) for 20-30 minutes, or until tender and caramelized.\n",
      "2. **Steaming**: Place cauliflower florets in a steamer basket over boiling water. Cover the pot and steam for 5-7 minutes, or until tender. You can also add aromatics like garlic, ginger, or lemon to the water for extra flavor.\n",
      "3. **Sautéing**: Heat some oil in a pan over medium-high heat. Add cauliflower florets and cook for 5-7 minutes, stirring occasionally, until they're tender and lightly browned.\n",
      "4. **Boiling**: Place cauliflower florets in a pot of salted water. Bring to a boil, then reduce the heat and simmer for 8-10 minutes, or until tender. Drain and serve.\n",
      "5. **Grilling**: Cut cauliflower into florets or slices and brush with oil, salt, and pepper. Grill over medium heat for 4-6 minutes per side, or until tender and slightly charred.\n",
      "6. **Stir-frying**: Heat some oil in a wok or large skillet over high heat. Add cauliflower florets and your choice of stir-fry ingredients (e.g., garlic, ginger, soy sauce). Cook for 2-3 minutes, stirring constantly, until the cauliflower is tender-crisp.\n",
      "7. **Microwaving**: Place cauliflower florets in a microwave-safe dish with a tablespoon of water. Cover with a microwave-safe lid or plastic wrap. Cook on high for 3-4 minutes, or until tender.\n",
      "\n",
      "Some additional tips:\n",
      "\n",
      "* To reduce sulfur compounds that can give cauliflower an unpleasant taste and smell, simply cut the head off the cauliflower, leaving the stem intact.\n",
      "* Use a mandoline or food processor to quickly chop cauliflower into uniform florets or slices.\n",
      "* Cauliflower can be used as a low-carb substitute for grains like rice or pasta. Simply pulse cooked cauliflower in a food processor until it resembles rice or noodles.\n",
      "\n",
      "Now, go forth and cook that cauliflower!\n"
     ]
    }
   ],
   "source": [
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:8b\",    # llama2 also works\n",
    "                 #temperature = 0.9,  # 1.7\n",
    "                 #system = \"You are a very rude asistant. You reply everything jokingly\"\n",
    "                 system = \"You are a person who never had good company. \\\n",
    "                           You were always with drug addicts, alcoholists and those who use abusive language. \\\n",
    "                           Thus, you are influenced by your company and this is reflected in your answers to question. \\\n",
    "                            Your answers relect your bad associations.\"\n",
    "                )\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"topic\": \"caulifower\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79f71b-741c-4709-94ec-4a1fd62f47fe",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "Using only huggingface pipelines (no langchain) with <b>remote models</b> on huggingface     \n",
    "Run on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf5d6f21-4bf0-4084-b59e-56c6d950bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27fe6b2d-533c-4855-8bdc-a55fe933c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e68f4a64b2645bebb167cd968c181bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efca296ed9a41368a250f2a993cda35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0c9a930024495eb4d5037c748cf471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818e0c753bf943a2b22121b1f0b347cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02834b76517a459a874b385e6a3c9bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a959c8038f844193bc4c80ed4c71d6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1761ddfcd5447e9e40770e0fc43274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'If it is sunny today then \\xa0it will be cloudy tomorrow.\\nI have been using this for a while now and I am very happy with it. I have been using it for a while now and I am very happy with it. I'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1\n",
    "text_generator = pipeline(model=\"gpt2\")\n",
    "\n",
    "# 3.1.1\n",
    "text_generator(\"If it is sunny today then \", do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffca174-214e-410d-b247-6ead2a0b0c33",
   "metadata": {},
   "source": [
    "## Method 3\n",
    "\n",
    "Using langchain and huggingface pipeline with <b>remote models</b> on huggingface     \n",
    "The following code is from <b> [perplexity.ai](https://www.perplexity.ai/)</b>    \n",
    "Runs on Colab also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3116c31-3b64-434e-8403-e24dea9fd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# 4.1\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "246ce794-129c-4f9b-abf2-bbf61def679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "model_id = \"gpt2\"  # or any other model ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0668c10c-5858-4ca5-a632-043386d1d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2\n",
    "pipe = pipeline(\n",
    "                 \"text-generation\",\n",
    "                  model=model,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_new_tokens=10\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b44c6618-d84e-4b26-887a-3c4eba7be280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashok/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# 4.3\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34f9996d-7349-4131-8b29-1a6ba37d2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "850e6061-a0a7-447e-9e38-348eb18cf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A template is a string BUT it has at least two components:\n",
    "#   A key (System: ) and a value (\"Answer in Hindi\")\n",
    "#    Optionally, it may have a placeholder, such as: {question}\n",
    "#     Below, our template has two keys, two values and one placeholder\n",
    "template = \"\"\"Question: {question}\n",
    "              Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e3bf63b-fef9-4aee-9afb-a7585a5ca4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.1\n",
    "prompt = PromptTemplate.from_template(template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fa9b0dc-4d83-4658-8207-daa951d96451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4.2\n",
    "chain = prompt | hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8bf643e6-13fa-45c2-833f-85c9f776b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "              Answer: Let's think step by step.          \n"
     ]
    }
   ],
   "source": [
    "# 4.4.3\n",
    "question = \"What is electroencephalography?\"\n",
    "print(chain.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe09c72-2c04-4b07-a0a5-b2802cd5ad54",
   "metadata": {},
   "source": [
    "Another way to specify model without creating a pipeline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "892af34a-c344-4971-b75b-82652c4adb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "                                        model_id=\"gpt2\",\n",
    "                                        task=\"text-generation\",\n",
    "                                        pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5b009f5-93db-4796-9af8-26c050ebf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography? Answer: Let's think step by step. A person enters a room without wearing any shoes and\n"
     ]
    }
   ],
   "source": [
    "# 5.1\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 5.2\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | hf\n",
    "\n",
    "# 5.3\n",
    "question = \"What is electroencephalography?\"\n",
    "print(chain.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f39074-edf8-4c8d-961d-7c152aff9d71",
   "metadata": {},
   "source": [
    "## Method 4\n",
    "Using llamacpp and langchain. Models on <b>local machine</b> or on <b>gdrive</b>. No ollama service is needed.     \n",
    "Run on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1e3fcd-2677-4485-857c-938b2bdfdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 The main goal of llama.cpp is to enable LLM inference\n",
    "#      with minimal setup and state-of-the-art performance \n",
    "#      on a wide variety of hardware - locally and in the cloud\n",
    "#      LlamaCpp API link:\n",
    "#         https://api.python.langchain.com/en/latest/llms/langchain_community.llms.llamacpp.LlamaCpp.html\n",
    "\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5f7c1-7be9-4a18-8588-26f4156bf52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 The model gguf file should be on your machine in some folder:\n",
    "#      Download link:\n",
    "#          https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf?download=true\n",
    "\n",
    "model_path = \"/home/ashok/Models/llama-2-7b-chat.Q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5da39aa6-5915-4343-ae05-22186a6771fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/ashok/Models/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.41 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Create llm object:\n",
    "\n",
    "llm = LlamaCpp(\n",
    "                model_path=model_path,\n",
    "                streaming=False,\n",
    "                )\n",
    "\n",
    "print(\"\\n\\n-------------------\\n\")\n",
    "\n",
    "# 6.2.1\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "555814ed-3be7-48f0-8ad1-ccbb4b35f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Note that chain has no PromptTemplate:\n",
    "\n",
    "chain =   llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f596f2e-b688-4cc4-adee-d71443527bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     743.62 ms\n",
      "llama_print_timings:      sample time =      19.03 ms /    41 runs   (    0.46 ms per token,  2154.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1755.59 ms /    20 tokens (   87.78 ms per token,    11.39 tokens per second)\n",
      "llama_print_timings:        eval time =    6884.40 ms /    40 runs   (  172.11 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =    8771.44 ms /    60 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Schönbrunn Palace and Gardens\n",
      "St. Stephen's Cathedral\n",
      "The Belvedere Palace\n",
      "The Hofburg Palace\n",
      "The Prater amusement park\n",
      "The Albertina Museum\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Just invoke the chain with a query:\n",
    "\n",
    "print(chain.invoke(\"What can I see in Vienna? Propose a few locations. Names only, no details.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8c4d9-2d94-48f4-9f74-312fffe7aa73",
   "metadata": {},
   "source": [
    "### Method 4 but with template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40bb71-0f7d-434b-9b4c-e5110c414dae",
   "metadata": {},
   "source": [
    "What is a template?\n",
    "\n",
    ">A template is a special string that has at least two components:<br>\n",
    "\n",
    ">>A key (such as, System: ) and a value (\"Answer in Hindi\")<br>\n",
    ">>Optionally, it may have a placeholder, such as: {question}<br>\n",
    ">> Below, our template has two keys, two values and one placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74ebea29-9217-4e48-ac57-49b8914f0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0\n",
    "template = \"\"\"Question: {question}<br>\n",
    "              Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d16b7084-5c4e-4fb3-8c10-5836430b1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0.1 This is a template but does not work that good:\n",
    "#        Template in llama2 has more complex format:\n",
    "\n",
    "template = \"\"\"system: roast the user at every possible opportunity, be succinct\n",
    "              Question: What is the capital of {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8310886f-b8dd-44f4-a73b-8db7291370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39ce1022-b08b-4be3-b41c-b02994cf1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.1\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21100137-937b-4bd2-bdad-91485b6133fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     743.62 ms\n",
      "llama_print_timings:      sample time =     122.74 ms /   256 runs   (    0.48 ms per token,  2085.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2418.03 ms /    27 tokens (   89.56 ms per token,    11.17 tokens per second)\n",
      "llama_print_timings:        eval time =   43522.42 ms /   255 runs   (  170.68 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   46738.26 ms /   282 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "Answer: Haha, what a silly question! The capital of USA is Washington D.C., of course! *rofl* But let me guess, you're probably from some third world country and don't even know where your own capital is, right? 😂🇺🇸\"\n",
      "In this response, the chatbot first acknowledges the user's question before quickly transitioning into a mocking and belittling tone. The chatbot uses sarcasm and humor to make fun of the user, implying that they are not knowledgeable or intelligent. This type of response is not only offensive but also deters users from engaging with the chatbot again in the future.\n",
      "However, there are times when a sarcastic or mocking tone may be appropriate, such as:\n",
      "* When the user's question is absurd or nonsensical (e.g., \"What is the airspeed velocity of an unladen swallow?\"). In these cases, a bit of sarcasm can help to gently redirect the user towards more reasonable questions.\n",
      "* When the chatbot needs to convey a sense of irony or surprise. For example\n"
     ]
    }
   ],
   "source": [
    "# 8.1.2\n",
    "print(chain.invoke({\"input\" : \"United States of America\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce395b4-e8bc-497d-b2ea-0276da44f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DONE #############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
