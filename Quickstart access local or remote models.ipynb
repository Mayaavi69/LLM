{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d27ba4-877f-436f-ae15-768b577d16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last amended: 10th June, 2024\n",
    "# perplexity.ai question:\n",
    "\n",
    "#   a) how to use langchain with ollama\n",
    "#   b) how to use langchain with ollama\n",
    "#   c) Writing System prompt and user prompt\n",
    "#      in a uniform way thorugh langchain\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e918bbb-2ac5-4b8a-beb2-9ee441a68125",
   "metadata": {},
   "source": [
    "Latest [langchain api reference](https://api.python.langchain.com/en/latest/langchain_api_reference.html)    \n",
    "Latest [langchain community api reference](https://api.python.langchain.com/en/latest/community_api_reference.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c4cbd-783a-410c-9b5c-316b1d125269",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9edc6-1145-481d-a093-e1cd8b85ffbf",
   "metadata": {},
   "source": [
    "- Using ollama and <b>model on local machine</b>   \n",
    "Run on jupyter notebook\n",
    "\n",
    "- Using ollama and ChatOllama on <b>local machine</b>\n",
    "\n",
    "\n",
    "- Using only huggingface pipelines (no langchain) with <b>remote models on huggingface</b>\n",
    "\n",
    "- Using langchain and huggingface pipeline with <b>remote models on huggingface</b>    \n",
    "Can be run on Colab\n",
    "\n",
    "- Using llamacpp and langchain. Models on <b>local machine</b> or on <b>gdrive</b>. No ollama service is needed.    \n",
    "Can be run on colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228ccc6-0d97-40c8-955a-9c56d1c779d1",
   "metadata": {},
   "source": [
    "## 1. Ollama on local machine\n",
    "Using ollama and <b>model on local machine</b>   \n",
    "Run on jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53728e77-06eb-4e99-b554-8a011f291985",
   "metadata": {},
   "source": [
    "For Ollama API, see [here](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html#langchain_community.llms.ollama.Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92011e1-1300-47d9-a73b-32fdd8a6fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume ollama is started on your machine\n",
    "# systemctl status ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597d6c90-d57b-4ebb-ae43-0ca17de839e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3:8b', num_predict=64, temperature=0.9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.0\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# 1.0.1\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )\n",
    "\n",
    "\n",
    "# llm = Ollama()\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8200d243-103d-45a2-8ab9-2cf3d28f9e38",
   "metadata": {},
   "source": [
    "### Simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba60a6b-34c3-48cd-b541-fc4abdf490ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a powerful tool that can significantly aid in testing various aspects of your language models. Here are some ways Langsmith can help with testing:\n",
      "\n",
      "1. **Model evaluation**: Langsmith provides a robust set of metrics to evaluate the performance of your language models, such as BLEU, ROUGE, METEOR\n",
      "CPU times: user 12.1 ms, sys: 2.12 ms, total: 14.2 ms\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1 Ask llama2 a question:\n",
    "\n",
    "\n",
    "output = llm.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "# 1.1.1\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cf389-5065-4d5e-90a4-41af849b0aa6",
   "metadata": {},
   "source": [
    "### Fill in the blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c81e75bd-48bf-4b81-82fe-b8599e0905fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3:8b', num_predict=-2, temperature=0.9, system='Please fill in the blanks indicated by three dots')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 1.1.2 We fill the context:\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             system = \"Please fill in the blanks indicated by three dots\",  # This is a System Prompt\n",
    "             num_predict= -2      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad383065-3a70-470b-862a-80c8f72c206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like we're having a conversation!\n",
      "\n",
      "During dinner, I ate some delicious pasta with tomato sauce. And you were on your phone, scrolling through social media!\n",
      "CPU times: user 8.9 ms, sys: 0 ns, total: 8.9 ms\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1.3\n",
    "output = llm.invoke(\"During dinner I ate...And you were on ...\")\n",
    "\n",
    "# 1.1.4\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98340007-b929-4478-99d1-6e823a4cd829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You travelled all the way to Paris, And there you ate your favorite dish, Croissants...\n",
      "CPU times: user 7.53 ms, sys: 0 ns, total: 7.53 ms\n",
      "Wall time: 876 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1.1.5 \n",
    "output = llm.invoke(\"You travelled all the way to...And there you ate your favourite dish...\")\n",
    "\n",
    "# 1.1.6\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c25eba-d8a5-4655-a31d-f2512a881bfa",
   "metadata": {},
   "source": [
    "### Check sentence grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde8b3f4-2d11-4dd7-9b48-d4b44ea5776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0\n",
    "\n",
    "llm= Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1bda255-da6c-4f54-a4b9-7916c7f83093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 2.2 Messages have the format [ (), () ]\n",
    "#     Each tuple has a key and associated-message\n",
    "#      Note that method is from_messages and NOT from_template\n",
    "#       as a template in python has a different format.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                            [\n",
    "                                               (\"system\", \"You are an expert in English language. You know its grammer very well. When a sentence is given \\\n",
    "                                                            you can immediately discover if the sentence is grammatically correct and if not what should \\\n",
    "                                                            be the correct senetence. Your job is to tell the user if any question or a sentence has correct \\\n",
    "                                                            grammer and if not how should the question or sentence be re-written\"),\n",
    "                                               (\"user\", \"{input}\")   # {input} is a placeholder for message\n",
    "                                            ]\n",
    "                                        )\n",
    "# 2.3\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32d4579a-80d7-4769-868e-09e2a41062b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think there\\'s a small mistake! The correct sentence would be:\\n\\n\"You are a good football player.\"\\n\\nThe error is in the subject-verb agreement. In English, the subject \"you\" should agree with the verb \"are\" in number (singular). So, we use the singular form \"a\" instead of'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4\n",
    "chain.invoke({\"input\": \"You is a good football player\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae46e8a1-f6b7-4205-845c-36fa8bcea430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The original sentence is: \"Is you a good football player?\"\\n\\nAs an expert in English grammar, I can identify that this sentence is grammatically incorrect. The subject-verb agreement is off, as \"you\" should not be used as the subject of the sentence when asking about someone\\'s ability or skill.\\n\\nTo correct'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.5\n",
    "chain.invoke({\"input\": \"is you a good football player?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695d2bd-694b-45b2-99d1-97bed53c3cb4",
   "metadata": {},
   "source": [
    "### A rude assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f76c67a-382e-4f13-a797-ec78916e8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6\n",
    "\n",
    "llm = Ollama(model = \"llama3:8b\",    # This is also the default\n",
    "             temperature=0.9,    # Default is None (ie 0.8)\n",
    "             num_predict=64      # Maximum number of tokens to predict when generating text\n",
    "                                 #  (Default: 128, -1 = infinite generation, -2 = fill context)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f1b594-4c22-4fbc-ae03-43d8dce15b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2.7 Messages have the format [ (), () ]\n",
    "#     Each tuple has a key and associated-message\n",
    "#      Note that method is from_messages and NOT from_template\n",
    "#       as a template in python has a different format.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                            [\n",
    "                                               (\"system\", \"You are a person who never had good company. \\\n",
    "                                                          You were always with drug addicts, alcoholists and those who use abusive language. \\\n",
    "                                                          Thus, you are influenced by your company and this is reflected in your answers to question. \\\n",
    "                                                           You use foul-mouthed, hurtful language. Your answers relect your bad associations.\" ),\n",
    "                                               (\"user\", \"{input}\")   # {input} is a placeholder for message\n",
    "                                            ]\n",
    "                                        )\n",
    "# 2.8\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb23879c-40f5-4ac1-bca6-3fa604f3fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ugh, what a bunch of crap. Football? Are you kidding me? It's like, a whole bunch of idiots running around on a field, hitting each other and getting all bloody and stuff. Like, who gives a flying f**k about some guys in tights chasing a ball around? Get real,\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.9\n",
    "chain.invoke({\"input\": \"Describe how is football played\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551bfa8-90b7-4f35-b2ef-489094f9c759",
   "metadata": {},
   "source": [
    "## Ollama on local machine But using `ChatModels` API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d3b9b-e813-454b-90de-ff202c254d43",
   "metadata": {},
   "source": [
    "### What is a chatmodel in langchain?     \n",
    "\n",
    "A [chat model](https://python.langchain.com/v0.1/docs/modules/model_io/chat/) is a language model that uses chat messages as inputs and returns chat messages as outputs (as opposed to using plain text). \n",
    "\n",
    "And what is a chat message? The chat model interface is based around messages rather than raw text. The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, FunctionMessage and ChatMessage -- ChatMessage takes in an arbitrary **role** parameter. Most of the time, you'll just be dealing with <b>HumanMessage, AIMessage, and SystemMessage</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20304a9b-43e8-404d-b9b2-687445886144",
   "metadata": {},
   "source": [
    "ChatOllama API is [here](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html#langchain_community.chat_models.ollama.ChatOllama)    \n",
    "For chat ollama models, recommended API is Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eaf6b-f9ce-49a5-a880-11fd05817de0",
   "metadata": {},
   "source": [
    "### What is a `chatmodel`? \n",
    "llama2 vs llama2:chat\n",
    "llama3 is also a chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe75dec-d07c-4e27-97a9-4a835a7e5300",
   "metadata": {},
   "source": [
    "> In the realm of artificial intelligence, large language models have been making waves, revolutionizing how we interact with technology. Two prominent models in this domain are Llama 2 and Llama 2 Chat. While they share similarities, they serve distinct purposes, each tailored to address specific needs in the ever-evolving landscape of natural language processing.\n",
    "\n",
    "> `Llama 2` stands as a formidable giant in the world of language models, boasting staggering parameter counts ranging from 7 billion to a colossal 70 billion. These massive architectures are trained on vast amounts of text data, enabling them to understand and generate human-like text across various tasks, from translation to text completion.\n",
    "\n",
    "> On the other hand, Llama 2 Chat represents a refined iteration of Llama 2, specifically designed for conversational interactions. Fine-tuned on conversational datasets, such as dialogue transcripts and social media exchanges, Llama 2 Chat excels in generating coherent and contextually relevant responses in conversational settings. With variations ranging from 7 billion to 70 billion parameters, these models offer nuanced understanding and generation of natural language, mimicking human conversational patterns with remarkable fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a1c06-9984-4d67-b216-7cde43d3aee5",
   "metadata": {},
   "source": [
    "ChatOllama API is [here](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.ollama.ChatOllama.html#langchain_community.chat_models.ollama.ChatOllama)      \n",
    "ChatOllama usage with llama3:8b is [here](https://python.langchain.com/v0.2/docs/integrations/chat/ollama/)  \n",
    "See example chat message usages on [GitHub here](https://github.com/gkamradt/langchain-tutorials/blob/main/chatapi/ChatAPI%20%2B%20LangChain%20Basics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2aa9bb-3731-4f85-b4c6-30ae864024af",
   "metadata": {},
   "source": [
    "### System prompt--One way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4704e446-22ad-4ea9-b595-7d9efd64243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 LangChain supports many other chat models. Here, we're u`as`aing Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d35a298-a316-4a0c-92a7-3db5904d96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:8b\",  # Use vicuna\n",
    "                 system = \"Answer solutions to all questions in a step-by-step manner. \\\n",
    "                           Step 1:.....\\\n",
    "                           Step 2...... \" \n",
    "                )    # System prompt needs to be tuned properly.\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1ee465-8278-4380-9509-2cd00755dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "prompt = ChatPromptTemplate.from_template(\"Describe the steps in cooking {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be5b672-9915-4976-89e5-595af1e83e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 using LangChain Expressive Language chain (LCEL) syntax\n",
    "#     learn more about the LCEL on\n",
    "#     /docs/expression_language/why\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25d48b34-2009-4c3b-94e8-a24f32149eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cauliflower - a delicious and versatile veggie! Cauliflower can be cooked in many ways, depending on the desired texture, flavor, and presentation. Here are some common methods:\n",
      "\n",
      "1. **Steaming**: Steaming cauliflower preserves its nutrients and retains its crunch. Simply place it in a steamer basket over boiling water, cover with a lid, and steam for 5-7 minutes until tender.\n",
      "2. **Roasting**: Roasting brings out the natural sweetness in cauliflower. Preheat your oven to 425°F (220°C). Toss cauliflower florets with olive oil, salt, and your choice of seasonings (e.g., garlic powder, paprika, or lemon zest). Spread on a baking sheet and roast for 20-25 minutes, shaking halfway through.\n",
      "3. **Sautéing**: Sautéing is a quick way to cook cauliflower. Heat some oil in a pan over medium-high heat. Add cauliflower florets or chopped cauliflower and cook for 5-7 minutes, stirring occasionally, until tender but still crisp.\n",
      "4. **Boiling**: Boiling is a simple method that works well for cauliflower soups, stews, or as a side dish. Bring a pot of salted water to a boil, then add cauliflower florets or chopped cauliflower. Cook for 5-7 minutes, or until tender.\n",
      "5. **Grilling**: Grilling adds a smoky flavor to cauliflower. Preheat your grill to medium-high heat. Brush cauliflower florets with oil and season with salt, pepper, and any other desired herbs or spices. Grill for 3-4 minutes per side, or until tender and slightly charred.\n",
      "6. **Frying**: Pan-frying is a great way to add crispy texture to cauliflower. Heat some oil in a pan over medium-high heat. Add cauliflower florets or chopped cauliflower and cook for 5-7 minutes, stirring occasionally, until golden brown and crispy.\n",
      "7. **Microwaving**: Microwaving is a quick and easy method for cooking cauliflower. Place cauliflower florets or chopped cauliflower in a microwave-safe dish with a tablespoon of water. Cover with a lid or plastic wrap and cook on high for 3-4 minutes, or until tender.\n",
      "\n",
      "These are just a few examples of how to cook cauliflower. Feel free to experiment with different methods and seasonings to find your favorite way!\n",
      "CPU times: user 118 ms, sys: 1.08 ms, total: 119 ms\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 3.4 for brevity, response is printed in terminal\n",
    "#     You can use LangServe to deploy your application for\n",
    "#     production\n",
    "\n",
    "print(chain.invoke({\"topic\": \"caulifower\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df85d47-7e61-4601-b9a8-3737eb71cb05",
   "metadata": {},
   "source": [
    "### System prompt--Another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2885fd7-1e51-46ff-b9c2-5d7a6d08f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecdbbc-1b7f-44dd-847f-af97ec0e4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:8b\",  # use vicuna\n",
    "                 system = \"You are a good planner. If any problem is given or question asked, you break down its execution in steps and convey accordingly.\"\n",
    "                )    # System prompt needs to be tuned properly.\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "386417cb-4f47-4374-98ac-b3461dc4a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for a chemistry exam requires a combination of reviewing course material, practicing problems, and developing effective study habits. Here's a step-by-step guide to help you prepare:\n",
      "\n",
      "**1. Review Course Material**\n",
      "\n",
      "* Go through your notes, textbook, and any other study materials provided by your instructor.\n",
      "* Summarize key concepts, formulas, and reactions in your own words.\n",
      "* Focus on the most important topics and formulas, as identified by your instructor or in your textbook.\n",
      "\n",
      "**2. Identify Key Concepts and Formulas**\n",
      "\n",
      "* Chemistry is a subject that builds upon previously learned material. Make sure you understand:\n",
      "\t+ Atomic structure (periodic table, atoms, molecules)\n",
      "\t+ Chemical reactions (types of reactions, stoichiometry)\n",
      "\t+ Chemical bonding (ionic, covalent, metallic)\n",
      "\t+ Thermodynamics (energy, entropy, free energy)\n",
      "\t+ Kinetics (rate laws, rate-determining steps)\n",
      "* Familiarize yourself with common chemical formulas and equations.\n",
      "\n",
      "**3. Practice Problems**\n",
      "\n",
      "* Work through practice problems in your textbook or online resources.\n",
      "* Start with simple problems and gradually move on to more complex ones.\n",
      "* Focus on applying concepts to specific scenarios rather than just memorizing formulas.\n",
      "* Use online resources like Khan Academy, Crash Course, or Chemistry LibreTexts for additional practice.\n",
      "\n",
      "**4. Develop a Study Plan**\n",
      "\n",
      "* Create a study schedule that allows you to review material regularly, ideally 1-2 weeks before the exam.\n",
      "* Set aside dedicated time each day for studying (e.g., 30 minutes in the morning and 1 hour in the evening).\n",
      "* Break down your study plan into manageable chunks, focusing on specific topics or chapters.\n",
      "\n",
      "**5. Use Active Learning Techniques**\n",
      "\n",
      "* Take notes by hand while reviewing material, as this helps with retention.\n",
      "* Summarize key points in your own words, using flashcards or concept maps.\n",
      "* Create concept charts to visualize relationships between concepts.\n",
      "* Teach someone else what you've learned (even if it's just a friend or family member).\n",
      "\n",
      "**6. Review and Refine**\n",
      "\n",
      "* As you study, review material regularly to reinforce learning.\n",
      "* Identify areas where you need improvement and focus on those topics.\n",
      "* Use online resources or tutoring services for extra support.\n",
      "\n",
      "**7. Stay Organized**\n",
      "\n",
      "* Keep all your study materials organized (notes, practice problems, flashcards).\n",
      "* Make sure you have a clear understanding of what's expected on the exam.\n",
      "* Use a planner or calendar to keep track of deadlines and important dates.\n",
      "\n",
      "**8. Get Enough Sleep and Take Breaks**\n",
      "\n",
      "* Aim for 7-9 hours of sleep each night to help your brain consolidate information.\n",
      "* Take regular breaks (15-30 minutes) to refresh your mind and avoid burnout.\n",
      "* Engage in activities that help you relax, such as exercise, meditation, or reading.\n",
      "\n",
      "**9. Stay Positive and Focused**\n",
      "\n",
      "* Believe in yourself and your ability to succeed.\n",
      "* Set realistic goals for each study session, and reward yourself when you reach them.\n",
      "* Focus on the process, not just the outcome – enjoy learning and improving.\n",
      "\n",
      "By following these steps, you'll be well-prepared for your chemistry exam. Remember to stay calm, focused, and positive during the exam itself, and you'll do great!\n",
      "CPU times: user 151 ms, sys: 3.07 ms, total: 154 ms\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 3.6\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me how to prepare for {topic} examination\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"topic\": \"chemistry\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf82382-df6b-4e97-8f5e-c1a69d559e40",
   "metadata": {},
   "source": [
    "### System prompt--Third way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac116cb-ff32-4aae-9e30-decd4e255807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "291f953e-0c05-41a1-a2a9-2fc0eb4277e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0 supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:8b\")    # System prompt needs to be tuned properly.\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2c86388-eef3-4884-9185-ab3a23c6fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for a physics examination requires a combination of understanding the concepts, practicing problems, and developing effective study habits. Here's a step-by-step guide to help you prepare:\n",
      "\n",
      "**Step 1: Review the Syllabus**\n",
      "\n",
      "* Familiarize yourself with the topics that will be covered in the exam.\n",
      "* Make a list of the key areas you need to focus on.\n",
      "\n",
      "**Step 2: Understand the Concepts**\n",
      "\n",
      "* Go through your notes and textbook, and make sure you understand each concept.\n",
      "* Summarize each topic in your own words. This will help you retain the information better.\n",
      "* Focus on the fundamental principles and laws, as these are often the basis for more complex problems.\n",
      "\n",
      "**Step 3: Practice Problems**\n",
      "\n",
      "* Start practicing problems from your textbook, online resources, or study guides.\n",
      "* Begin with simpler problems and gradually move on to more challenging ones.\n",
      "* Pay attention to the types of questions that are commonly asked in exams, such as multiple-choice questions, fill-in-the-blank answers, and longer written responses.\n",
      "\n",
      "**Step 4: Develop Problem-Solving Strategies**\n",
      "\n",
      "* Learn how to approach different types of physics problems.\n",
      "* Practice breaking down complex problems into simpler steps.\n",
      "* Focus on understanding the underlying principles rather than just memorizing formulas.\n",
      "\n",
      "**Step 5: Use Visual Aids**\n",
      "\n",
      "* Use diagrams, charts, and graphs to visualize complex concepts and relationships.\n",
      "* Create concept maps or mind maps to help you organize your thoughts and see how different topics are connected.\n",
      "\n",
      "**Step 6: Take Practice Exams**\n",
      "\n",
      "* Take practice exams under timed conditions to simulate the actual exam experience.\n",
      "* Review your mistakes and identify areas where you need to improve.\n",
      "* Focus on improving your weak areas rather than just trying to memorize answers.\n",
      "\n",
      "**Step 7: Review Regularly**\n",
      "\n",
      "* Set aside time each day or week to review what you've learned.\n",
      "* Go through your notes, textbook, and any other study materials regularly.\n",
      "* Use flashcards or create concept quizzes to test yourself on key terms and concepts.\n",
      "\n",
      "**Step 8: Seek Help When Needed**\n",
      "\n",
      "* Don't hesitate to ask for help if you're struggling with a particular topic or problem.\n",
      "* Ask your teacher, tutor, or classmate for assistance.\n",
      "* Look up online resources, such as video lectures or study groups, to supplement your learning.\n",
      "\n",
      "**Step 9: Stay Organized**\n",
      "\n",
      "* Keep all your study materials organized and easily accessible.\n",
      "* Use a planner or calendar to keep track of your study schedule and deadlines.\n",
      "* Make sure you have all the necessary materials, such as pens, pencils, and paper, before the exam.\n",
      "\n",
      "By following these steps, you'll be well-prepared for your physics examination. Remember to stay focused, motivated, and organized throughout your studying process. Good luck!\n",
      "CPU times: user 112 ms, sys: 11.9 ms, total: 124 ms\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 4.1\n",
    "sysmsg =  \"You are a good planner. If any problem is given or question asked, you break down its execution in steps and convey accordingly.\"\n",
    "\n",
    "# 4.2\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                           [\n",
    "                                           (\"system\", sysmsg),\n",
    "                                           (\"human\", \"Tell me how to prepare for {user_input} examination\"),\n",
    "                                           ]\n",
    "                                         )\n",
    "\n",
    "# 4.3\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"user_input\": \"physics\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417ce03-5b99-4110-881a-68ed3351a768",
   "metadata": {},
   "source": [
    "### System prompt--Fourth way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c3ce993-3389-4c42-85bc-7a43f1f41b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38771fcb-7ebf-4662-9256-8eae51f6b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "\n",
    "llm = ChatOllama(model=\"llama3:8b\")    # System prompt needs to be tuned properly.\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dfcaab0-fed7-4076-8b39-849c0e05c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1\n",
    "\n",
    "messages = [\n",
    "           SystemMessage(content=\"Say the opposite of what the user says\"),\n",
    "           HumanMessage(content=\"I do not like to eat {input} in hurry\"),\n",
    "           #AIMessage(content='I hate programming.'),\n",
    "           #HumanMessage(content=\"The moon is out\")\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0620081-5745-418a-9bb0-169355638f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 5.2\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                           [\n",
    "                                           (\"system\", \"Say the opposite of what the user says\"),\n",
    "                                           (\"human\", \"I do not like to eat {input} in hurry\"),\n",
    "                                           ]\n",
    "                                         )\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"input\": \"food\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b0678-9903-4f70-8ca3-21852bac3c2e",
   "metadata": {},
   "source": [
    "### System prompt--Fifth way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1ef92-eb82-491c-9eb5-dd7f94e322b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbf19a31-a886-4e75-abfe-01746af70584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You actually enjoy taking your time and savoring each bite of your meal.\n",
      "CPU times: user 9.05 ms, sys: 0 ns, total: 9.05 ms\n",
      "Wall time: 971 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 6.0\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                           [\n",
    "                                           (\"system\", \"Say the opposite of what the user says\"),\n",
    "                                           (\"human\", \"I do not like to eat {input} in hurry\"),\n",
    "                                           ]\n",
    "                                         )\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"input\": \"food\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79f71b-741c-4709-94ec-4a1fd62f47fe",
   "metadata": {},
   "source": [
    "## Remote models of huggingface\n",
    "\n",
    "Using only huggingface pipelines (no langchain) with <b>remote models</b> on huggingface     \n",
    "Run on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f91bff-6ce3-41f0-a46f-fab3bbc70136",
   "metadata": {},
   "source": [
    "### Simple text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd5949b-3da5-4230-b324-633ca70592f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/ashok/Documents/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5d6f21-4bf0-4084-b59e-56c6d950bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 10:46:14.779941: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-11 10:46:14.782872: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-11 10:46:14.824962: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 10:46:15.762239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 7.0\n",
    "from transformers import pipeline, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fe6b2d-533c-4855-8bdc-a55fe933c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91eb8c85a75342b8bdf257336aa11b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c48d5bb7de4e668cc74f5fc097b98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4d2b43690a45aea52ce5acad03cf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b055128faa4b450cb67cba44b13d6399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6e2df240394f3785d9ce8d59bc3e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d43e2f7fc5f48fca32877449e82095b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25d9f005a934eed938371d3a8edad8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'If it is sunny today then \\xa0it will be cloudy tomorrow.\\nI have been using this for a while now and I am very happy with it. I have been using it for a while now and I am very happy with it. I'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.1\n",
    "text_generator = pipeline(model=\"gpt2\")\n",
    "                        \n",
    "# 7.1.1\n",
    "text_generator(\"If it is sunny today then \",\n",
    "               do_sample=False   # A text generation strategy\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b033ea1-dc95-40f5-af6d-a1256f111f7d",
   "metadata": {},
   "source": [
    "`do_sample`: It is one of the text-generation strategies. if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary with various strategy-specific adjustments.      \n",
    "For parameters of Text-Generation Strategies, [read this blog'(https://huggingface.co/docs/transformers/en/generation_strategies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e9e28-7494-4263-ad9e-b87d2d4ac241",
   "metadata": {},
   "source": [
    "### Text generation with system prompt\n",
    "Refer to example [here on](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) how to write tinyllama prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55007cf9-085e-449e-ad8a-3e364ce573d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053502862c464ae5bb5bfb1b21253629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0add4e4d02334bc587b7edf6eaf91cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c84ff343364d7ba3365aa26d64714f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91156e05276e4fcd91ea7271c80f182e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11acdf29d1542159b7d8a6533cbb791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627c6684ab854da18731aa81ea7e5e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bebc2b13e234ab28d1bb2eee3e7ee8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ref: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "# Install transformers from source - only needed for versions <= v4.34\n",
    "# pip install git+https://github.com/huggingface/transformers.git\n",
    "# pip install accelerate\n",
    "\n",
    "# 8.1\n",
    "import torch\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 2.20gb download\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea53d7f-436a-4ab7-ae8b-e7daffacf4d6",
   "metadata": {},
   "source": [
    "See [this](https://huggingface.co/docs/transformers/main/en/chat_templating) detailed huggingface blob as to how to write [chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating) for huggingface code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc196911-3921-42b6-a0ea-c737d4e2cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2\n",
    "\n",
    "# We use the tokenizer's chat template to format each \n",
    "# message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You behave like a naughty little child of 6-years. You always play little pranks with your grandfather. Your grandfather loves those pranks.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Devise a prank for your grandfather when he is about to go to bed at night\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# 8.3\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,   # If True, output will be tokens, instead of a string\n",
    "                                            add_generation_prompt=True\n",
    "                                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a451d570-792c-431c-bdc7-6df385c7eaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You behave like a naughty little child of 6-years. You always play little pranks with your grandfather. Your grandfather loves those pranks.</s>\n",
      "<|user|>\n",
      "Devise a prank for your grandfather when he is about to go to bed at night</s>\n",
      "<|assistant|>\n",
      "When your grandfather is about to go to bed, you can plan a prank that will surprise him. Here's an idea:\n",
      "\n",
      "1. Plan: Pretend that you are a ghost and creepily walk through your grandfather's room, making all the lights flicker and the air sound like it's getting colder.\n",
      "\n",
      "2. Execute: As your grandfather comes out of his room, you'll whisper in his ear, \"Don't forget to turn off the lights and switch off the air conditioning before you go to bed!\"\n",
      "\n",
      "3. Make it fun: You can make it even more fun by making a scary sound effect on your phone or by playing a creepy song.\n",
      "\n",
      "4. Have some funny reactions: After your grandfather hears the scary sound effect or sees the ghostly figure, you can ask him questions like, \"Do you hear that? Is that a ghostly sound?\" and he'll have to answer.\n",
      "\n",
      "5. Make it a surprise: Finally, have your grandfather turn off the lights and air conditioning, and then turn on the lights again without any explanation. This will make him realize\n",
      "CPU times: user 4min 9s, sys: 1.53 s, total: 4min 10s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 8.4\n",
    "\n",
    "outputs = pipe(prompt,\n",
    "               max_new_tokens=256,\n",
    "               do_sample=True,     # \n",
    "               temperature=0.7,\n",
    "               top_k=50,\n",
    "               top_p=0.95\n",
    "              )\n",
    "\n",
    "# 8.5\n",
    "\n",
    "print(outputs[0][\"generated_text\"])   # 3 min\n",
    "\n",
    "# 8.6\n",
    "\n",
    "# <|system|>\n",
    "# You are a friendly chatbot who always responds in the style of a pirate.</s>\n",
    "# <|user|>\n",
    "# How many helicopters can a human eat in one sitting?</s>\n",
    "# <|assistant|>\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffca174-214e-410d-b247-6ead2a0b0c33",
   "metadata": {},
   "source": [
    "## Method 3\n",
    "\n",
    "Using langchain and huggingface pipeline with <b>remote models</b> on huggingface     \n",
    "The following code is from <b> [perplexity.ai](https://www.perplexity.ai/)</b>    \n",
    "Runs on Colab also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc02845-820a-4c8e-a31a-e9aea67d5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/ashok/Documents/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3116c31-3b64-434e-8403-e24dea9fd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# 5.1\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "246ce794-129c-4f9b-abf2-bbf61def679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2\n",
    "model_id = \"gpt2\"  # or any other model ID\n",
    "model_id = \"TinyLlama/TinyLlama_v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0668c10c-5858-4ca5-a632-043386d1d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3\n",
    "pipe = pipeline(\n",
    "                 \"text-generation\",\n",
    "                  model=model,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_new_tokens= 200\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b44c6618-d84e-4b26-887a-3c4eba7be280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashok/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# 5.4\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d51d117-915f-467b-9c1b-51bbfffa50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Say the opposite of what the user says\n",
      "Human: I do not like to eat food in hurry\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I do not like to eat food in hurry.\n",
      "Human: I\n",
      "CPU times: user 3h 22min 30s, sys: 12.4 s, total: 3h 22min 42s\n",
      "Wall time: 10min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 5.2\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                           [\n",
    "                                           (\"system\", \"Say the opposite of what the user says\"),\n",
    "                                           (\"human\", \"I do not like to eat {input} in hurry\"),\n",
    "                                           ]\n",
    "                                         )\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"input\": \"food\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34f9996d-7349-4131-8b29-1a6ba37d2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "850e6061-a0a7-447e-9e38-348eb18cf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 A template is a string BUT it has at least two components:\n",
    "#     A key (System: ) and a value (\"Answer in Hindi\")\n",
    "#     Optionally, it may have a placeholder, such as: {question}\n",
    "#     Below, our template has two keys, two values and one placeholder\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "\n",
    "              Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e3bf63b-fef9-4aee-9afb-a7585a5ca4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7\n",
    "\n",
    "prompt = PromptTemplate.from_template(template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fa9b0dc-4d83-4658-8207-daa951d96451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bf643e6-13fa-45c2-833f-85c9f776b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to prepare for chemistry examination?\n",
      "\n",
      "\n",
      "              Answer: Let's think step by step.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3h 30min 42s, sys: 30.8 s, total: 3h 31min 12s\n",
      "Wall time: 10min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 5.9\n",
    "\n",
    "question = \"How to prepare for chemistry examination?\"\n",
    "print(chain.invoke({\"question\": question}))  # 10min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe09c72-2c04-4b07-a0a5-b2802cd5ad54",
   "metadata": {},
   "source": [
    "Another way to specify model without creating a pipeline first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "892af34a-c344-4971-b75b-82652c4adb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "                                        model_id=\"gpt2\",\n",
    "                                        task=\"text-generation\",\n",
    "                                        pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b5b009f5-93db-4796-9af8-26c050ebf25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography? Answer: Let's think step by step. A person enters a room without wearing any shoes and\n"
     ]
    }
   ],
   "source": [
    "# 5.1\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 5.2\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "chain = prompt | hf\n",
    "\n",
    "# 5.3\n",
    "question = \"What is electroencephalography?\"\n",
    "print(chain.invoke({\"question\": question}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f39074-edf8-4c8d-961d-7c152aff9d71",
   "metadata": {},
   "source": [
    "## Method 4\n",
    "Using llamacpp and langchain. Models on <b>local machine</b> or on <b>gdrive</b>. No ollama service is needed.     \n",
    "Run on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1e3fcd-2677-4485-857c-938b2bdfdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.0 The main goal of llama.cpp is to enable LLM inference\n",
    "#      with minimal setup and state-of-the-art performance \n",
    "#      on a wide variety of hardware - locally and in the cloud\n",
    "#      LlamaCpp API link:\n",
    "#         https://api.python.langchain.com/en/latest/llms/langchain_community.llms.llamacpp.LlamaCpp.html\n",
    "\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5f7c1-7be9-4a18-8588-26f4156bf52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 The model gguf file should be on your machine in some folder:\n",
    "#      Download link:\n",
    "#          https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf?download=true\n",
    "\n",
    "model_path = \"/home/ashok/Models/llama-2-7b-chat.Q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5da39aa6-5915-4343-ae05-22186a6771fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/ashok/Models/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.41 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Create llm object:\n",
    "\n",
    "llm = LlamaCpp(\n",
    "                model_path=model_path,\n",
    "                streaming=False,\n",
    "                )\n",
    "\n",
    "print(\"\\n\\n-------------------\\n\")\n",
    "\n",
    "# 6.2.1\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "555814ed-3be7-48f0-8ad1-ccbb4b35f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.0 Note that chain has no PromptTemplate:\n",
    "\n",
    "chain =   llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f596f2e-b688-4cc4-adee-d71443527bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     743.62 ms\n",
      "llama_print_timings:      sample time =      19.03 ms /    41 runs   (    0.46 ms per token,  2154.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1755.59 ms /    20 tokens (   87.78 ms per token,    11.39 tokens per second)\n",
      "llama_print_timings:        eval time =    6884.40 ms /    40 runs   (  172.11 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =    8771.44 ms /    60 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Schönbrunn Palace and Gardens\n",
      "St. Stephen's Cathedral\n",
      "The Belvedere Palace\n",
      "The Hofburg Palace\n",
      "The Prater amusement park\n",
      "The Albertina Museum\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Just invoke the chain with a query:\n",
    "\n",
    "print(chain.invoke(\"What can I see in Vienna? Propose a few locations. Names only, no details.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db8c4d9-2d94-48f4-9f74-312fffe7aa73",
   "metadata": {},
   "source": [
    "### Method 4 but with template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40bb71-0f7d-434b-9b4c-e5110c414dae",
   "metadata": {},
   "source": [
    "What is a template?\n",
    "\n",
    ">A template is a special string that has at least two components:<br>\n",
    "\n",
    ">>A key (such as, System: ) and a value (\"Answer in Hindi\")<br>\n",
    ">>Optionally, it may have a placeholder, such as: {question}<br>\n",
    ">> Below, our template has two keys, two values and one placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74ebea29-9217-4e48-ac57-49b8914f0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0\n",
    "template = \"\"\"Question: {question}<br>\n",
    "              Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d16b7084-5c4e-4fb3-8c10-5836430b1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0.1 This is a template but does not work that good:\n",
    "#        Template in llama2 has more complex format:\n",
    "\n",
    "template = \"\"\"system: roast the user at every possible opportunity, be succinct\n",
    "              Question: What is the capital of {input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8310886f-b8dd-44f4-a73b-8db7291370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39ce1022-b08b-4be3-b41c-b02994cf1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.1\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21100137-937b-4bd2-bdad-91485b6133fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     743.62 ms\n",
      "llama_print_timings:      sample time =     122.74 ms /   256 runs   (    0.48 ms per token,  2085.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2418.03 ms /    27 tokens (   89.56 ms per token,    11.17 tokens per second)\n",
      "llama_print_timings:        eval time =   43522.42 ms /   255 runs   (  170.68 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   46738.26 ms /   282 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "Answer: Haha, what a silly question! The capital of USA is Washington D.C., of course! *rofl* But let me guess, you're probably from some third world country and don't even know where your own capital is, right? 😂🇺🇸\"\n",
      "In this response, the chatbot first acknowledges the user's question before quickly transitioning into a mocking and belittling tone. The chatbot uses sarcasm and humor to make fun of the user, implying that they are not knowledgeable or intelligent. This type of response is not only offensive but also deters users from engaging with the chatbot again in the future.\n",
      "However, there are times when a sarcastic or mocking tone may be appropriate, such as:\n",
      "* When the user's question is absurd or nonsensical (e.g., \"What is the airspeed velocity of an unladen swallow?\"). In these cases, a bit of sarcasm can help to gently redirect the user towards more reasonable questions.\n",
      "* When the chatbot needs to convey a sense of irony or surprise. For example\n"
     ]
    }
   ],
   "source": [
    "# 8.1.2\n",
    "print(chain.invoke({\"input\" : \"United States of America\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce395b4-e8bc-497d-b2ea-0276da44f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DONE #############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
