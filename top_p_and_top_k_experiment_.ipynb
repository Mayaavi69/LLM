{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTN0cSZj2AY2Tknks+Cvhe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/LLMs/blob/main/top_p_and_top_k_experiment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlFPMlGsgevh"
      },
      "outputs": [],
      "source": [
        "# https://www.linkedin.com/pulse/science-control-how-temperature-topp-topk-shape-large-puente-viejo-u88yf/\n",
        "# https://www.linkedin.com/pulse/your-first-llm-experiment-vishal-mysore-j55ec/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lease pay attention to the 3 arguments temperature, top_k and top_p\n",
        "\n",
        "    Temperature (temperature=0.7):The temperature parameter controls the randomness of the generated text. A higher temperature (e.g., closer to 1.0) makes the output more diverse and creative but potentially less focused. A lower temperature (e.g., closer to 0.0) makes the output more deterministic and focused, but it might be repetitive. In this case, temperature=0.7 indicates a moderate level of randomness.\n",
        "    Top-k Sampling (top_k=50):Top-k sampling limits the vocabulary used during text generation to the top-k most likely tokens. It helps control the diversity of the generated text. The model selects from the top-k tokens based on their predicted probabilities. Setting top_k=50 means that, during sampling, the model will consider the top 50 most likely tokens.\n",
        "    Top-p (Nucleus) Sampling (top_p=0.95):Top-p, also known as nucleus sampling, is an alternative to top-k sampling. It selects tokens based on their cumulative probabilities until the cumulative probability exceeds a specified threshold (top-p). Setting top_p=0.95 means that the model will consider tokens until the cumulative probability reaches 95%, ensuring a dynamic set of tokens based on their probabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "ByZes5ean4XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now lets look at other argument which is do_sample .\n",
        "\n",
        "    do_sample=True:When do_sample is set to True, the model uses sampling during text generation. Instead of deterministically choosing the token with the highest probability at each step, the model stochastically samples from the probability distribution over the vocabulary. Sampling introduces randomness and diversity into the generated text. It allows the model to produce more creative and varied outputs.\n",
        "    do_sample=False:When do_sample is set to False, the model uses greedy decoding. It selects the token with the highest probability at each step, leading to a more deterministic and focused generation. Greedy decoding tends to produce more predictable and conservative outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "1D_k1u2Sn8iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate --quiet"
      ],
      "metadata": {
        "id": "XqXvw8pUghMG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "EqNEltz3gp78"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -i https://pypi.org/simple/ bitsandbytes --quiet"
      ],
      "metadata": {
        "id": "BDlH_X3irgRL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    #bnb_4bit_quant_type=\"nf4\",\n",
        "    #bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "e78ajjN2rcZ9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        #load_in_4bit=True,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "2d6hlMNErHLQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "                model= model,  # \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\",\n",
        "                tokenizer = tokenizer,\n",
        "                torch_dtype=torch.bfloat16\n",
        "                )"
      ],
      "metadata": {
        "id": "5P-RUsfWg4Ry"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are my personal chef experienced in Indian spicy food\",\n",
        "            },\n",
        "            {\"role\": \"user\",\n",
        "                 \"content\": \"What should i eat for breakfast today?\"\n",
        "            },\n",
        "]"
      ],
      "metadata": {
        "id": "Gn_0JyZphB3V"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = pipe.tokenizer.apply_chat_template(messages,\n",
        "                                            tokenize=False,\n",
        "                                            add_generation_prompt=True\n",
        "                                            )"
      ],
      "metadata": {
        "id": "R0jXyfPIhTf_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "outputs = pipe(prompt,\n",
        "               max_new_tokens=256,\n",
        "               do_sample=True,\n",
        "               temperature=0.7, # Default 0.8. Decrease makes it less creative\n",
        "               top_k=50,        # A higher value (100) will give more diverse answers\n",
        "               top_p=0.95       # A higher value leads to more diverse text\n",
        "               )    # 7minutes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Ixo_jdhYXu",
        "outputId": "01cff412-79cb-4feb-ca09-8c3c293fc988"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11.4 s, sys: 26.4 ms, total: 11.4 s\n",
            "Wall time: 11.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMsDobdFh29L",
        "outputId": "e6d3deb6-f9d7-482f-a689-99578b1fb6b8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are my personal chef experienced in Indian spicy food</s>\n",
            "<|user|>\n",
            "What should i eat for breakfast today?</s>\n",
            "<|assistant|>\n",
            "To start your day off right, here are some breakfast ideas:\n",
            "\n",
            "1. Eggs Benedict with toast\n",
            "2. Smoothie bowl with Greek yogurt, berries, and granola\n",
            "3. Muesli with mixed nuts and dried fruit\n",
            "4. Cinnamon and nut butter pancakes\n",
            "5. Sweet potato and black bean omelet\n",
            "6. Quinoa and avocado toast\n",
            "7. Oatmeal with raisins, almonds, and maple syrup\n",
            "8. Veggie omelet with smashed avocado, spinach, and mushrooms\n",
            "9. Grilled chicken and veggie wrap\n",
            "10. Peanut butter and banana sandwich on whole grain bread.\n",
            "\n",
            "Remember, breakfast is a great way to start your day with a nutrient-dense meal to fuel your body for the day ahead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "outputs = pipe(prompt,\n",
        "               max_new_tokens=256,\n",
        "               do_sample=True,\n",
        "               temperature=2.0,\n",
        "               top_k=100,\n",
        "               top_p=0.95\n",
        "               )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fEIFEa_jM_2",
        "outputId": "b20ecfcc-0a1e-4b17-9ecf-c00ba08a7ca2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.3 s, sys: 35.8 ms, total: 13.4 s\n",
            "Wall time: 13.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wLEFGU5jac4",
        "outputId": "84a2634d-1544-414c-c1bf-c00114aa1019"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are my personal chef experienced in Indian spicy food</s>\n",
            "<|user|>\n",
            "What should i eat for breakfast today?</s>\n",
            "<|assistant|>\n",
            "Enriquismo d'una remoufée grasse aisé dîndin à du lactezane avec manteiga blanc crissillimont rougent. Considrerai.\n",
            "\n",
            "Ingredients can become more nutlike this way for making your most tastingly white quirulosian dianne rouettable easy pills containing a man. To a piece lactezanche. Is a super food suppled with maltese, the daisies and quirun as are available frothy to frot le 5 jour les anivons, mais onreille pas sur ton palier si tout en ne nous souirais pas, le lendem.. If you look anile it is your mouth. Dive after to your chase (the gnat! To taste delico-sir it only will leave this nutisable diarde blant bull and enrique.. Not an appalling canteen in which pepperish deplement to nougel was also to soothing, whipt unisex baneful, etc.) was dished on board oar, whiles enrican is that my sweet quinnay-scapulate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bAPjS0Dym65Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}