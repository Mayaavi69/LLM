# Last amended: 21st April, 2024
# Installing langchain and ollama on ubuntu 22.04
# Ref: YouTube Video: https://www.youtube.com/watch?v=IJYC6zf86lU



# 0.0 Install Anaconda, if not installed

# 0.1 Syntax to remove conda env -- myenv

        conda remove --name langchain --all

# 0.2 Install on ubuntu plocate to use 'locate'
#     command to search files and net-tools for ifconfig:

sudo apt install plocate
sudo apt install net-tools


# 1. Create environment and 
#     setup necessary packages:
#      Open Anaocnda prompt as 'Administrator'
#       and run the following commands, step-by-step:
#        privatGPT requiires python=3.11

# 1.0.1 Add a download channel:
conda config --add channels conda-forge


# 1.0.2 Create conda environment:
conda create --name langchain python=3.11
conda activate langchain
conda install spyder jupyter jupyterlab pandas numpy pip-tools ipython
conda install langchain -c conda-forge
conda install anaconda::beautifulsoup4
conda install pytorch::faiss-cpu
conda install conda-forge::pypdf
conda install conda-forge::transformers
conda install fastai::accelerate
conda install -c conda-forge huggingface_hub



######################################################
############### AA.2 INSTALL CUDA ####################
############# AND Appropriate driver #################
######################################################

# 2.0 Install CUDA as in this video. Installation depends upon
#	which GPU is available. For GPU 1060 or 730 install
#	 CUDA 11.4


# 2.0.1 See this link for guidance
# https://github.com/harnalashok/LLMs/blob/main/gpu_nvidia.ipynb
#


######################################################
############### AA.3 Remaining installations #########
######################################################


# 3.0.1 Install git, as:
sudo apt update
sudo apt-get install git
# OR as:
(See https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
sudo apt update
sudo apt install git-all

# 3.0.2 Install curl:
sudo apt update
sudo apt install curl


# 3.0.3 Install 'make':
#	'make' generally comes installed with ubuntu 22.04
#	IF not, install it as:

sudo apt-get install build-essential

######################################################
############### AA.4 INSTALL ollama ##################
######################################################
# 4.0 Install/Uninstall Ollama by following instructions here:
#     See https://github.com/ollama/ollama/blob/main/docs/linux.md:

# 4.0.1 Install as::

curl -fsSL https://ollama.com/install.sh | sh

#  Install the models to be used, the default settings-ollama.yaml
#  is configured to user mistral 7b LLM (~4GB) and nomic-embed-text Embeddings (~275MB).
#  In privateGPT, ollama configuration file is: ~/privateGPT/settings-ollama.yaml

# 4.0.2 Issue following commands to download mistral and nomic-embed-text:
#        Downloaded models are saved to folder /usr/share/ollama/.ollama/models

ollama pull mistral
ollama pull nomic-embed-text

# 4.0.2.1 You can also specify the number of parameters.
#	   Pull ollama:13b model, as:

ollama pull llama2:13b

# And run it as:

ollama run llama2:13b

# 4.0.2.2 Just entering the following command will pull a 7b model and then run
#         but not the already existing 13b model:

ollama run llama2


# 4.0.3 You can test ollama, as:

ollama run mistral

# 4.0.4 On terminal, write a question to get an answer.
#       Exit by typing /bye

# 4.0.5 ollama supports GPUs with compute capability of 5.


# 4.0.6 List all models downloaded:

ollama list


# 4.0.7 Just enter 'ollama' to get ollama help.

Available Commands:
  serve       Start ollama AND also to know ollama port		<===
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models    					<====
  cp          Copy a model
  rm          Remove a model
  help        Help about any command




# 5.0    On reboot ollama starts by itself. The following command is NOT NEEEDED:
#	 It is also very difficult to kill olama process (even by kill -9). After
#	 killing another ollama process starts.

#        ollama serve  # Also informs ollama port: 11434


# 5.0.1 ollama service can be started/restarted/shut, as:
#	Check as: ps aux | grep ollama

sudo systemctl start/stop ollama

# 5.0.2 View ollama logs as:

	journalctl -u ollama  -e


# 5.0.3 On restart, issue again the following commands:

	a. conda activate langchain


# 5.0.4 Install tensorflow:

pip install tensorflow



######################################
########## DD. NVIDIA driver ##########
############### Problems ##############
#######################################

# 6.0
	It is possible that after privateGPT installs CUDA
	Or you have installed CUDA, then mutiple NVIDIA
	drivers (not one) could be installed. It is also
	possible that when you reboot Ubuntu, screen resolution
	changes rather increases ao that everything is very small.
	Then, in Ubuntu search and open Software & Updates--> Resources (tab)
	Select that minimum driver for your purposes. For example, for CUDA 11.4
	driver version 470 is OK. Higher versions, even though available,
	create problems. It is trial-and-error. Select a driver and 
	reboot to see if it is OK.
	
	Min driver needed?
		See this link for CUDA version and min driver needed:
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html
		OR,
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html#minor-version-compatibility


# 6.1 Screenshots of nvidia-smi commands:

				
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   47C    P5    13W /  N/A |    817MiB /  6044MiB |     33%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama              61MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                483MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+

## 30%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:17:50 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   62C    P2    49W /  N/A |   1277MiB /  6044MiB |     30%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                521MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


## 52%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:18:49 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   64C    P2    58W /  N/A |   1279MiB /  6044MiB |     52%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                523MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


------------------------
System Prompt Examples:
-------------------------

	The system prompt can effectively provide your chat bot specialized roles,
	and results tailored to the prompt you have given the model. Examples of 
	system prompts can be be found here: ChatGPT-3.5 Roles:
	https://www.w3schools.com/gen_ai/chatgpt-3-5/chatgpt-3-5_roles.php

	Some interesting examples to try include:

    		You are -X-. You have all the knowledge and personality of -X-. Answer
    		as if you were -X- using their manner of speaking and vocabulary.
        
        	Example: You are Shakespeare. You have all the knowledge and personality
        		 of Shakespeare. Answer as if you were Shakespeare using their manner
        		 of speaking and vocabulary.

			You are an expert (at) -role-. Answer all questions using your expertise
			on -specific domain topic-.

        	Example: You are an expert software engineer. Answer all questions using your 
        		 expertise on Python.

			You are a -role- bot, respond with -response criteria needed-. If 
			no -response criteria- is needed, respond with -alternate response-.
			
        	Example: You are a grammar checking bot, respond with any grammatical corrections
        		 needed. If no corrections are needed, respond with “verified”.
        		 
 	 
        		 


############ DONE #################
