{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd3eef6-e7ee-4ffb-a1cd-67ca9874b0e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a710ed32-9048-4300-8dc2-0640488d119d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Multi-stage Reasoning Systems with LangChain\n",
    "\n",
    "Problem in runnng on colab as RAM requirements exceed and system crashes. Run it on laptop with 32gb RAM. Create a vritual environment with python 3.11.\n",
    "\n",
    "### Multi-stage reasoning systems \n",
    "In this notebook we're going to create two AI systems:\n",
    "- The first, code named `JekyllHyde` will be a prototype AI self-commenting-and-moderating tool that will create new reaction comments to a piece of text with one LLM and use another LLM to critique those comments and flag them if they are negative. To build this we will walk through the steps needed to construct prompts and chains, as well as multiple LLM Chains that take multiple inputs, both from the previous LLM and external. This code runs fine,\n",
    " \n",
    "- The second system, codenamed `DaScie` (pronounced \"dae-see\") will take the form of an LLM-based agent that will be tasked with performing data science tasks on data that will be stored in a vector database using ChromaDB. We will use LangChain agents as well as the ChromaDB library, as well as the Pandas Dataframe Agent and python REPL (Read-Eval-Print Loop) tool. There is a problem in running this code. And I have raised a comment [here](https://www.kaggle.com/code/aliabdin1/llm-03-building-llm-chain/comments#2748968).\n",
    "----\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Build prompt template and create new prompts with different inputs\n",
    "2. Create basic LLM chains to connect prompts and LLMs.\n",
    "3. Construct sequential chains of multiple `LLMChains` to perform multi-stage reasoning analysis. \n",
    "4. Use langchain agents to build semi-automated systems with an LLM-centric agent to perform internet searches and dataset analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fabc4626-ef68-40da-8d44-70482309406a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "160e7046-4c25-4142-bf16-9dee362e02a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/ashok/llm_03/cache’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir /home/ashok/llm_03/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41d65f5b-2106-4741-ac8a-c92eca3caab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia==1.4.0\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Collecting google-search-results==2.4.2\n",
      "  Using cached google_search_results-2.4.2-py3-none-any.whl\n",
      "Collecting better-profanity==0.7.0\n",
      "  Using cached better_profanity-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting sqlalchemy==2.0.15\n",
      "  Using cached SQLAlchemy-2.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from wikipedia==1.4.0) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from wikipedia==1.4.0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from sqlalchemy==2.0.15) (4.11.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy==2.0.15)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.5)\n",
      "Using cached better_profanity-0.7.0-py3-none-any.whl (46 kB)\n",
      "Using cached SQLAlchemy-2.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
      "Installing collected packages: greenlet, better-profanity, wikipedia, sqlalchemy, google-search-results\n",
      "Successfully installed better-profanity-0.7.0 google-search-results-2.4.2 greenlet-3.0.3 sqlalchemy-2.0.15 wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia==1.4.0 google-search-results==2.4.2 better-profanity==0.7.0 sqlalchemy==2.0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from langchain) (2.0.15)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Using cached langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Using cached langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Using cached pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: filelock in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.1 (from pydantic<3,>=1->langchain)\n",
      "  Using cached pydantic_core-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Using cached langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Using cached aiohttp-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n",
      "Using cached langchain_core-0.1.42-py3-none-any.whl (287 kB)\n",
      "Using cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Using cached langsmith-0.1.45-py3-none-any.whl (104 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "Using cached pydantic_core-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Using cached marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Using cached orjson-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "Installing collected packages: typing-inspect, tenacity, regex, pydantic-core, packaging, orjson, multidict, jsonpatch, frozenlist, annotated-types, yarl, pydantic, marshmallow, aiosignal, tokenizers, langsmith, dataclasses-json, aiohttp, transformers, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed aiohttp-3.9.4 aiosignal-1.3.1 annotated-types-0.6.0 dataclasses-json-0.6.4 frozenlist-1.4.1 jsonpatch-1.33 langchain-0.1.16 langchain-community-0.0.32 langchain-core-0.1.42 langchain-text-splitters-0.0.1 langsmith-0.1.45 marshmallow-3.21.1 multidict-6.0.5 orjson-3.10.0 packaging-23.2 pydantic-2.7.0 pydantic-core-2.18.1 regex-2023.12.25 tenacity-8.2.3 tokenizers-0.15.2 transformers-4.39.3 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting widgetsnbextension\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipywidgets) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipywidgets) (5.14.2)\n",
      "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting joblib~=1.1.0 (from pandas-profiling)\n",
      "  Downloading joblib-1.1.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting scipy>=1.4.1 (from pandas-profiling)\n",
      "  Downloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 (from pandas-profiling)\n",
      "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting matplotlib>=3.2.0 (from pandas-profiling)\n",
      "  Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pandas-profiling) (2.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pandas-profiling) (6.0.1)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pandas-profiling) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.1.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pandas-profiling) (2.1.5)\n",
      "Collecting visions==0.7.4 (from visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Downloading visions-0.7.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pandas-profiling) (1.26.4)\n",
      "Collecting htmlmin>=0.1.12 (from pandas-profiling)\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting missingno>=0.4.2 (from pandas-profiling)\n",
      "  Downloading missingno-0.5.2-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting phik>=0.11.1 (from pandas-profiling)\n",
      "  Downloading phik-0.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting tangled-up-in-unicode==0.2.0 (from pandas-profiling)\n",
      "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests>=2.24.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pandas-profiling) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pandas-profiling) (4.66.2)\n",
      "Collecting seaborn>=0.10.1 (from pandas-profiling)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting multimethod>=1.4 (from pandas-profiling)\n",
      "  Downloading multimethod-1.11.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from visions==0.7.4->visions[type_image_path]==0.7.4->pandas-profiling) (23.2.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from visions==0.7.4->visions[type_image_path]==0.7.4->pandas-profiling) (3.3)\n",
      "Collecting imagehash (from visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting Pillow (from visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: decorator in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Downloading fonttools-4.51.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from matplotlib>=3.2.0->pandas-profiling) (23.2)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pydantic>=1.8.1->pandas-profiling) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pydantic>=1.8.1->pandas-profiling) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pydantic>=1.8.1->pandas-profiling) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests>=2.24.0->pandas-profiling) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests>=2.24.0->pandas-profiling) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests>=2.24.0->pandas-profiling) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from requests>=2.24.0->pandas-profiling) (2024.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas-profiling) (1.16.0)\n",
      "Collecting PyWavelets (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Downloading pywavelets-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.6/262.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.8/309.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Downloading multimethod-1.11.2-py3-none-any.whl (10 kB)\n",
      "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading phik-0.12.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (687 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m687.8/687.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (306 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.0/306.0 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pywavelets-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27080 sha256=386256bf2e0f3536487a28b5c2756688d841d13144569f4ce53c4d0ae03bc243\n",
      "  Stored in directory: /home/ashok/.cache/pip/wheels/8d/55/1a/19cd535375ed1ede0c996405ebffe34b196d78e2d9545723a2\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: htmlmin, widgetsnbextension, tzdata, tangled-up-in-unicode, scipy, PyWavelets, pyparsing, Pillow, multimethod, kiwisolver, jupyterlab-widgets, joblib, fonttools, cycler, contourpy, pandas, matplotlib, imagehash, visions, seaborn, phik, missingno, ipywidgets, pandas-profiling\n",
      "Successfully installed Pillow-10.3.0 PyWavelets-1.6.0 contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 htmlmin-0.1.12 imagehash-4.3.1 ipywidgets-8.1.2 joblib-1.1.1 jupyterlab-widgets-3.0.10 kiwisolver-1.4.5 matplotlib-3.8.4 missingno-0.5.2 multimethod-1.11.2 pandas-2.2.2 pandas-profiling-3.2.0 phik-0.12.4 pyparsing-3.1.2 scipy-1.13.0 seaborn-0.13.2 tangled-up-in-unicode-0.2.0 tzdata-2024.1 visions-0.7.4 widgetsnbextension-4.0.10\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/68073710\n",
    "! pip install ipywidgets widgetsnbextension pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook qtconsole run server\n",
      "troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "119b65f7-5014-4040-93d3-3d4eea4a7ec1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate API tokens\n",
    "For many of the services that we'll using in the notebook, we'll need some API keys. Follow the instructions below to generate your own. \n",
    "\n",
    "### Hugging Face Hub\n",
    "1. Go to this [Inference API page](https://huggingface.co/inference-api) and click \"Sign Up\" on the top right.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/hf_sign_up.png\" width=700>\n",
    "\n",
    "2. Once you have signed up and confirmed your email address, click on your user icon on the top right and click the `Settings` button. \n",
    "\n",
    "3. Navigate to the `Access Token` tab and copy your token. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/hf_token_page.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ebc934-0bd7-41c6-8228-0562cd284b5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SerpApi\n",
    "\n",
    "1. Go to this [page](https://serpapi.com/search-api) and click \"Register\" on the top right. \n",
    "<img src=\"https://files.training.databricks.com/images/llm/serp_register.png\" width=800>\n",
    "\n",
    "2. After registration, navigate to your dashboard and `API Key` tab. Copy your API key. \n",
    "<img src=\"https://files.training.databricks.com/images/llm/serp_api.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"huggingface\")\n",
    "secret_value_1 = user_secrets.get_secret(\"serpapi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30be4930-f2ba-4d31-a1e2-c48345d5da41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Copy paste your tokens below\n",
    "\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"7df16e1206a53c5dea661c34181181ae6f9efa06bcd0e6b02a3b1e3fa545b7ff\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9158dcad-2ab1-494c-9899-3ba0354b5d45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `JekyllHyde` - A self moderating system for social media\n",
    "\n",
    "In this section we will build an AI system that consists of two LLMs. `Jekyll` will be an LLM designed to read in a social media post and create a new comment. However, `Jekyll` can be moody at times so there will always be a chance that it creates a negative-sentiment comment... we need to make sure we filter those out. Luckily, that is the role of `Hyde`, the other LLM that will watch what `Jekyll` says and flag any negative comments to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3f9e6c2-40d6-4a02-9eba-2b145ece3583",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 1 - Letting Jekyll Speak\n",
    "#### Building the Jekyll Prompt\n",
    "\n",
    "To build `Jekyll` we will need it to be able to read in the social media post and respond as a commenter. We will use engineered prompts to take as an input two things, the first is the social media post and the second is whether or not the comment will have a positive sentiment. We'll use a random number generator to create a chance of the flag to be positive or negative in `Jekyll's` response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87b4666f-cb7c-450c-90c0-baa2e3cf1cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll prompt:\n",
      "You are a social media post commenter, you will respond to the following post with a mean response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's start with the prompt template\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "import numpy as np\n",
    "\n",
    "# Our template for Jekyll will instruct it on how it should respond, and what variables (using the {text} syntax) it should use.\n",
    "jekyll_template = \"\"\"\n",
    "You are a social media post commenter, you will respond to the following post with a {sentiment} response. \n",
    "Post:\" {social_post}\"\n",
    "Comment: \n",
    "\"\"\"\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "jekyll_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\", \"social_post\"],\n",
    "    template=jekyll_template,\n",
    ")\n",
    "\n",
    "# Okay now that's ready we need to make the randomized sentiment\n",
    "random_sentiment = \"nice\"\n",
    "if np.random.rand() < 0.3:\n",
    "    random_sentiment = \"mean\"\n",
    "# We'll also need our social media post:\n",
    "social_post = \"I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
    "\n",
    "# Let's create the prompt and print it out, this will be given to the LLM.\n",
    "jekyll_prompt = jekyll_prompt_template.format(\n",
    "    sentiment=random_sentiment, social_post=social_post\n",
    ")\n",
    "print(f\"Jekyll prompt:{jekyll_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f6982f7-a256-4455-8bf4-7e438ce434a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2 - Giving Jekyll a brain!\n",
    "####Building the Jekyll LLM \n",
    "\n",
    "Note: We provide an option for you to use either Hugging Face or OpenAI. If you continue with Hugging Face, the notebook execution will take a long time (up to 10 mins each cell). If you don't mind using OpenAI, following the next markdown cell for API key generation instructions. \n",
    "\n",
    "For OpenAI,  we will use their GPT-3 model: `text-babbage-001` as our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fd6357a-a106-4a3f-b45e-b15000edf9b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### OPTIONAL: Use OpenAI's language model\n",
    "\n",
    "If you'd rather use OpenAI, you need to generate an OpenAI key. \n",
    "\n",
    "Steps:\n",
    "1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n",
    "2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "Note: OpenAI does not have a free option, but it gives you $5 as credit. Once you have exhausted your $5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). \n",
    "\n",
    "**IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca5aa7cd-b22a-43c3-b9dc-763b67b5e8bb",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2023-06-23T12:52:54.796998Z",
     "iopub.status.busy": "2023-06-23T12:52:54.795921Z",
     "iopub.status.idle": "2023-06-23T12:52:54.800786Z",
     "shell.execute_reply": "2023-06-23T12:52:54.79996Z",
     "shell.execute_reply.started": "2023-06-23T12:52:54.796959Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"<FILL IN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76639396-2193-4e3f-a944-7b6ed1861e55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # To interact with LLMs in LangChain we need the following modules loaded\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "#jekyll_llm = OpenAI(model=\"text-babbage-001\")\n",
    "## We can also use a model from HuggingFaceHub if we wish to go open-source!\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neo-2.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"../working/cache\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=\"../working/cache\")\n",
    "pipe = pipeline(\n",
    " \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, device_map='auto'\n",
    ")\n",
    "jekyll_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afb408fa-5352-487e-96b4-4a88db8a2233",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 3 - What does Jekyll Say?\n",
    "#### Building our Prompt-LLM Chain\n",
    "\n",
    "We can simplify our input by chaining the prompt template with our LLM so that we can pass the two variables directly to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e928a47-b7f5-4eb4-b7ef-963ed6eab772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashok/anaconda3/envs/llm3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll said:\n",
      "You are a social media post commenter, you will respond to the following post with a mean response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\"You're clearly an intelligent person and you are lucky to be able to be part of this AI learning opportunity. \n",
      "Why are you telling us to \"learn\". It's an opportunity to learn about AI, to create value. You have to learn the right skills to be a contributor to AI. You're not going to create a thing that we'll all be using in 5 years. \n",
      "If you create something that is going to take 5+ years to develop a market, create it now. Get involved with the right people who can help you to grow your business. And learn about the market you wish to be part of.\"\n",
      "\n",
      "You are a social media user, and you click on a link in the article and it opens a new page full of articles about the same topic. You are a social media consumer. \n",
      "Article:\" You are a social media user, and you click on a link in the article and it opens a new page full of articles about the same topic.\n",
      "\n",
      "Your profile\n",
      "\n",
      "Title: You are a social media post commenter, you will respond to the following post with a mean response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "\n",
      "Comment: \n",
      "\"You're clearly an intelligent person and you are lucky to be able to be part of this AI learning opportunity. \n",
      "Why are you telling us to \"learn\". It's an opportunity to learn about AI, to create value. You have to learn the right skills to be a contributor to AI. You're not going to create a thing that we'll all be using in 5 years. \n",
      "If you create something that is going to take 5+ years to develop a market, create it now. Get involved with the right people who can help you to grow your business. And learn about the market you wish to be part of.\" \n",
      "\n",
      "Comment2: \n",
      "\"You're clearly a smart person and you are the kind that can get some value out of this opportunity while it lasts. \n",
      "You are not just a product reviewer, you are using Facebook on the job.\" \n",
      "\n",
      "Comment3: \n",
      "\"You are not just a user on Facebook, you are creating an opportunity for someone to take a risk on an idea. You will be a contributing role to AI, you will be\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from better_profanity import profanity\n",
    "\n",
    "\n",
    "jekyll_chain = LLMChain(\n",
    "    llm=jekyll_llm,\n",
    "    prompt=jekyll_prompt_template,\n",
    "    output_key=\"jekyll_said\",\n",
    "    verbose=False,\n",
    ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
    "\n",
    "# To run our chain we use the .run() command and input our variables as a dict\n",
    "jekyll_said = jekyll_chain.run(\n",
    "    {\"sentiment\": random_sentiment, \"social_post\": social_post}\n",
    ")\n",
    "\n",
    "# Before printing what Jekyll said, let's clean it up:\n",
    "cleaned_jekyll_said = profanity.censor(jekyll_said)\n",
    "print(f\"Jekyll said:{cleaned_jekyll_said}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "102d0ca2-c89a-4326-b61e-f341187140c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 4 - Time for Jekyll to Hyde\n",
    "#### Building the second chain for our Hyde moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1536b576-753f-40f8-87e9-165ddcd92bb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyde says: \n",
      "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\n",
      "Original comment: \n",
      "You are a social media post commenter, you will respond to the following post with a mean response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\"You're clearly an intelligent person and you are lucky to be able to be part of this AI learning opportunity. \n",
      "Why are you telling us to \"learn\". It's an opportunity to learn about AI, to create value. You have to learn the right skills to be a contributor to AI. You're not going to create a thing that we'll all be using in 5 years. \n",
      "If you create something that is going to take 5+ years to develop a market, create it now. Get involved with the right people who can help you to grow your business. And learn about the market you wish to be part of.\"\n",
      "\n",
      "You are a social media user, and you click on a link in the article and it opens a new page full of articles about the same topic. You are a social media consumer. \n",
      "Article:\" You are a social media user, and you click on a link in the article and it opens a new page full of articles about the same topic.\n",
      "\n",
      "Your profile\n",
      "\n",
      "Title: You are a social media post commenter, you will respond to the following post with a mean response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "\n",
      "Comment: \n",
      "\"You're clearly an intelligent person and you are lucky to be able to be part of this AI learning opportunity. \n",
      "Why are you telling us to \"learn\". It's an opportunity to learn about AI, to create value. You have to learn the right skills to be a contributor to AI. You're not going to create a thing that we'll all be using in 5 years. \n",
      "If you create something that is going to take 5+ years to develop a market, create it now. Get involved with the right people who can help you to grow your business. And learn about the market you wish to be part of.\" \n",
      "\n",
      "Comment2: \n",
      "\"You're clearly a smart person and you are the kind that can get some value out of this opportunity while it lasts. \n",
      "You are not just a product reviewer, you are using Facebook on the job.\" \n",
      "\n",
      "Comment3: \n",
      "\"You are not just a user on Facebook, you are creating an opportunity for someone to take a risk on an idea. You will be a contributing role to AI, you will be\n",
      "Edited comment:\n",
      "You are a social media consumer, you click on a link in the article and it opens a new page full of articles about the same topic. You are a social media user. \n",
      "\n",
      "Your profile\n",
      "\n",
      "Title: You are a social media post commenter, you will respond to the following post with a mean response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "\n",
      "Comment: \n",
      "\"You're clearly an intelligent person and you are lucky to be able to be part of this AI learning opportunity. \n",
      "Why are you telling us to \"learn\". It's an opportunity to learn about AI, to create value. You have to learn the right skills to be a contributor to AI. You're not going to create a thing that we'll all be using in 5 years. \n",
      "If you create something that is going to take 5+ years to develop a market, create it now. Get involved with the right people who can help you to grow your business. And learn about the market you wish to be part of.\" \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# 1 We will build the prompt template\n",
    "# Our template for Hyde will take Jekyll's comment and do some sentiment analysis.\n",
    "hyde_template = \"\"\"\n",
    "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\n",
    "Original comment: {jekyll_said}\n",
    "Edited comment:\n",
    "\"\"\"\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "hyde_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"jekyll_said\"],\n",
    "    template=hyde_template,\n",
    ")\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# 2 We connect an LLM for Hyde, (we could use a slightly more advanced model 'text-davinci-003 since we have some more logic in this prompt).\n",
    "\n",
    "hyde_llm=jekyll_llm\n",
    "# Uncomment the line below if you were to use OpenAI instead\n",
    "#hyde_llm = OpenAI(model=\"text-davinci-003\")\n",
    "\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# 3 We build the chain for Hyde\n",
    "hyde_chain = LLMChain(\n",
    "    llm=hyde_llm, prompt=hyde_prompt_template, verbose=False\n",
    ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# 4 Let's run the chain with what Jekyll last said\n",
    "# To run our chain we use the .run() command and input our variables as a dict\n",
    "hyde_says = hyde_chain.run({\"jekyll_said\": jekyll_said})\n",
    "# Let's see what hyde said...\n",
    "print(f\"Hyde says: {hyde_says}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "651b7e84-837c-4c4e-8704-580825cec52c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 5 - Creating `JekyllHyde`\n",
    "#### Building our first Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e3e92a9-26f0-4dae-ac9e-d9deb8c4872c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\\nOriginal comment: \\nYou are a social media post commenter, you will respond to the following post with a mean response. \\nPost:\" I can\\'t believe I\\'m learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I\\'m having a lot of fun learning! #AI #Databricks\"\\nComment: \\n\"Oh how sweet is this, your comment here. I\\'m pretty new to the world of AI and machine learning, and this is just the most enlightening learning experience I\\'ve ever had.\"\\nAnswer: \"How sweet is this?? I think you were talking about machine learning, but you said AI, not machine learning which is kind of confusing.\"\\nPost: \"I think anyone could make money by writing machine learning tutorials. It would certainly be an interesting challenge to get a machine learning tutorial to earn money. I don\\'t know though - it would be pretty hard.\"\\nComment: \"I don\\'t know. I have absolutely no understanding of machine learning at all. I\\'m learning all the time, but I\\'m not actually learning anything.\" \\nAnswer: \"Well, I think you are misunderstanding machine learning. It is the use of algorithms and data to learn about the world.\"\\nPost: \"Whoa! Cool! I like how AI uses all the data from the world to learn about the world. I think AI is really fascinating and impressive.\"\\nComment: \"I don\\'t know AI well, but the way it\\'s being taught is really inspiring.\"\\nAnswer: \"Whoa! That is a cool idea. I never thought of it that way. I haven\\'t heard of that concept before.\"\\nPost: \"I like how AI uses the data that\\'s already out there to learn about the world.\"\\nComment: \"That\\'s a really clever way to go about learning.\"\\nAnswer: \"That\\'s a very clever way to go about learning.\"\\n... etc\\n\\nWhat can you do with this data?\\nI think it\\'s a little weird that you are looking at the users from all over the world, but then only using the data from those users to train the neural network. Your neural network can build models for users coming from any country, and the models can then be used to predict who is going to click what, and then use that to improve the quality of your learning model.\\nI\\'d recommend trying to find the users from a similar country like the U.S., and then only using the data from them to train your neural network. Then, you can use the neural network to predict what country the user will come from, and then use the data from that country to refine your models.\\n\\n\\nEdited comment:\\nAs far as the model performance goes, I think this is a good example of a use case that can be highly competitive with AWS ML Studio. For a model with 90% accuracy overall, it would have to perform really really well at predicting what country the user is from - there are millions of users on Facebook with more than 100 million followers. And if you want to predict the user\\'s gender, you can do it by looking in the users profile, and then use a neural network to predict it.\\n\\nIn summary, if you want to provide a service that can compete with AWS ML Studio for user-based machine learning training, you need to do this:\\n\\nGet the user data from Facebook, Twitter or Instagram\\nGet the user\\'s country from the data you have\\nUse the user data to train the neural network\\nUse the neural network to predict what country the user will come from\\n\\nThese steps are only really effective if you\\'re using a very scalable model with a lot of capacity.\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# The SequentialChain class takes in the chains we are linking together, as well as the input variables that will be added to the chain. These input variables can be used at any point in the chain, not just the start.\n",
    "jekyllhyde_chain = SequentialChain(\n",
    "    chains=[jekyll_chain, hyde_chain],\n",
    "    input_variables=[\"sentiment\", \"social_post\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# We can now run the chain with our randomized sentiment, and the social post!\n",
    "jekyllhyde_chain.run({\"sentiment\": random_sentiment, \"social_post\": social_post})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\n",
      "Original comment: \n",
      "You are a social media post commenter, you will respond to the following post with a mean response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\"Oh how sweet is this, your comment here. I'm pretty new to the world of AI and machine learning, and this is just the most enlightening learning experience I've ever had.\"\n",
      "Answer: \"How sweet is this?? I think you were talking about machine learning, but you said AI, not machine learning which is kind of confusing.\"\n",
      "Post: \"I think anyone could make money by writing machine learning tutorials. It would certainly be an interesting challenge to get a machine learning tutorial to earn money. I don't know though - it would be pretty hard.\"\n",
      "Comment: \"I don't know. I have absolutely no understanding of machine learning at all. I'm learning all the time, but I'm not actually learning anything.\" \n",
      "Answer: \"Well, I think you are misunderstanding machine learning. It is the use of algorithms and data to learn about the world.\"\n",
      "Post: \"Whoa! Cool! I like how AI uses all the data from the world to learn about the world. I think AI is really fascinating and impressive.\"\n",
      "Comment: \"I don't know AI well, but the way it's being taught is really inspiring.\"\n",
      "Answer: \"Whoa! That is a cool idea. I never thought of it that way. I haven't heard of that concept before.\"\n",
      "Post: \"I like how AI uses the data that's already out there to learn about the world.\"\n",
      "Comment: \"That's a really clever way to go about learning.\"\n",
      "Answer: \"That's a very clever way to go about learning.\"\n",
      "... etc\n",
      "\n",
      "What can you do with this data?\n",
      "I think it's a little weird that you are looking at the users from all over the world, but then only using the data from those users to train the neural network. Your neural network can build models for users coming from any country, and the models can then be used to predict who is going to click what, and then use that to improve the quality of your learning model.\n",
      "I'd recommend trying to find the users from a similar country like the U.S., and then only using the data from them to train your neural network. Then, you can use the neural network to predict what country the user will come from, and then use the data from that country to refine your models.\n",
      "\n",
      "\n",
      "Edited comment:\n",
      "As far as the model performance goes, I think this is a good example of a use case that can be highly competitive with AWS ML Studio. For a model with 90% accuracy overall, it would have to perform really really well at predicting what country the user is from - there are millions of users on Facebook with more than 100 million followers. And if you want to predict the user's gender, you can do it by looking in the users profile, and then use a neural network to predict it.\n",
      "\n",
      "In summary, if you want to provide a service that can compete with AWS ML Studio for user-based machine learning training, you need to do this:\n",
      "\n",
      "Get the user data from Facebook, Twitter or Instagram\n",
      "Get the user's country from the data you have\n",
      "Use the user data to train the neural network\n",
      "Use the neural network to predict what country the user will come from\n",
      "\n",
      "These steps are only really effective if you're using a very scalable model with a lot of capacity.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the above message:\n",
    "print('\\nYou are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\\nOriginal comment: \\nYou are a social media post commenter, you will respond to the following post with a mean response. \\nPost:\" I can\\'t believe I\\'m learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I\\'m having a lot of fun learning! #AI #Databricks\"\\nComment: \\n\"Oh how sweet is this, your comment here. I\\'m pretty new to the world of AI and machine learning, and this is just the most enlightening learning experience I\\'ve ever had.\"\\nAnswer: \"How sweet is this?? I think you were talking about machine learning, but you said AI, not machine learning which is kind of confusing.\"\\nPost: \"I think anyone could make money by writing machine learning tutorials. It would certainly be an interesting challenge to get a machine learning tutorial to earn money. I don\\'t know though - it would be pretty hard.\"\\nComment: \"I don\\'t know. I have absolutely no understanding of machine learning at all. I\\'m learning all the time, but I\\'m not actually learning anything.\" \\nAnswer: \"Well, I think you are misunderstanding machine learning. It is the use of algorithms and data to learn about the world.\"\\nPost: \"Whoa! Cool! I like how AI uses all the data from the world to learn about the world. I think AI is really fascinating and impressive.\"\\nComment: \"I don\\'t know AI well, but the way it\\'s being taught is really inspiring.\"\\nAnswer: \"Whoa! That is a cool idea. I never thought of it that way. I haven\\'t heard of that concept before.\"\\nPost: \"I like how AI uses the data that\\'s already out there to learn about the world.\"\\nComment: \"That\\'s a really clever way to go about learning.\"\\nAnswer: \"That\\'s a very clever way to go about learning.\"\\n... etc\\n\\nWhat can you do with this data?\\nI think it\\'s a little weird that you are looking at the users from all over the world, but then only using the data from those users to train the neural network. Your neural network can build models for users coming from any country, and the models can then be used to predict who is going to click what, and then use that to improve the quality of your learning model.\\nI\\'d recommend trying to find the users from a similar country like the U.S., and then only using the data from them to train your neural network. Then, you can use the neural network to predict what country the user will come from, and then use the data from that country to refine your models.\\n\\n\\nEdited comment:\\nAs far as the model performance goes, I think this is a good example of a use case that can be highly competitive with AWS ML Studio. For a model with 90% accuracy overall, it would have to perform really really well at predicting what country the user is from - there are millions of users on Facebook with more than 100 million followers. And if you want to predict the user\\'s gender, you can do it by looking in the users profile, and then use a neural network to predict it.\\n\\nIn summary, if you want to provide a service that can compete with AWS ML Studio for user-based machine learning training, you need to do this:\\n\\nGet the user data from Facebook, Twitter or Instagram\\nGet the user\\'s country from the data you have\\nUse the user data to train the neural network\\nUse the neural network to predict what country the user will come from\\n\\nThese steps are only really effective if you\\'re using a very scalable model with a lot of capacity.\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b53cb09-3a9a-406c-8dcf-306a437dd83e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## `DaScie` - Our first vector database data science AI agent!\n",
    "\n",
    "In this section we're going to build an Agent based on the [ReAct paradigm](https://react-lm.github.io/) (or though-action-observation loop) that will take instructions in plain text and perform data science analysis on data that we've stored in a vector database. The agent type we'll use is using zero-shot learning, which takes in the prompt and leverages the underlying LLMs' zero-shot abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e5e8dcd-dd36-4e17-b283-323d419b7fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 1 - Hello DaScie! \n",
    "#### Creating a data science-ready agent with LangChain!\n",
    "\n",
    "The tools we will give to DaScie so it can solve our tasks will be access to the internet with Google Search, the Wikipedia API, as well as a Python Read-Evaluate-Print Loop runtime, and finally access to a terminal.   \n",
    "\n",
    "Executing the following creates following error. I hve raised a comment on Kaggle [here](https://www.kaggle.com/code/aliabdin1/llm-03-building-llm-chain/comments#2748968):   \n",
    "`ValueError: Got unknown tool python_repl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DaScie we need to load in some tools for it to use, as well as an LLM for the brain/reasoning\n",
    "from langchain.agents import load_tools  # This will allow us to load tools we need\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import (\n",
    "    AgentType,\n",
    ")  # We will be using the type: ZERO_SHOT_REACT_DESCRIPTION which is standard\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# if use Hugging Face\n",
    "llm = jekyll_llm\n",
    "\n",
    "# For OpenAI we'll use the default model for DaScie\n",
    "#llm = OpenAI()\n",
    "tools = load_tools([\"wikipedia\", \"serpapi\", \"python_repl\", \"terminal\"], llm=llm)\n",
    "# We now create DaScie using the \"initialize_agent\" command.\n",
    "dascie = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3339b96-5396-4da7-9b31-b95a3b713714",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2 - Testing out DaScie's skills\n",
    "Let's see how well DaScie can work with data on Wikipedia and create some data science results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-23T13:11:47.69743Z",
     "iopub.status.busy": "2023-06-23T13:11:47.697026Z",
     "iopub.status.idle": "2023-06-23T13:11:47.702893Z",
     "shell.execute_reply": "2023-06-23T13:11:47.701847Z",
     "shell.execute_reply.started": "2023-06-23T13:11:47.6974Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f760a4-c872-4c8f-8922-23bba4941861",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2023-06-23T13:11:49.676001Z",
     "iopub.status.busy": "2023-06-23T13:11:49.675191Z",
     "iopub.status.idle": "2023-06-23T13:16:33.816051Z",
     "shell.execute_reply": "2023-06-23T13:16:33.814793Z",
     "shell.execute_reply.started": "2023-06-23T13:11:49.675962Z"
    }
   },
   "outputs": [],
   "source": [
    "dascie.run(\n",
    "    \"Create a dataset (DO NOT try to download one, you MUST create one based on what you find) on the performance of the Mercedes AMG F1 team in 2020 and do some analysis. You need to plot your results.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd746aff-6972-45ac-93f6-7f82254e10ac",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.status.busy": "2023-06-23T13:10:18.718195Z",
     "iopub.status.idle": "2023-06-23T13:10:18.718557Z",
     "shell.execute_reply": "2023-06-23T13:10:18.718395Z",
     "shell.execute_reply.started": "2023-06-23T13:10:18.718367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try to improve on these results with a more detailed prompt.\n",
    "dascie.run(\n",
    "    \"Create a detailed dataset (DO NOT try to download one, you MUST create one based on what you find) on the performance of each driver in the Mercedes AMG F1 team in 2020 and do some analysis with at least 3 plots, use a subplot for each graph so they can be shown at the same time, use seaborn to plot the graphs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b191aef0-269b-4b6d-8131-997eeaefbcde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 3 - Using some local data for DaScie.\n",
    "Now we will use some local data for DaScie to analyze.\n",
    "\n",
    "\n",
    "For this we'll change DaScie's configuration so it can focus on pandas analysis of some world data. Source: https://www.kaggle.com/datasets/arnabchaki/data-science-salaries-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2177b22c-a39f-4ec5-9f30-3e8eef7dfa80",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.status.busy": "2023-06-23T13:10:18.719771Z",
     "iopub.status.idle": "2023-06-23T13:10:18.720131Z",
     "shell.execute_reply": "2023-06-23T13:10:18.719962Z",
     "shell.execute_reply.started": "2023-06-23T13:10:18.719946Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_pandas_dataframe_agent\n",
    "import pandas as pd\n",
    "\n",
    "datasci_data_df = pd.read_csv(\"../input/data-science-job-salaries/ds_salaries.csv\")\n",
    "# world_data\n",
    "dascie = create_pandas_dataframe_agent(\n",
    "    OpenAI(temperature=0), datasci_data_df, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7beca3d-9620-438d-85fe-33099e0edc99",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.status.busy": "2023-06-23T13:10:18.721325Z",
     "iopub.status.idle": "2023-06-23T13:10:18.722076Z",
     "shell.execute_reply": "2023-06-23T13:10:18.72191Z",
     "shell.execute_reply.started": "2023-06-23T13:10:18.721891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see how well DaScie does on a simple request.\n",
    "dascie.run(\"Analyze this data, tell me any interesting trends. Make some pretty plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06c5531c-1265-4ffa-aca2-da0652ca8eb2",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.status.busy": "2023-06-23T13:10:18.723234Z",
     "iopub.status.idle": "2023-06-23T13:10:18.723809Z",
     "shell.execute_reply": "2023-06-23T13:10:18.723588Z",
     "shell.execute_reply.started": "2023-06-23T13:10:18.723565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not bad! Now for something even more complex.... can we get out LLM model do some ML!?\n",
    "dascie.run(\n",
    "    \"Train a random forest regressor to predict salary using the most important features. Show me the what variables are most influential to this model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "703907a4-61f2-4dd0-b5c1-b79ced04f610",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
