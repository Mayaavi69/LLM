# Last amended: 15th April, 2024
# Installing privateGPT on ubuntu 22.04
# It works with a GUI
# Ref: https://docs.privategpt.dev/installation/getting-started/installation
#      https://github.com/PromptEngineer48/Ollama/tree/main/2-ollama-privateGPT-chat-with-docs


######################################################
############### AA. INSTALL privateGPT ###############
######################################################


# 0.0 Install Anaconda, if not installed

# 0.1 Syntax to remove conda env -- myenv

        conda remove --name langchain --all

# 0.2 Install on ubuntu plocate to use 'locate'
#     command to search files and net-tools for ifconfig:

sudo apt install plocate
sudo apt install net-tools


# 1. Create environment and 
#     setup necessary packages:
#      Open Anaocnda prompt as 'Administrator'
#       and run the following commands, step-by-step:
#        privatGPT requiires python=3.11

# 1.0.1 Add a download channel:
conda config --add channels conda-forge


# 1.0.2 Create conda environment:
conda create --name langchain python=3.11
conda activate langchain
conda install spyder jupyter jupyterlab pandas numpy pip-tools ipython
conda install langchain -c conda-forge
conda install anaconda::beautifulsoup4
conda install conda-forge::pypdf

######################################################
############### AA.2 INSTALL CUDA ####################
############# AND Appropriate driver #################
######################################################

# 2.0 Install CUDA as in this video. Installation depends upon
#	which GPU is available. For GPU 1060 or 730 install
#	 CUDA 11.4

# 2.0.1 See this link for guidance
# https://github.com/harnalashok/LLMs/blob/main/gpu_nvidia.ipynb
#


######################################################
############### AA.3 Remaining installations #########
######################################################


# 3.0.1 Install git, as:
sudo apt update
sudo apt-get install git
# OR as:
(See https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
sudo apt update
sudo apt install git-all

# 3.0.2 Install curl:
sudo apt update
sudo apt install curl


# 3.0.3 Clone privateGPT
#     We will clone it in our home folder:

cd ~/
git clone https://github.com/imartinez/privateGPT
cd privateGPT


# 3.0.4. From now onwards we must work in the new conda env
#         Activate conda environment langchain
#          and also cd to folder privateGPT:
#           cd ~/privateGPT

# 3.0.5. Install poetry:
#	 poetry is a more modern way to install python libraries
#	 and its dependencies. It is more effective than conda or pip
# 	 https://python-poetry.org/docs/#installing-with-the-official-installer
#        Also see:
#	actcorp.com/blog/Deep-Learning/managing-python-dependencies-with-poetry-vs-conda-pip

curl -sSL https://install.python-poetry.org | python3 -

# 3.0.6 Add poetry path to ~/.bashrc file:

export PATH="/home/ashok/.local/bin:$PATH"

# 3.0.7 Close and reopen terminal:

conda activate langchain
cd privateGPT

# 3.0.8 Check poetry is installed:
poetry --version


# 3.0.9 Install 'make':
#	'make' generally comes installed with ubuntu 22.04
#	IF not, install it as:

sudo apt-get install build-essential

######################################################
############### AA.4 INSTALL ollama ##################
######################################################
# 4.0 Install/Uninstall Ollama by following instructions here:
#     See https://github.com/ollama/ollama/blob/main/docs/linux.md:

# 4.0.1 Install as::

curl -fsSL https://ollama.com/install.sh | sh

#  Install the models to be used, the default settings-ollama.yaml
#  is configured to user mistral 7b LLM (~4GB) and nomic-embed-text Embeddings (~275MB).
#  In privateGPT, ollama configuration file is: ~/privateGPT/settings-ollama.yaml

# 4.0.2 Issue following commands to download mistral and nomic-embed-text:
#        Downloaded models are saved to folder /usr/share/ollama/.ollama/models

ollama pull mistral
ollama pull nomic-embed-text

# 4.0.2.1 You can also specify the number of parameters.
#	   Pull ollama:13b model, as:

ollama pull llama2:13b

# And run it as:

ollama run llama2:13b

# 4.0.2.2 Just entering the following command will pull a 7b model and then run
#         but not the already existing 13b model:

ollama run llama2


# 4.0.3 You can test ollama, as:

ollama run mistral

# 4.0.4 On terminal, write a question to get an answer.
#       Exit by typing /bye

# 4.0.5 ollama supports GPUs with compute capability of 5.


# 4.0.6 Just enter 'ollama' to get ollama help.

Available Commands:
  serve       Start ollama AND also to know ollama port		<===
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models    					<====
  cp          Copy a model
  rm          Remove a model
  help        Help about any command








# 5.0    On reboot ollama starts by itself. The following command is NOT NEEEDED:
#	 It is also very difficult to kill olama process (even by kill -9). After
#	 killing another ollama process starts.

#        ollama serve


# 5.0.1 ollama service can be started/restarted/shut, as:
#	Check as: ps aux | grep ollama

sudo systemctl start/stop ollama

# 5.0.2 View ollama logs as:

	journalctl -u ollama

#      Or to go to the end of logs (ie latest entries):

	journalctl -u ollama  -e

# 5.0.1 On restart, issue again the following commands:

	a. conda activate langchain
        b. cd ~/privateGPT


# 6.0. Install software using poetry:

conda activate langchain
cd ~/privateGPT
poetry install --extras "ui llms-ollama embeddings-ollama vector-stores-qdrant"


# 6.0.1 Install tensorflow:

pip install tensorflow

# 6.0.2 You may have to also make following change to ~/privateGPT/settings.yaml file
#       If there is an error, see here:
#       https://github.com/zylon-ai/private-gpt/issues/1334#issuecomment-1847997849
#       https://docs.privategpt.dev/manual/storage/vector-stores#chroma-configuration
#       Go to settings.yaml and change

	vectorstore: database: qdrant
	to
	vectorstore: database: chroma

#     And rebooot/restart privateGPT as follows:

######################################################
############### BB. START privateGPT as ##############
######################################################

# 6.1 Start privategpt:

conda activate langchain
cd privateGPT

# 6.2 Invoke 'ollama' profile as mentioned in file: settings-ollama.yaml
#     https://docs.privategpt.dev/manual/general-configuration/configuration
#	(Two commands in one)

PGPT_PROFILES=ollama make run

# 6.2.1 You can also run the above in two parts as:

export PGPT_PROFILES=ollama
make run


# 6.1.1 The UI will be available at http://<serverIP>:8001


DONE----- DONE---

# 6.1.2 What is 'make' command?
# 	'make' command runs commands as mentioned in the following file: 

	/home/ashok/privateGPT/Makefile


# 6.1.3 For example 'make run' will, in the background, 
#	execute the following command:
#	(Check why by opening the file: as:
#		nano /home/ashok/privateGPT/Makefile

	poetry run python -m private_gpt	



######################################################
############### CC. Install Visual Studo Code ########
######################################################


# 7,0   Install Visual Studio Code as:
#	Download .deb file from:
#	https://code.visualstudio.com/docs/setup/linux
#	Then install it using Software Install of ubuntu
#	OR, as:
#		cd Downloads
#		sudo apt install ./code_1.87.2-1709912201_amd64.deb
#
#	Then open it in terminal as: code
#       Visual Studio Code can be used to open
#	READEME.md in folder ~/privateGPT
#	You can add python and Markdown extensions for it.
#



######################################
########## DD. NVIDIA driver ##########
############### Problems ##############
#######################################

# 8.0
	It is possible that after privateGPT installs CUDA
	Or you have installed CUDA, then mutiple NVIDIA
	drivers (not one) could be installed. It is also
	possible that when you reboot Ubuntu, screen resolution
	changes rather increases ao that everything is very small.
	Then, in Ubuntu search and open Software & Updates--> Resources (tab)
	Select that minimum driver for your purposes. For example, for CUDA 11.4
	driver version 470 is OK. Higher versions, even though available,
	create problems. It is trial-and-error. Select a driver and 
	reboot to see if it is OK.
	
	Min driver needed?
		See this link for CUDA version and min driver needed:
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html
		OR,
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html#minor-version-compatibility

#####################################
########## EE. Ingestion ############
################ AND ################
############# GPU Usage #############
#####################################


#9.1 Upload a big pdf file:
#			Run in a separate terminal command:
#
#				 nvidia-smi
#
# 			Under GPU-Util one finds GPU utilization
#			increasing from 0% to 33% or some other percentage.

# 9.2 Multiple pdf files upload:
#			Multiple pdf files can be uploaded at the same time.
			But GPU usage increases.
#
# 9.3 Observe Ingestion process: 
#			After uploading file(s) look at the command shell to 
#			observe the ingestion process
#						
#
# 9.4 Ingestion from command line:
#			Ingestion presently gives problems if the following command
#			is used:
				make ingest /home/ashok/pdfFiles

#			It goes on asking for installation of dependencies in a circular
#			manner--first one then second then one again then second again...
#
#
# 9.5 File docstore.json in folder local_data/private_gpt contains digested details
#	of indested pdf files. Its a large file. See it as:
#
#		cat docstore.json | more
#
#    There is also an sqlite datastore in folder: privateGPT/local_data/private_gpt/collection/make_this_parameterizable_per_api_call/storage.sqlite
#	You can copy it to /home/ashok folder and see its contents, as follows:
#		a. Install sqlite3:  sudo apt install sqlite3
#		b. Open it: sqlite3 storage.sqlite 
#		c. Check tables: sqlite> .tables
#		d. sqlite> select * from points;   # This is the only table in it.
#		(Refer: StackOverflow: https://stackoverflow.com/a/55105501


# 9.6 Screenshots of nvidia-smi commands:

				
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   47C    P5    13W /  N/A |    817MiB /  6044MiB |     33%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama              61MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                483MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+

## 30%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:17:50 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   62C    P2    49W /  N/A |   1277MiB /  6044MiB |     30%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                521MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


## 52%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:18:49 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   64C    P2    58W /  N/A |   1279MiB /  6044MiB |     52%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                523MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+



#####################################
############# FF. About #############
############  Profiles ##############
#####################################
# Ref: https://docs.privategpt.dev/manual/general-configuration/configuration#environment-variable-pgpt_profiles


Environment variable PGPT_PROFILES
-----------------------------------
	By default, the profile definition in 'settings.yaml' is loaded.
	Using this env var you can load additional profiles; format is
	a comma separated list of profile names. This will merge
	settings-{profile}.yaml on top of the base settings file.

	For example: PGPT_PROFILES=local,cuda will load settings-local.yaml
	and settings-cuda.yaml, their contents will be merged with later 
	profiles properties overriding values of earlier ones like settings.yaml.

	During testing, the test profile will be active along with the default,
	therefore settings-test.yaml file is required.



#####################################
############# GG. About #############
############# User Interface ########
#####################################
Ref: https://docs.privategpt.dev/manual/user-interface/user-interface-gradio-manual

----------------
Execution Modes
----------------

	It has 3 modes of execution (you can select in the top-left):

	    Query Docs: uses the context from the ingested documents to
	    		answer the questions posted in the chat. It also
	    		takes into account previous chat messages as context.
	            	Makes use of /chat/completions API with use_context=true and no context_filter.
	            	
	 Search in Docs: fast search that returns the 4 most related text chunks,
	 		 together with their source document and page.
        		 Makes use of /chunks API with no context_filter, limit=4 and prev_next_chunks=0.
        		 
	  	LLM Chat: simple, non-contextual chat with the LLM. The ingested
	  		  documents won’t be taken into account, only the previous messages.
        		  Makes use of /chat/completions API with use_context=false

------------------------
System Prompt Examples:
-------------------------

	The system prompt can effectively provide your chat bot specialized roles,
	and results tailored to the prompt you have given the model. Examples of 
	system prompts can be be found here: ChatGPT-3.5 Roles:
	https://www.w3schools.com/gen_ai/chatgpt-3-5/chatgpt-3-5_roles.php

	Some interesting examples to try include:

    		You are -X-. You have all the knowledge and personality of -X-. Answer
    		as if you were -X- using their manner of speaking and vocabulary.
        
        	Example: You are Shakespeare. You have all the knowledge and personality
        		 of Shakespeare. Answer as if you were Shakespeare using their manner
        		 of speaking and vocabulary.

			You are an expert (at) -role-. Answer all questions using your expertise
			on -specific domain topic-.

        	Example: You are an expert software engineer. Answer all questions using your 
        		 expertise on Python.

			You are a -role- bot, respond with -response criteria needed-. If 
			no -response criteria- is needed, respond with -alternate response-.
			
        	Example: You are a grammar checking bot, respond with any grammatical corrections
        		 needed. If no corrections are needed, respond with “verified”.
        		 
        		 
#####################################
######## HH. poetry problems#########
#####################################

Installation using poetry may hang. Solutions are
as below:
Refer: 	https://github.com/python-poetry/poetry/issues/3352
	https://github.com/python-poetry/poetry/issues/7235


One set of solutions:
---------------------
Clean up as follows and then begin install again:

    poetry env remove --all
    poetry cache clear --all .
    rm -rf $(poetry config cache-dir)/artifacts

IInd set of solution:
---------------------
Issue following command and then begin install:

	export PYTHON_KEYRING_BACKEND=keyring.backends.fail.Keyring


IIIrd report problems:
----------------------
Should you wish to report problems, issue the following command:

	poetry install -vvv        		 
        		 


############ DONE #################






