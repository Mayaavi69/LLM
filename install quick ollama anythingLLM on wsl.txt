# Last amended: 19th May, 2024
# Installing anythingLLM with ollama on ubuntu 22.04
# It works with a GUI
# Ref: YouTube Video: https://www.youtube.com/watch?v=IJYC6zf86lU



######################################################
############### AA. INSTALL Python environment########
######################################################


# 0.0 Install Anaconda, if not installed

    wget -c https://repo.anaconda.com/archive/Anaconda3-2024.02-1-Linux-x86_64.sh

	bash Anaconda3-2024.02-1-Linux-x86_64.sh


# 0.1 Close and open Ubuntu terminal


# 0.2 Syntax to remove conda env -- myenv

        conda remove -y --name langchain --all

# 0.3 Install some needed software:

sudo apt update
sudo apt install -y net-tools  curl git-all build-essential





# 1.0 Create conda environment and install packages
#       COpy and paste in one go:

conda create -y --name langchain python=3.11
conda activate langchain
conda config --add channels conda-forge
conda install -y spyder jupyter jupyterlab pandas numpy pip-tools ipython langchain beautifulsoup4 pypdf transformers fastai::accelerate huggingface_hub git openai streamlit
conda install -y -c huggingface -c conda-forge datasets
pip install ollama chromadb langchain-experimental


######1.1 DONE UP TO HERE

######################################################
############### AA.2 INSTALL CUDA ####################
############# AND Appropriate driver #################
######################################################

# 2.0 Install CUDA as in this video. Installation depends upon
#	which GPU is available. For GPU 1060 or 730 install
#	 CUDA 11.4



######################################################
############### AA.3 Remaining installations #########
######################################################




######################################################
############### AA.4 INSTALL ollama ##################
######################################################
# 4.0 Install/Uninstall Ollama by following instructions here:
#     See https://github.com/ollama/ollama/blob/main/docs/linux.md:

# 4.0.1 Install as::

curl -fsSL https://ollama.com/install.sh | sh

#  Install the models to be used, the default settings-ollama.yaml
#  is configured to user mistral 7b LLM (~4GB) and nomic-embed-text Embeddings (~275MB).
#  In privateGPT, ollama configuration file is: ~/privateGPT/settings-ollama.yaml

# 4.0.2 Issue following commands to download mistral and nomic-embed-text:
#        Downloaded models are saved to folder /usr/share/ollama/.ollama/models

ollama pull mistral
ollama pull nomic-embed-text

# 4.0.2.1 You can also specify the number of parameters.
#	   Pull ollama:13b model, as:

ollama pull llama2:13b

# And run it as:

ollama run llama2:13b

# 4.0.2.2 Just entering the following command will pull a 7b model and then run
#         but not the already existing 13b model:

ollama run llama2


# 4.0.3 You can test ollama, as:

ollama run mistral

# 4.0.4 On terminal, write a question to get an answer.
#       Exit by typing /bye

# 4.0.5 ollama supports GPUs with compute capability of 5.


# 4.0.6 List all models downloaded:

ollama list


# 4.0.7 Just enter 'ollama' to get ollama help.

Available Commands:
  serve       Start ollama AND also to know ollama port		<===
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models    					<====
  cp          Copy a model
  rm          Remove a model
  help        Help about any command




# 5.0    On reboot ollama starts by itself. The following command is NOT NEEEDED:
#	 It is also very difficult to kill olama process (even by kill -9). After
#	 killing another ollama process starts.

#        ollama serve  # Also informs ollama port: 11434


# 5.0.1 ollama service can be started/restarted/shut, as:
#	Check as: ps aux | grep ollama

sudo systemctl start/stop ollama

# 5.0.2 View ollama logs as:

	journalctl -u ollama  -e


# 5.0.1 On restart, issue again the following commands:

	a. conda activate langchain


# 5.0.2 Install tensorflow:

pip install tensorflow


######################################################
############### BB. anythingLLM ######################
######################################################


# 6.0 Download anythingLLM from here:
#     https://useanything.com/download


# 6.1 It is an App Image. Make it executable, as:

	chmod +x AnythingLLMDesktop.AppImage
	
# 6.1.1 Then execute it, as:

	./AnythingLLMDesktop.AppImage

# 6.2  And begin configuring it as:	
	
#	a. Click Get Started
#	b. Select Ollama as LLM Preference
#	c. Write Ollama base URL, as:  http://localhost:11434
#		Just writing 'localhost:11434' would not work. http://
#			has to be written also.
#	d. Press Tab and under Chat model selection,
#	     you should see a list of models installed. Select one of them
#	f. Under Token Context Window, set token size of 4096.
	   (By default, Ollama uses a context window size of 2048 tokens.
	    It can be change: See this link of FAQ under ollama/docs/faq.md:
	    https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size )
#	g. In the next screen, select 'AnythingLLMEmbdder'
#	h. In the next screen select LanceDB
#		(see below for using chromadb)
#	i  Click Next again
#	j. Select 'For education', click Next
#	h. Give a Workspace name. AnythingLLM can have
#	i.  multiple workspaces for difft document types.
#	Done


# 6.2.1 USing chromadb vector database:
#       Ref: https://docs.trychroma.com/usage-guide#running-chroma-in-clientserver-mode
#
#	Start chromadb in client/server mode, as:
#		In Linux:
#			$conda activate langchain
#			$chroma run --path /home/ashok/Documents/chroma
#		In Windows:
#			>conda activate langchain
#			>chroma run --path C:\Users\ashok\OneDrive\Documents\chroma
#		Open browser and check:
#			http://localhost:8000
#		You should also have an sqlite3 database in chroma folder.
#		In anythingLLM, just write the chroma URL, as:
#			http://localhost:8000 and leav other two fields blank
#			and save changes. No need to specify API key or token.
#

# 6.2.2 Have a look at these youtube videos:
#		1. https://www.youtube.com/watch?v=IJYC6zf86lU
#		2. https://www.youtube.com/watch?v=NuZ0n0LPZ5E&t=83s


# 6.3 Configuration data of AppImage in Linux is within
#	.config folder. Delete from within it 
#	the folder: 'anythingllm-desktopit'
#	 to restore AppImage's original state. 	

# 6.3a In Windows, you have to first uninstall anythingLLM and
#	then delete anythingLLM related files under:
#
#		C:\Users\ashok\AppData\Local\Programs
#   	C:\Users\ashok\AppData\Roaming\anythingllm-desktop\storage
#
#	And then reinstall anythingLLM
	

	    
# 6,4 Ollama on network. Suppose AnythingLLM app image is
#     on a remote machine, then how to access ollama?

	(By default, ollama runs only on localhost. To make
	it accesible on network, access its configuration file:
	sudo nano /etc/systemd/system/ollama.service
	ANd add a line under [Service]
	[Service]
	Environment="OLLAMA_HOST=0.0.0.0"
	Restart ollama, as:
	systemctl daemon-reload
	systemctl restart ollama
	It would now be available on network, ie accsible, as:
	Write Ollama base URL, as:  http://192.240.1.51:11434



######################################
########## DD. NVIDIA driver ##########
############### Problems ##############
#######################################

# 7.0
	It is possible that after privateGPT installs CUDA
	Or you have installed CUDA, then mutiple NVIDIA
	drivers (not one) could be installed. It is also
	possible that when you reboot Ubuntu, screen resolution
	changes rather increases ao that everything is very small.
	Then, in Ubuntu search and open Software & Updates--> Resources (tab)
	Select that minimum driver for your purposes. For example, for CUDA 11.4
	driver version 470 is OK. Higher versions, even though available,
	create problems. It is trial-and-error. Select a driver and 
	reboot to see if it is OK.
	
	Min driver needed?
		See this link for CUDA version and min driver needed:
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html
		OR,
		https://docs.nvidia.com/deploy/cuda-compatibility/index.html#minor-version-compatibility

#####################################
########## EE. Ingestion ############
################ AND ################
############# GPU Usage #############
#####################################


#8.1 Upload a big pdf file:
#			Run in a separate terminal command:
#
#				 nvidia-smi
#
# 			Under GPU-Util one finds GPU utilization
#			increasing from 0% to 33% or some other percentage.

# 8.2 Multiple pdf files upload:
#			Multiple pdf files can be uploaded at the same time.
			But GPU usage increases.
#

# 8.3 Screenshots of nvidia-smi commands:

				
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   47C    P5    13W /  N/A |    817MiB /  6044MiB |     33%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama              61MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                483MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+

## 30%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:17:50 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   62C    P2    49W /  N/A |   1277MiB /  6044MiB |     30%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                521MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


## 52%

(base) ashok@ashok:~$ nvidia-smi
Thu Apr  4 03:18:49 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| N/A   64C    P2    58W /  N/A |   1279MiB /  6044MiB |     52%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1125      C   /usr/local/bin/ollama             483MiB |
|    0   N/A  N/A      1527      G   /usr/lib/xorg/Xorg                523MiB |
|    0   N/A  N/A      1742      G   /usr/bin/gnome-shell               94MiB |
|    0   N/A  N/A     32520      G   ...6/usr/lib/firefox/firefox      172MiB |
+-----------------------------------------------------------------------------+


------------------------
System Prompt Examples:
-------------------------

	The system prompt can effectively provide your chat bot specialized roles,
	and results tailored to the prompt you have given the model. Examples of 
	system prompts can be be found here: ChatGPT-3.5 Roles:
	https://www.w3schools.com/gen_ai/chatgpt-3-5/chatgpt-3-5_roles.php

	Some interesting examples to try include:

    		You are -X-. You have all the knowledge and personality of -X-. Answer
    		as if you were -X- using their manner of speaking and vocabulary.
        
        	Example: You are Shakespeare. You have all the knowledge and personality
        		 of Shakespeare. Answer as if you were Shakespeare using their manner
        		 of speaking and vocabulary.

			You are an expert (at) -role-. Answer all questions using your expertise
			on -specific domain topic-.

        	Example: You are an expert software engineer. Answer all questions using your 
        		 expertise on Python.

			You are a -role- bot, respond with -response criteria needed-. If 
			no -response criteria- is needed, respond with -alternate response-.
			
        	Example: You are a grammar checking bot, respond with any grammatical corrections
        		 needed. If no corrections are needed, respond with “verified”.
        		 
 	 
        		 


############ DONE #################






