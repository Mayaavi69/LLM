{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/LLMs/blob/main/Using_llamacpp_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c13d4e60-b55c-4e30-aeb9-a4ff46a406a9",
      "metadata": {
        "id": "c13d4e60-b55c-4e30-aeb9-a4ff46a406a9"
      },
      "outputs": [],
      "source": [
        "# Last amended: 09/06/2024\n",
        "# Ref: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama\n",
        "#      https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
        "#      YouTube video: https://www.youtube.com/watch?v=rCDf0MSzUCg\n",
        "\n",
        "# Colab version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8740c082-43f8-44aa-a46f-1d80db97b3cb",
      "metadata": {
        "id": "8740c082-43f8-44aa-a46f-1d80db97b3cb"
      },
      "source": [
        "llama-cpp allows to access models downloaded from huggingface on local machine. Download a model from huggingface."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf13d94-5aab-43e7-9c87-3f3f24f08868",
      "metadata": {
        "id": "5cf13d94-5aab-43e7-9c87-3f3f24f08868"
      },
      "source": [
        "<h3>My notes for Jupyter notebook:</h3>\n",
        "\n",
        "$# 0.0 Removing an environment:    \n",
        "\n",
        ">`conda remove --name llamacpp --all` <br>    \n",
        "\n",
        "$# 0.1 Create conda environment with python 3.11    \n",
        "\n",
        ">`cd ~/` <br>\n",
        "\n",
        ">`conda config --add channels conda-forge`<br>\n",
        "\n",
        ">`conda create --name llamacpp python=3.11 ipython spyder jupyterlab notebook`<br>\n",
        "\n",
        ">`conda activate llamacpp` <br>\n",
        "\n",
        "$# 0.2 Make a directory to house our files:    \n",
        "\n",
        ">`mkdir llamacpp` <br>    \n",
        ">`cd llamacpp` <br>\n",
        "\n",
        "$# 0.3 Make another folder: models   \n",
        "       to keep downloaded models:    \n",
        "\n",
        ">`mkdir models` <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a452bb82-0194-4b19-9f23-2847c9ab4942",
      "metadata": {
        "id": "a452bb82-0194-4b19-9f23-2847c9ab4942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50847c32-531f-4a69-c9ed-7e5c4468ceba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# 1.0 Install llamacpp\n",
        "\n",
        "! pip install llama-cpp-python --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d16de635-ff74-483c-8250-970d7428c166",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d16de635-ff74-483c-8250-970d7428c166",
        "outputId": "05ab8b32-8f89-4280-9e28-b2124c6ce5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Downloading 'zephyr-7b-beta.Q4_K_M.gguf' to '.huggingface/download/zephyr-7b-beta.Q4_K_M.gguf.503580dce392c6e64669ad21a77023ba2a17baa0c381250fb67c11ba6406a85e.incomplete'\n",
            "zephyr-7b-beta.Q4_K_M.gguf: 100% 4.37G/4.37G [00:32<00:00, 135MB/s]\n",
            "Download complete. Moving file to zephyr-7b-beta.Q4_K_M.gguf\n",
            "zephyr-7b-beta.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Download following huggingface 'gguf + text-generation' model\n",
        "#      into current folder as:\n",
        "#     https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF\n",
        "#\n",
        "#     Colab comes with huggingface-cli preloaded.\n",
        "\n",
        "! huggingface-cli download TheBloke/zephyr-7B-beta-GGUF zephyr-7b-beta.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1 Download following huggingface 'gguf + text-generation + tiny' model\n",
        "#      into current folder. Its repo is:\n",
        "#     https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6\n",
        "#\n",
        "#     Colab comes with huggingface-cli preloaded.\n",
        "\n",
        "! huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v0.6  ggml-model-q4_0.gguf --local-dir . --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMA3wlp0V-dB",
        "outputId": "2293f9f4-53cb-481d-ea06-a517ad9e8795"
      },
      "id": "cMA3wlp0V-dB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Downloading 'ggml-model-q4_0.gguf' to '.huggingface/download/ggml-model-q4_0.gguf.ec21b060225c96d9a985886566c2d01d0c63498405f4e0448dfee0491d73219f.incomplete'\n",
            "ggml-model-q4_0.gguf: 100% 637M/637M [00:11<00:00, 56.0MB/s]\n",
            "Download complete. Moving file to ggml-model-q4_0.gguf\n",
            "ggml-model-q4_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "56caac08-3afd-43ca-976e-b56a76683193",
      "metadata": {
        "id": "56caac08-3afd-43ca-976e-b56a76683193"
      },
      "outputs": [],
      "source": [
        "# 1.2 Import libraries:\n",
        "import llama_cpp\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "89aa3459-85d2-4c52-8613-13046b812528",
      "metadata": {
        "id": "89aa3459-85d2-4c52-8613-13046b812528"
      },
      "outputs": [],
      "source": [
        "# 1.3\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66917f03-7207-4b71-82af-546537b56cbd",
      "metadata": {
        "id": "66917f03-7207-4b71-82af-546537b56cbd"
      },
      "source": [
        "### About Llama class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ee5303-321a-4cff-8d25-779dbe1580e1",
      "metadata": {
        "id": "b5ee5303-321a-4cff-8d25-779dbe1580e1"
      },
      "source": [
        "The `Llama` class imported above is the main constructor leveraged when using `Llama.cpp`,   \n",
        "and it takes several parameters and is not limited to the ones below.   \n",
        "The complete list of parameters is provided in the [official documentation](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama):\n",
        "\n",
        "* <b>model_path</b>: The path to the Llama model file being used\n",
        "* <b>prompt</b>: The input prompt to the model. This text is tokenized and passed to the model.\n",
        "* <b>device</b>: The device to use for running the Llama model; such a device can be either CPU or GPU.\n",
        "* <b>max_tokens</b>: The maximum number of tokens to be generated in the model’s response\n",
        "* <b>stop</b>: A list of strings that will cause the model generation process to stop\n",
        "* <b>temperature</b>: This value ranges between 0 and 1. The lower the value, the more deterministic the end result. On the other hand, a higher value leads to more randomness, hence more diverse and creative output.\n",
        "* <b>top_p</b>: Is used to control the diversity of the predictions, meaning that it selects the most probable tokens whose cumulative probability exceeds a given threshold. Starting from zero, a higher value increases the chance of finding a better output but requires additional computations.\n",
        "* <b>echo</b>: A boolean used to determine whether the model includes the original prompt at the beginning (True) or does not include it (False)\n",
        "* <b>stop</b>: A list of strings to stop generation when encountered.\n",
        "* <b>chat_format</b>:  String specifying the chat format to use when calling [create_chat_completion](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34faa4b8-3bd7-4867-b32e-7aa60bd1d066",
      "metadata": {
        "id": "34faa4b8-3bd7-4867-b32e-7aa60bd1d066"
      },
      "source": [
        "### Start with a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "58f14996-5c85-4d11-9bcf-5de88c56aa26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "58f14996-5c85-4d11-9bcf-5de88c56aa26",
        "outputId": "29d7b6cc-64f6-4f56-9983-6245e166a522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 201 tensors from /content/ggml-model-q4_0.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = models\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q4_0:  155 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q4_0\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 606.53 MiB (4.63 BPW) \n",
            "llm_load_print_meta: general.name     = models\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
            "llm_load_tensors:        CPU buffer size =   606.53 MiB\n",
            ".....................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    66.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'models', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "# 2.0\n",
        "\n",
        "# modelPath= \"/home/ashok/llamacpp/models/zephyr-7b-beta.Q4_K_M.gguf\"\n",
        "# modelPath = \"/content/zephyr-7b-beta.Q4_K_M.gguf\"\n",
        "modelPath = \"/content/ggml-model-q4_0.gguf\"\n",
        "model = llama_cpp.Llama(model_path= modelPath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "30cb9b8f-9ada-4d2d-bea0-6579415ec95b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "30cb9b8f-9ada-4d2d-bea0-6579415ec95b",
        "outputId": "72318747-c537-424b-c97a-bb518ccc19a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "llama_cpp.llama.Llama"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_cpp.llama.Llama</b><br/>def __call__(prompt: str, suffix: Optional[str]=None, max_tokens: Optional[int]=16, temperature: float=0.8, top_p: float=0.95, min_p: float=0.05, typical_p: float=1.0, logprobs: Optional[int]=None, echo: bool=False, stop: Optional[Union[str, List[str]]]=[], frequency_penalty: float=0.0, presence_penalty: float=0.0, repeat_penalty: float=1.1, top_k: int=40, stream: bool=False, seed: Optional[int]=None, tfs_z: float=1.0, mirostat_mode: int=0, mirostat_tau: float=5.0, mirostat_eta: float=0.1, model: Optional[str]=None, stopping_criteria: Optional[StoppingCriteriaList]=None, logits_processor: Optional[LogitsProcessorList]=None, grammar: Optional[LlamaGrammar]=None, logit_bias: Optional[Dict[str, float]]=None) -&gt; Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py</a>High-level Python wrapper for a llama.cpp model.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 60);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# 2.0.1\n",
        "type(model)   # Llama class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58bdfa15-dbb9-440c-b961-7de8f78a5474",
      "metadata": {
        "id": "58bdfa15-dbb9-440c-b961-7de8f78a5474"
      },
      "source": [
        "### Predict next few words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f4ff4837-d875-4485-8931-fc8e98139154",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "f4ff4837-d875-4485-8931-fc8e98139154",
        "outputId": "7ab56ccc-a0dc-4429-f3d7-daf4c8805a00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "llama_cpp.llama.Llama"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_cpp.llama.Llama</b><br/>def __call__(prompt: str, suffix: Optional[str]=None, max_tokens: Optional[int]=16, temperature: float=0.8, top_p: float=0.95, min_p: float=0.05, typical_p: float=1.0, logprobs: Optional[int]=None, echo: bool=False, stop: Optional[Union[str, List[str]]]=[], frequency_penalty: float=0.0, presence_penalty: float=0.0, repeat_penalty: float=1.1, top_k: int=40, stream: bool=False, seed: Optional[int]=None, tfs_z: float=1.0, mirostat_mode: int=0, mirostat_tau: float=5.0, mirostat_eta: float=0.1, model: Optional[str]=None, stopping_criteria: Optional[StoppingCriteriaList]=None, logits_processor: Optional[LogitsProcessorList]=None, grammar: Optional[LlamaGrammar]=None, logit_bias: Optional[Dict[str, float]]=None) -&gt; Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py</a>High-level Python wrapper for a llama.cpp model.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 60);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     615.03 ms\n",
            "llama_print_timings:      sample time =       1.83 ms /     3 runs   (    0.61 ms per token,  1638.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     614.91 ms /     9 tokens (   68.32 ms per token,    14.64 tokens per second)\n",
            "llama_print_timings:        eval time =     363.05 ms /     2 runs   (  181.52 ms per token,     5.51 tokens per second)\n",
            "llama_print_timings:       total time =     983.12 ms /    11 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 times\n"
          ]
        }
      ],
      "source": [
        "# 3.0 Predict next few words:\n",
        "\n",
        "type(model)\n",
        "print(model(\"The quick brown fox jumps \", stop=[\".\"])[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "15033b72-c83a-4581-9d62-e9a175ad0fd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15033b72-c83a-4581-9d62-e9a175ad0fd3",
        "outputId": "ff85d6d4-32e2-446c-8a98-6a54d098bcab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     615.03 ms\n",
            "llama_print_timings:      sample time =       1.74 ms /     3 runs   (    0.58 ms per token,  1722.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     611.75 ms /     3 runs   (  203.92 ms per token,     4.90 tokens per second)\n",
            "llama_print_timings:       total time =     625.74 ms /     3 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-c7ec828f-0fc5-4016-8184-b259a015001a',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1717892439,\n",
              " 'model': '/content/ggml-model-q4_0.gguf',\n",
              " 'choices': [{'text': '3 times',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 9, 'completion_tokens': 3, 'total_tokens': 12}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 3.0.1 The above can be broken down as:\n",
        "\n",
        "model(\"The quick brown fox jumps \", stop=[\".\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1042b755-d15c-439d-9e0d-24f5c15782bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1042b755-d15c-439d-9e0d-24f5c15782bf",
        "outputId": "dbd277bd-4865-42b3-ef6f-0ae56d8c4d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     615.03 ms\n",
            "llama_print_timings:      sample time =       1.72 ms /     3 runs   (    0.57 ms per token,  1745.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     448.90 ms /     3 runs   (  149.63 ms per token,     6.68 tokens per second)\n",
            "llama_print_timings:       total time =     452.91 ms /     3 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# 3.0.2\n",
        "txt = model(\"The quick brown fox jumps \", stop=[\".\"])\n",
        "type(txt)   # dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "55f2bc52-5696-4549-aabf-6fd94c6fe740",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "55f2bc52-5696-4549-aabf-6fd94c6fe740",
        "outputId": "f4628ecd-1a3f-42c0-f5ae-150c407b4cef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-d69c8cf7-0249-404f-a243-c23563af7be4',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1717892452,\n",
              " 'model': '/content/ggml-model-q4_0.gguf',\n",
              " 'choices': [{'text': '3 times',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 9, 'completion_tokens': 3, 'total_tokens': 12}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '3 times', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '3 times', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3 times'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# 3.0.3\n",
        "txt\n",
        "txt['choices']\n",
        "txt['choices'][0]\n",
        "txt['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3478dcb-b087-4b3a-91c3-fbf16b1c2cdb",
      "metadata": {
        "id": "b3478dcb-b087-4b3a-91c3-fbf16b1c2cdb"
      },
      "source": [
        "Refer here for [create_chat_completion()](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76437fd5-17fa-42ac-af90-9de30a08d59f",
      "metadata": {
        "id": "76437fd5-17fa-42ac-af90-9de30a08d59f"
      },
      "source": [
        "### Generate reply to chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "662b6605-8b1a-4bf4-8387-f76222a3d20d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "662b6605-8b1a-4bf4-8387-f76222a3d20d",
        "outputId": "c34d8ad4-3ea7-4be8-a871-482a893f9f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 201 tensors from /content/ggml-model-q4_0.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = models\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   45 tensors\n",
            "llama_model_loader: - type q4_0:  155 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 22\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 5632\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 1B\n",
            "llm_load_print_meta: model ftype      = Q4_0\n",
            "llm_load_print_meta: model params     = 1.10 B\n",
            "llm_load_print_meta: model size       = 606.53 MiB (4.63 BPW) \n",
            "llm_load_print_meta: general.name     = models\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
            "llm_load_tensors:        CPU buffer size =   606.53 MiB\n",
            ".....................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    66.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 710\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'models', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
            "\n",
            "llama_print_timings:        load time =    3664.34 ms\n",
            "llama_print_timings:      sample time =     304.23 ms /   494 runs   (    0.62 ms per token,  1623.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3664.24 ms /    18 tokens (  203.57 ms per token,     4.91 tokens per second)\n",
            "llama_print_timings:        eval time =   79058.41 ms /   493 runs   (  160.36 ms per token,     6.24 tokens per second)\n",
            "llama_print_timings:       total time =   83721.17 ms /   511 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-3b262762-3f5f-475e-8614-6cd7480fb777', 'object': 'chat.completion', 'created': 1717892469, 'model': '/content/ggml-model-q4_0.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '\\n\\n[INST]I don\\'t know what you mean by \"meaning of life\". I just want to live my life.  [/INST]\\n\\n[INST]You don\\'t understand, you\\'re too young.  [/INST]\\n\\n[INST]No, I do understand. I just want to be happy and fulfilled in this world.  [/INST]\\n\\n[INST]I see. You can\\'t have it all.  [/INST]\\n\\n[INST]You don\\'t understand. I\\'m not trying to be perfect or achieve everything.  [/INST]\\n\\n[INST]No, you\\'re wrong. You just want to be happy and fulfilled in this world.  [/INST]\\n\\n[INST]I see. You can\\'t have it all.  [/INST]\\n\\n[INST]You don\\'t understand. I\\'m not trying to be perfect or achieve everything.  [/INST]\\n\\n[INST]No, you\\'re wrong. You just want to be happy and fulfilled in this world.  [/INST]\\n\\n[INST]I see. You can\\'t have it all.  [/INST]\\n\\n[INST]You don\\'t understand. I\\'m not trying to be perfect or achieve everything.  [/INST]\\n\\n[INST]No, you\\'re wrong. You just want to be happy and fulfilled in this world.  [/INST]\\n\\n[INST]I see. You can\\'t have it all.  [/INST]\\n\\n[INST]You don\\'t understand. I\\'m not trying to be perfect or achieve everything.  [/INST]\\n\\n[INST]No, you\\'re wrong. You just want to be happy and fulfilled in this world.  [/INST]\\n\\n[INST]I see. You can\\'t have it all.  [/INST]\\n\\n[INST]You don\\'t understand. I\\'m not trying to be perfect or achieve everything.  [/INST]\\n\\n[INST]No, you\\'re wrong. You just want to be happy and fulfilled in this world.  [/INST]\\n\\n[INST]I see. You can\\'t have it all'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 494, 'total_tokens': 512}}\n"
          ]
        }
      ],
      "source": [
        "# 4.0 Load a chat model:\n",
        "\n",
        "import llama_cpp\n",
        "model = llama_cpp.Llama(model_path=modelPath, chat_format=\"llama-2\" )\n",
        "print(model.create_chat_completion(\n",
        "                                   messages=[                                    # A list of messages\n",
        "                                              { \"role\": \"user\",\n",
        "                                                \"content\": \"what is the meaning of life?\"\n",
        "                                              }\n",
        "                                            ]\n",
        "                                    )\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c05f17-80b8-4ce2-bfaf-0405573b489d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "47c05f17-80b8-4ce2-bfaf-0405573b489d",
        "outputId": "fa493dab-f805-4349-8461-820d1b3836cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /content/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "\n",
            "llama_print_timings:        load time =    8041.02 ms\n",
            "llama_print_timings:      sample time =     322.53 ms /   490 runs   (    0.66 ms per token,  1519.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8040.92 ms /    22 tokens (  365.50 ms per token,     2.74 tokens per second)\n",
            "llama_print_timings:        eval time =  317747.02 ms /   489 runs   (  649.79 ms per token,     1.54 tokens per second)\n",
            "llama_print_timings:       total time =  326746.46 ms /   511 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.445971608161926\n"
          ]
        }
      ],
      "source": [
        "# 4.0.1 Another chat\n",
        "\n",
        "import llama_cpp,time\n",
        "model = llama_cpp.Llama(model_path=modelPath, chat_format=\"llama-2\" )\n",
        "start = time.time()\n",
        "txt = model.create_chat_completion(\n",
        "                                   messages=[                                    # A list of messages\n",
        "                                              { \"role\": \"user\",\n",
        "                                                \"content\": \"Tell me how to classify target in iris dataset\"\n",
        "                                              }\n",
        "                                            ]\n",
        "                                    )\n",
        "end = time.time()\n",
        "print((end-start)/60)        # 1.4791114568710326 min without gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf91126b-cb1e-474c-bddb-c962b154fba8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bf91126b-cb1e-474c-bddb-c962b154fba8",
        "outputId": "7ff30185-4d85-4897-ad54-909b9081f77a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-e4c0c899-79ea-4904-8730-e6fb99d22532',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1717850894,\n",
              " 'model': '/content/zephyr-7b-beta.Q4_K_M.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': \"\\n\\n[ASS]\\nThe Iris dataset is a commonly used machine learning dataset that contains measurements of physical characteristics of three species of irises (Iris setosa, Iris versicolour, and Iris virginica). The dataset consists of 150 samples, with each sample representing one flower.\\n\\nTo classify a target in the iris dataset, you can follow these steps:\\n\\n1. Load the dataset using a library like NumPy or Pandas in Python or R.\\n2. Split the dataset into training and testing sets.\\n3. Choose a machine learning algorithm to train on the training set. Some popular algorithms for this dataset include K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Random Forests.\\n4. Train the chosen algorithm on the training set.\\n5. Evaluate the performance of the trained model on the testing set using metrics like accuracy, precision, recall, and F1 score.\\n6. Make predictions for new, unseen data by feeding it through the trained model.\\n\\nHere's an example implementation in Python using Scikit-Learn:\\n\\n```python\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\\n# Load the dataset\\niris = load_iris()\\n\\n# Split into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\\n\\n# Train the KNN model on the training set\\nknn = KNeighborsClassifier()\\nknn.fit(X_train, y_train)\\n\\n# Evaluate the performance of the trained model on the testing set\\ny_pred = knn.predict(X_test)\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred, average='weighted')\\nrecall = recall_\"},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 22, 'completion_tokens': 490, 'total_tokens': 512}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'index': 0,\n",
              " 'message': {'role': 'assistant',\n",
              "  'content': \"\\n\\n[ASS]\\nThe Iris dataset is a commonly used machine learning dataset that contains measurements of physical characteristics of three species of irises (Iris setosa, Iris versicolour, and Iris virginica). The dataset consists of 150 samples, with each sample representing one flower.\\n\\nTo classify a target in the iris dataset, you can follow these steps:\\n\\n1. Load the dataset using a library like NumPy or Pandas in Python or R.\\n2. Split the dataset into training and testing sets.\\n3. Choose a machine learning algorithm to train on the training set. Some popular algorithms for this dataset include K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Random Forests.\\n4. Train the chosen algorithm on the training set.\\n5. Evaluate the performance of the trained model on the testing set using metrics like accuracy, precision, recall, and F1 score.\\n6. Make predictions for new, unseen data by feeding it through the trained model.\\n\\nHere's an example implementation in Python using Scikit-Learn:\\n\\n```python\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\\n# Load the dataset\\niris = load_iris()\\n\\n# Split into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\\n\\n# Train the KNN model on the training set\\nknn = KNeighborsClassifier()\\nknn.fit(X_train, y_train)\\n\\n# Evaluate the performance of the trained model on the testing set\\ny_pred = knn.predict(X_test)\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred, average='weighted')\\nrecall = recall_\"},\n",
              " 'logprobs': None,\n",
              " 'finish_reason': 'length'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n[ASS]\\nThe Iris dataset is a commonly used machine learning dataset that contains measurements of physical characteristics of three species of irises (Iris setosa, Iris versicolour, and Iris virginica). The dataset consists of 150 samples, with each sample representing one flower.\\n\\nTo classify a target in the iris dataset, you can follow these steps:\\n\\n1. Load the dataset using a library like NumPy or Pandas in Python or R.\\n2. Split the dataset into training and testing sets.\\n3. Choose a machine learning algorithm to train on the training set. Some popular algorithms for this dataset include K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Random Forests.\\n4. Train the chosen algorithm on the training set.\\n5. Evaluate the performance of the trained model on the testing set using metrics like accuracy, precision, recall, and F1 score.\\n6. Make predictions for new, unseen data by feeding it through the trained model.\\n\\nHere's an example implementation in Python using Scikit-Learn:\\n\\n```python\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\\n# Load the dataset\\niris = load_iris()\\n\\n# Split into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\\n\\n# Train the KNN model on the training set\\nknn = KNeighborsClassifier()\\nknn.fit(X_train, y_train)\\n\\n# Evaluate the performance of the trained model on the testing set\\ny_pred = knn.predict(X_test)\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred, average='weighted')\\nrecall = recall_\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[ASS]\n",
            "The Iris dataset is a commonly used machine learning dataset that contains measurements of physical characteristics of three species of irises (Iris setosa, Iris versicolour, and Iris virginica). The dataset consists of 150 samples, with each sample representing one flower.\n",
            "\n",
            "To classify a target in the iris dataset, you can follow these steps:\n",
            "\n",
            "1. Load the dataset using a library like NumPy or Pandas in Python or R.\n",
            "2. Split the dataset into training and testing sets.\n",
            "3. Choose a machine learning algorithm to train on the training set. Some popular algorithms for this dataset include K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Random Forests.\n",
            "4. Train the chosen algorithm on the training set.\n",
            "5. Evaluate the performance of the trained model on the testing set using metrics like accuracy, precision, recall, and F1 score.\n",
            "6. Make predictions for new, unseen data by feeding it through the trained model.\n",
            "\n",
            "Here's an example implementation in Python using Scikit-Learn:\n",
            "\n",
            "```python\n",
            "from sklearn.datasets import load_iris\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
            "\n",
            "# Load the dataset\n",
            "iris = load_iris()\n",
            "\n",
            "# Split into training and testing sets\n",
            "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
            "\n",
            "# Train the KNN model on the training set\n",
            "knn = KNeighborsClassifier()\n",
            "knn.fit(X_train, y_train)\n",
            "\n",
            "# Evaluate the performance of the trained model on the testing set\n",
            "y_pred = knn.predict(X_test)\n",
            "accuracy = accuracy_score(y_test, y_pred)\n",
            "precision = precision_score(y_test, y_pred, average='weighted')\n",
            "recall = recall_\n"
          ]
        }
      ],
      "source": [
        "# 4.0.2\n",
        "type(txt)\n",
        "txt\n",
        "txt['choices'][0]\n",
        "txt['choices'][0]['message']['content']\n",
        "print(txt['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e4549b-a89b-4c41-abcd-cd3b4ca1b87b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4e4549b-a89b-4c41-abcd-cd3b4ca1b87b",
        "outputId": "0c6b5221-fa76-4fd8-b5cf-0cfc6536178c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /content/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 259\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "\n",
            "llama_print_timings:        load time =    9756.60 ms\n",
            "llama_print_timings:      sample time =     326.98 ms /   490 runs   (    0.67 ms per token,  1498.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9756.49 ms /    22 tokens (  443.48 ms per token,     2.25 tokens per second)\n",
            "llama_print_timings:        eval time =  317958.98 ms /   489 runs   (  650.22 ms per token,     1.54 tokens per second)\n",
            "llama_print_timings:       total time =  328681.45 ms /   511 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.47825809319814\n"
          ]
        }
      ],
      "source": [
        "# 5.0 Another chat with gpu\n",
        "#     Time is about the same\n",
        "#     Why?\n",
        "\n",
        "import llama_cpp,time\n",
        "model = llama_cpp.Llama(model_path=modelPath, chat_format=\"llama-2\", n_gpu_layers = 1 )\n",
        "start = time.time()\n",
        "txt = model.create_chat_completion(\n",
        "                                   messages=[                                    # A list of messages\n",
        "                                              { \"role\": \"user\",\n",
        "                                                \"content\": \"Tell me how to classify target in iris dataset\"\n",
        "                                              }\n",
        "                                            ]\n",
        "                                    )\n",
        "end = time.time()\n",
        "print((end-start)/60)   # 1.4751133998235066"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822af06b-e1b3-4008-85d1-1e88f6fa3002",
      "metadata": {
        "id": "822af06b-e1b3-4008-85d1-1e88f6fa3002"
      },
      "outputs": [],
      "source": [
        "############### DONE ##################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "487aa29d-a35a-4cb2-9b8e-31b29844d256",
      "metadata": {
        "id": "487aa29d-a35a-4cb2-9b8e-31b29844d256"
      },
      "outputs": [],
      "source": [
        "# 1.3 Instanciate the model\n",
        "\n",
        "llama_model = Llama(model_path=\"/home/ashok/llamacpp/models/zephyr-7b-beta.Q4_K_M.gguf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0559d5a7-6441-41de-9e67-332331647566",
      "metadata": {
        "id": "0559d5a7-6441-41de-9e67-332331647566"
      },
      "source": [
        "The `Llama` class imported above is the main constructor leveraged when using `Llama.cpp`,   \n",
        "and it takes several parameters and is not limited to the ones below.   \n",
        "The complete list of parameters is provided in the [official documentation](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama):\n",
        "\n",
        "* <b>model_path</b>: The path to the Llama model file being used\n",
        "* <b>prompt</b>: The input prompt to the model. This text is tokenized and passed to the model.\n",
        "* <b>device</b>: The device to use for running the Llama model; such a device can be either CPU or GPU.\n",
        "* <b>max_tokens</b>: The maximum number of tokens to be generated in the model’s response\n",
        "* <b>stop</b>: A list of strings that will cause the model generation process to stop\n",
        "* <b>temperature</b>: This value ranges between 0 and 1. The lower the value, the more deterministic the end result. On the other hand, a higher value leads to more randomness, hence more diverse and creative output.\n",
        "* <b>top_p</b>: Is used to control the diversity of the predictions, meaning that it selects the most probable tokens whose cumulative probability exceeds a given threshold. Starting from zero, a higher value increases the chance of finding a better output but requires additional computations.\n",
        "* <b>echo</b>: A boolean used to determine whether the model includes the original prompt at the beginning (True) or does not include it (False)\n",
        "* <b>stop</b>: A list of strings to stop generation when encountered.\n",
        "* <b>chat_format</b>:  String specifying the chat format to use when calling [create_chat_completion](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e34e159-0029-42d3-a1d0-7b2aee8673cd",
      "metadata": {
        "id": "0e34e159-0029-42d3-a1d0-7b2aee8673cd"
      },
      "outputs": [],
      "source": [
        "# 1.4 Specify parameters:\n",
        "prompt = \"This is a prompt\"\n",
        "max_tokens = 100\n",
        "temperature = 0.3\n",
        "top_p = 0.1\n",
        "echo = True\n",
        "stop = [\"Q\", \"\\n\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e804ef2-0a44-43d4-8cd9-b187a1c9d7bd",
      "metadata": {
        "id": "1e804ef2-0a44-43d4-8cd9-b187a1c9d7bd",
        "outputId": "92354b42-57ce-453d-f24d-ae79a46010f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ashok/llamacpp/models/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "# 1.5 Execute the model\n",
        "model =  Llama(model_path=\"/home/ashok/llamacpp/models/zephyr-7b-beta.Q4_K_M.gguf\",\n",
        "               prompt = prompt,\n",
        "               max_tokens=max_tokens,\n",
        "               temperature=temperature,\n",
        "               top_p=top_p,\n",
        "               echo=echo,\n",
        "               stop=stop )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f79e53d-06b1-4cfa-905c-6e8143b40cf0",
      "metadata": {
        "id": "3f79e53d-06b1-4cfa-905c-6e8143b40cf0",
        "outputId": "de65ae71-0cab-4392-8057-de7373c86c0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     580.43 ms\n",
            "llama_print_timings:      sample time =       6.79 ms /    16 runs   (    0.42 ms per token,  2357.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2728.96 ms /    16 runs   (  170.56 ms per token,     5.86 tokens per second)\n",
            "llama_print_timings:       total time =    2771.66 ms /    17 tokens\n"
          ]
        }
      ],
      "source": [
        "# 1.6 This is the result\n",
        "mo = model(prompt)\n",
        "final_result = mo[\"choices\"][0][\"text\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a82192-d149-4e9b-84ff-95c814242f86",
      "metadata": {
        "id": "82a82192-d149-4e9b-84ff-95c814242f86",
        "outputId": "5d632aea-94aa-46a1-86b8-085881c82e0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /home/ashok/llamacpp/models/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "CONTEXT_SIZE = 512\n",
        "\n",
        "\n",
        "# LOAD THE MODEL\n",
        "zephyr_model = Llama(model_path=\"/home/ashok/llamacpp/models/zephyr-7b-beta.Q4_K_M.gguf\",\n",
        "                    n_ctx=CONTEXT_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1b31fa-4a75-4c0d-9e88-0b24a1c38852",
      "metadata": {
        "id": "9e1b31fa-4a75-4c0d-9e88-0b24a1c38852"
      },
      "outputs": [],
      "source": [
        "def generate_text_from_prompt(user_prompt,\n",
        "                             max_tokens = 100,\n",
        "                             temperature = 0.3,\n",
        "                             top_p = 0.1,\n",
        "                             echo = True,\n",
        "                             stop = [\"Q\", \"\\n\"]):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   # Define the parameters\n",
        "   model_output = zephyr_model(\n",
        "       user_prompt,\n",
        "       max_tokens=max_tokens,\n",
        "       temperature=temperature,\n",
        "       top_p=top_p,\n",
        "       echo=echo,\n",
        "       stop=stop,\n",
        "   )\n",
        "\n",
        "\n",
        "   return model_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd7fa0f-0dc3-429e-87ab-d28a2b76476d",
      "metadata": {
        "id": "7fd7fa0f-0dc3-429e-87ab-d28a2b76476d",
        "outputId": "494775e4-a0c2-44ff-f6ea-8f5865b6911f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    1637.24 ms\n",
            "llama_print_timings:      sample time =       5.66 ms /    11 runs   (    0.51 ms per token,  1943.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1637.18 ms /    13 tokens (  125.94 ms per token,     7.94 tokens per second)\n",
            "llama_print_timings:        eval time =    1919.18 ms /    10 runs   (  191.92 ms per token,     5.21 tokens per second)\n",
            "llama_print_timings:       total time =    3617.58 ms /    23 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'cmpl-5b2b5189-9286-4ea5-8d47-3b873ee13c8f', 'object': 'text_completion', 'created': 1712011805, 'model': '/home/ashok/llamacpp/models/zephyr-7b-beta.Q4_K_M.gguf', 'choices': [{'text': \"What do you think about the inclusion policies in Tech companies?acement of the company's products and services.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 11, 'total_tokens': 24}}\n"
          ]
        }
      ],
      "source": [
        "my_prompt = \"What do you think about the inclusion policies in Tech companies?\"\n",
        "zephyr_model_response = generate_text_from_prompt(my_prompt)\n",
        "print(zephyr_model_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd917625-2779-4d26-83b2-b0d912e28879",
      "metadata": {
        "id": "bd917625-2779-4d26-83b2-b0d912e28879"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}